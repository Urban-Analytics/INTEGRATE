---
title: "Sensitivity analysis-restaurant reviews (binary) [0.1 step]"
author: "Yiyu Wang"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document:
    keep_tex: true
    latex_engine: xelatex
subparagraph: true
urlcolor: blue
linkcolor: black
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{booktabs}
---

```{r setup, eval = T, echo = F, message=F, warning=F, results='hide'}
library(knitr)
library(kableExtra)
options(width=60)
opts_chunk$set(echo = TRUE, comment="", error=FALSE, 
               #cache.lazy = FALSE, 
               fig.align="center", message=FALSE, warning=FALSE, tidy = FALSE)
```

# LLM-based Analysis for Restaurant, Movie and Music Reviews
## Part 1: Sensitivity analysis - restaurant reviews (binary)

```{r codeblock}
### Setup
library(tidyverse)
library(httr)
library(jsonlite)
library(glue)
library(purrr)
library(readr)
library(stringr)
library(viridis)
library(dplyr)
library(ggplot2)
library(scales)
library(ggrepel)
library(patchwork)
```

```{r, eval=FALSE}
## Load restaurant review data
restaurant_data <- read.csv("data/restuarant_review_binary.csv")
restaurant_data <- restaurant_data %>%
  transmute(id = row_number(),
            Review = as.character(Review),
            Liked = as.integer(Liked))

# create a new results directory to avoid overwriting old results.
base_dir <- "Sensitivity_Analysis_new_prams"
log_dir <- file.path(base_dir, "logs", "logs_restaurant")
# list_dir <- file.path(base_dir, "results_list", "restaurant_listing_1")
# result_dir <- file.path(base_dir, "results", "binary_restaurant_1")
# save new results in different directories
list_dir <- file.path(base_dir, "results_list", "restaurant_listing_2")
result_dir <- file.path(base_dir, "results", "binary_restaurant_2")

LOG_FILE <- file.path(log_dir, paste0("restaurant_prams_", format(Sys.time(), "%Y-%m-%d-%H%M%S"), ".log"))
dir.create(log_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(list_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(result_dir, recursive = TRUE, showWarnings = FALSE)

# function: split into batches
split_batches <- function(data, batch_size) {
  split(data, ceiling(seq_len(nrow(data)) / batch_size))
}

# generate prompt with different batch_size (0-shot example)
# 1) Dynamically generate the "task text" used in user message
generate_prompt <- function(batch_size) {
  glue(
    "You are an AI designed to assess sentiment in restaurant reviews.
    I will provide you with a batch of {batch_size} restaurant reviews. 
    Your task is to analyze each review and determine whether the reviewer liked the restaurant (1) or did not like it (0).

    Specifically: Read each review closely and identify any words, phrases, or sentiments that indicate positive or negative sentiment about the restaurant experience.
    Consider:
    - Food quality and taste
    - Service quality
    - Atmosphere and ambiance
    - Value for money
    - Overall satisfaction

    Assign a score (0 or 1) for each review, where:
    1 = The reviewer liked the restaurant; 
    0 = The reviewer did not like the restaurant."
  )
}

# 2) Build messages: strict rules in system; task + reviews in user
build_messages <- function(batch_reviews) {
  n <- nrow(batch_reviews)
  
  sys_msg <- glue(
    "You are an AI that classifies sentiment in restaurant reviews.
     The output MUST be a JSON array of 0/1 with exactly {n} integers.
     No reasoning, no explanations, no labels, no extra text, no trailing commas.
     Example output: [1, 0, 1, ..., 0] (exactly {n} numbers).
     If there are fewer or more than {n} scores, correct the response before returning it.
     Make sure the JSON array is correctly formatted before returning it."
  )
  
  user_msg <- paste0(
    generate_prompt(n), "\n\n",
    paste0(seq_along(batch_reviews$Review), ". ", batch_reviews$Review, collapse = "\n")
  )
  
  list(
    list(role = "system", content = sys_msg),
    list(role = "user",   content = user_msg)
  )
}

# 3) Robustly parse model outputs into a numeric vector of length n_expected
parse_scores <- function(raw_output, n_expected) {
  # If reply looks like it was cut before ']', gently try to complete it
  txt <- trimws(raw_output)
  if (is.character(txt) && startsWith(txt, "[") && !grepl("\\]$", txt) && nchar(txt) < 20000) {
    raw_output <- paste0(raw_output, "]")
  }
  
  # 1) Direct JSON parse
  parsed <- try(jsonlite::fromJSON(raw_output), silent = TRUE)
  if (!inherits(parsed, "try-error") && is.numeric(parsed)) {
    scores <- as.numeric(parsed)
  } else {
    scores <- NULL
  }
  
  # 2) If failed, extract the first [...] block and parse
  if (is.null(scores)) {
    m <- regexpr("\\[[^\\]]*\\]", raw_output, perl = TRUE)
    if (m[1] != -1) {
      candidate <- substr(raw_output, m[1], m[1] + attr(m, "match.length") - 1)
      parsed2 <- try(jsonlite::fromJSON(candidate), silent = TRUE)
      if (!inherits(parsed2, "try-error") && is.numeric(parsed2)) {
        scores <- as.numeric(parsed2)
      }
    }
  }
  
  # 3) If still failed, extract all 0/1 tokens and truncate to expected length
  if (is.null(scores)) {
    toks <- gregexpr("\\b[01]\\b", raw_output, perl = TRUE)
    if (toks[[1]][1] != -1) {
      vals <- as.numeric(regmatches(raw_output, toks)[[1]])
      if (length(vals) >= n_expected) {
        scores <- vals[seq_len(n_expected)]
      }
    }
  }
  
  # 4) Final validation (length & values in {0,1})
  if (!is.null(scores) &&
      length(scores) == n_expected &&
      all(scores %in% c(0, 1))) {
    return(scores)
  }
  
  # Fallback to all-NA
  return(rep(NA_real_, n_expected))
}

# API configuration
url <- "https://api.together.xyz/v1/chat/completions"
api_key <- Sys.getenv("TOGETHER_API_KEY")

# Batch-level execution function
run_batch <- function(batch_reviews, param_row, batch_index, log_file) {
  messages <- build_messages(batch_reviews)
  scores <- rep(NA_real_, nrow(batch_reviews))  # default fallback
  
  # Request body (no stop token; rely on robust parser)
  body_list <- list(
    model = "meta-llama/Llama-3.3-70B-Instruct-Turbo",
    messages = messages,
    temperature = param_row$temperature,
    top_p = param_row$top_p,
    top_k = param_row$top_k,
    max_tokens = param_row$max_tokens,
    stream = FALSE
  )
  
  response <- httr::POST(
    url,
    httr::add_headers(Authorization = paste("Bearer", api_key), `Content-Type` = "application/json"),
    body = jsonlite::toJSON(body_list, auto_unbox = TRUE),
    encode = "json"
  )
  Sys.sleep(0.2)
  
  status <- httr::status_code(response)
  ok <- status == 200
  
  # Parse response content (even on error, to capture server message)
  result <- try(httr::content(response, as = "parsed", type = "application/json"), silent = TRUE)
  
  # Extract raw assistant text if present
  assistant_reply <- NA_character_
  if (!inherits(result, "try-error") &&
      !is.null(result$choices) &&
      length(result$choices) > 0 &&
      !is.null(result$choices[[1]]$message$content)) {
    assistant_reply <- stringr::str_trim(result$choices[[1]]$message$content)
  }
  
  # Log HTTP + raw output
  log_entry <- paste0(
    "\n--- BATCH ", batch_index, " ---\n",
    "HTTP STATUS: ", status, "\n",
    "PARAMS: ", glue("temperature={param_row$temperature}, top_p={param_row$top_p}, top_k={param_row$top_k}, max_tokens={param_row$max_tokens}, batch_size={param_row$batch_size}"), "\n",    
    "RESPONSE:\n", ifelse(is.na(assistant_reply), "<no assistant content>", assistant_reply), "\n")
  write(log_entry, file = log_file, append = TRUE)
  
  # If HTTP OK and we have assistant content, try robust parsing
  if (ok && !is.na(assistant_reply)) {
    parsed_scores <- parse_scores(assistant_reply, nrow(batch_reviews))
    
    # Optional one-time retry if too many NA (>50%)
    if (mean(is.na(parsed_scores)) > 0.5) {
      messages_retry <- messages
      messages_retry[[1]]$content <- paste0(messages_retry[[1]]$content,
                                            "\nIMPORTANT: Output ONLY the JSON array. No other text.")
      body_list$messages <- messages_retry
      
      response2 <- httr::POST(
        url,
        httr::add_headers(Authorization = paste("Bearer", api_key), `Content-Type` = "application/json"),
        body = jsonlite::toJSON(body_list, auto_unbox = TRUE),
        encode = "json"
      )
      result2 <- try(httr::content(response2, as = "parsed", type = "application/json"), silent = TRUE)
      assistant_reply2 <- NA_character_
      if (!inherits(result2, "try-error") &&
          !is.null(result2$choices) &&
          length(result2$choices) > 0 &&
          !is.null(result2$choices[[1]]$message$content)) {
        assistant_reply2 <- stringr::str_trim(result2$choices[[1]]$message$content)
      }
      if (!is.na(assistant_reply2)) {
        parsed_scores <- parse_scores(assistant_reply2, nrow(batch_reviews))
        write(paste0("Retry used for batch ", batch_index, "\n"), file = log_file, append = TRUE)
      }
    }
    
    scores <- parsed_scores
  } else {
    # HTTP NOT OK: log error message returned by server if present
    err_text <- try(httr::content(response, as = "text", encoding = "UTF-8"), silent = TRUE)
    write(paste0("ERROR in batch ", batch_index, ": HTTP ", status, " | ", as.character(err_text), "\n"),
          file = log_file, append = TRUE)
  }
  
  tibble(
    id = batch_reviews$id,
    Review = batch_reviews$Review,
    GroundTruth = batch_reviews$Liked,
    Prediction = scores,
    batch_index = batch_index,
    temperature = param_row$temperature,
    top_p = param_row$top_p,
    top_k = param_row$top_k,
    max_tokens = param_row$max_tokens,
    batch_size = nrow(batch_reviews)
  )
}

# Parameter combination runner with auto-saving (each combo once)
# run_exp_chunk <- function(dataset, param_chunk, chunk_id = 1) {
#   batch_counter <- 1
run_exp_chunk <- function(dataset, param_chunk, chunk_id = 1, start_batch = 1, log_dir) {
  batch_counter <- start_batch
  
  # logs for each chunk
  LOG_FILE <- file.path(
    log_dir, 
    paste0("restaurant_chunk", chunk_id, "_", format(Sys.time(), "%Y-%m-%d-%H%M%S"), ".log")
  )

  for (i in 1:nrow(param_chunk)) {
    param_row <- param_chunk[i, ]
    param_id <- paste0("chunk", chunk_id, "_param", i)
    output_file <- glue("{list_dir}/{param_id}.csv")
    
    # Skip if already processed (resumable)
    if (file.exists(output_file)) {
      cat(glue("[{Sys.time()}] Skipping {param_id} (already exists)\n"))
      next
    }
    cat(glue("[{Sys.time()}] Running {param_id} with parameters: ", 
             "T={param_row$temperature}, P={param_row$top_p}, K={param_row$top_k}, ",
             "max_tokens={param_row$max_tokens}, batch_size={param_row$batch_size}\n"))
    
    batches <- split_batches(dataset, param_row$batch_size)
    combo_results <- vector("list", length(batches))
    
    for (b in seq_along(batches)) {
      result <- run_batch(batches[[b]], param_row, batch_counter, log_file = LOG_FILE)
      combo_results[[b]] <- result
      batch_counter <- batch_counter + 1
    }
    
    # Save results for this parameter combination
    result_df <- bind_rows(combo_results)
    write_csv(result_df, output_file)
    
    cat(glue("[{Sys.time()}] Finished {param_id}, saved {nrow(result_df)} rows\n"))
  }
  return(batch_counter) # return the last batch
}

### Run experiments
# existing parameter combos in previous exp.
done_params1 <- expand.grid(
  temperature = c(0.1, 0.2, 0.5),
  top_p = c(0.8, 0.9, 1.0),
  top_k = c(20, 50, 100),
  max_tokens = c(50, 100, 200),
  batch_size = c(5, 10, 20),
  stringsAsFactors = FALSE
)
nrow(done_params1) # 243

# new_grid <- expand.grid(
done_params2 <- expand.grid(
  temperature = c(0.1, 0.3, 0.5, 0.7, 0.9, 1),
  top_p = c(0.1, 0.3, 0.5, 0.7, 0.9, 1),
  top_k = c(10, 30, 50, 70, 90, 100),
  max_tokens = c(100),
  batch_size = c(10),
  stringsAsFactors = FALSE
)
nrow(done_params2) # 216

done_params_all <- bind_rows(done_params1, done_params2) %>%
  distinct()
nrow(done_params_all) # 451

### New parameter grid (finer resolution)
new_grid <- expand.grid(
  temperature = round(seq(0.1, 1.0, by = 0.1), 1),   # step = 0.1
  top_p       = round(seq(0.1, 1.0, by = 0.1), 1),   # step = 0.1
  top_k       = seq(10, 100, by = 10),               # step = 10
  max_tokens  = c(100),                              # fixed
  batch_size  = c(10),                               # fixed
  stringsAsFactors = FALSE
)
nrow(new_grid) # 1000

# remove the param combos processed before
todo_grid <- new_grid %>%
  anti_join(done_params_all, by = c("temperature","top_p","top_k","max_tokens","batch_size"))

nrow(todo_grid) # 765

# Split the grid into chunks (manageable groups, e.g., 20 combos per chunk)
param_chunks <- split(todo_grid, ceiling(seq_len(nrow(todo_grid)) / 20))

# Run each chunk (resumable; you can run a subset if desired)
## global batch_index + logs for each chunk
global_batch <- 1
for (i in seq_along(param_chunks)) {
  cat(glue("\n[{Sys.time()}] === Running chunk {i}/{length(param_chunks)} ===\n"))
  global_batch <- run_exp_chunk(
    restaurant_data, 
    param_chunks[[i]], 
    chunk_id = i, 
    start_batch = global_batch, 
    log_dir = log_dir
  )
}

# Merge all new result CSVs into one final file
all_new_csvs    <- list.files(list_dir, pattern = "*.csv", full.names = TRUE)
all_new_results <- purrr::map_dfr(all_new_csvs, readr::read_csv, show_col_types = FALSE)
# write_csv(all_results, file.path(result_dir, "restaurant_sensitivity_all_results_1.csv"))
final_file <- file.path(result_dir, paste0("restaurant_sensitivity_all_new_results_", format(Sys.time(), "%Y-%m-%d-%H%M%S"), ".csv"))

write_csv(all_new_results, final_file)
cat("Final merged results written to:", final_file, "\n")

## check logs
# Function to check NA rate in Prediction column
check_missing_rate <- function(df) {
  if (!"Prediction" %in% colnames(df)) {
    stop("No column named 'Prediction' in the dataframe")
  }
  
  total <- nrow(df)
  missing <- sum(is.na(df$Prediction))
  rate <- missing / total * 100
  
  cat("Total rows: ", total, "\n")
  cat("Missing Predictions: ", missing, "\n")
  cat("Missing rate: ", round(rate, 2), "%\n")
  
  return(invisible(rate))
}

check_missing_rate(all_new_results)

## merge new results + previous results
# 1) load old results (without id column)
res1 <- read_csv("restaurant_tuning_all_results_2.csv") # without id column
res2 <- read_csv(file.path(base_dir, "results/binary_restaurant_1/restaurant_sensitivity_all_results_merged.csv"))
# res3 <- read_csv(file.path(base_dir, "results/binary_restaurant_2/restaurant_sensitivity_all_new_results_2025-10-04-171244.csv"))
res3 <- all_new_results %>% select(-id)

# 2) Keep only rows from old_results that match new_grid
# (parameter combination must be in new_grid)
res1_filtered <- res1 %>%
  semi_join(new_grid, by = c("temperature", "top_p", "top_k", "max_tokens", "batch_size"))
# 4) Bind the two datasets
all_results <- bind_rows(res3, res2, res1_filtered)
# 5) Save final merged file
write_csv(all_results, file.path(result_dir, "restaurant_sensitivity_results_newgrid1000.csv"))

check_missing_rate(all_results)

# check if the merged results contain all parameter combos.
library(dplyr)
# Extract unique param combos from merged results
merged_params <- all_results %>%
  distinct(temperature, top_p, top_k, max_tokens, batch_size)

# Count
cat("Number of unique param combos in merged results:", nrow(merged_params), "\n")
cat("Number of param combos in new_grid:", nrow(new_grid), "\n")

### calculate metrics
### calculate metrics
# Helper: avoid division by zero
safe_div <- function(a, b) {
  as.numeric(ifelse(b == 0, NA_real_, a / b))
}

# Compute metrics per param combo
compute_metrics_from_vecs <- function(y_true, y_pred) {
  tryCatch({
    y_true <- as.numeric(y_true)
    y_pred <- as.numeric(y_pred)
    
    tp <- sum(y_pred == 1 & y_true == 1, na.rm = TRUE)
    tn <- sum(y_pred == 0 & y_true == 0, na.rm = TRUE)
    fp <- sum(y_pred == 1 & y_true == 0, na.rm = TRUE)
    fn <- sum(y_pred == 0 & y_true == 1, na.rm = TRUE)
    
    acc <- safe_div(tp + tn, tp + tn + fp + fn)
    prec_pos <- safe_div(tp, tp + fp)
    rec_pos  <- safe_div(tp, tp + fn)
    f1_pos   <- ifelse(is.na(prec_pos) | is.na(rec_pos) | (prec_pos + rec_pos) == 0,
                       NA_real_, 2 * prec_pos * rec_pos / (prec_pos + rec_pos))
    
    # negative class metrics
    prec_neg <- safe_div(tn, tn + fn)
    rec_neg  <- safe_div(tn, tn + fp)
    f1_neg   <- ifelse(is.na(prec_neg) | is.na(rec_neg) | (prec_neg + rec_neg) == 0,
                       NA_real_, 2 * prec_neg * rec_neg / (prec_neg + rec_neg))
    
    macro_f1 <- mean(c(f1_pos, f1_neg), na.rm = TRUE)
    specificity <- rec_neg
    bal_acc <- mean(c(rec_pos, specificity), na.rm = TRUE)
    
    # cast to numeric to avoid integer overflow
    denom <- sqrt(as.numeric(tp + fp) * as.numeric(tp + fn) *
                    as.numeric(tn + fp) * as.numeric(tn + fn))
    mcc <- ifelse(denom == 0, NA_real_, ((tp * tn) - (fp * fn)) / denom)
    
    missing_rate <- mean(is.na(y_pred))
    
    tibble(
      accuracy = acc,
      precision = prec_pos,
      recall = rec_pos,
      f1_pos = f1_pos,
      f1_neg = f1_neg,
      macro_f1 = macro_f1,
      specificity = specificity,
      balanced_accuracy = bal_acc,
      mcc = mcc,
      missing_rate = missing_rate
    )
  }, error = function(e) {
    # return NA if error occur
    tibble(
      accuracy = NA_real_,
      precision = NA_real_,
      recall = NA_real_,
      f1_pos = NA_real_,
      f1_neg = NA_real_,
      macro_f1 = NA_real_,
      specificity = NA_real_,
      balanced_accuracy = NA_real_,
      mcc = NA_real_,
      missing_rate = mean(is.na(y_pred))
    )
  })
}

# Apply to restaurant results
# all_restuls <- read_csv(file.path(result_dir, "restaurant_sensitivity_results_newgrid1000.csv"))
results_df <- all_results %>% mutate(
  GroundTruth = as.integer(GroundTruth),
  Prediction  = as.integer(Prediction)
)
metrics_df <- results_df %>%
  group_by(temperature, top_p, top_k, max_tokens, batch_size) %>%
  summarise(
    metrics = list(compute_metrics_from_vecs(GroundTruth, Prediction)),
    .groups = "drop"
  ) %>%
  tidyr::unnest_wider(metrics) %>%
  mutate(across(where(is.double), ~round(., 4))) %>%
  arrange(desc(macro_f1 * (1 - missing_rate)), desc(accuracy), desc(mcc))

write_csv(metrics_df, file.path(result_dir, "restaurant_tuning_metrics_newgrid1000.csv"))
```

# Visualization
### 1) Accuracy heatmap across temperature and top_p, faceted by top_k

```{r p1, fig.width=16, fig.height=10}
# load data
metrics_df <- read_csv(file.path("results/binary_restaurant_2/restaurant_tuning_metrics_newgrid1000.csv"))
metrics_df <- metrics_df %>%
  mutate(top_k = as.integer(top_k),
         temperature = as.numeric(temperature),
         top_p = as.numeric(top_p))

# 1) Accuracy Heatmap with global best points
# temperature x top_p, facet by top_k
min_acc <- min(metrics_df$accuracy, na.rm = TRUE)
max_acc <- max(metrics_df$accuracy, na.rm = TRUE)

p_hm_acc <- ggplot(metrics_df, aes(factor(temperature), factor(top_p), fill = accuracy)) +
  geom_tile(color = "white") +
  facet_wrap(~ top_k, labeller = label_both) +
  scale_fill_gradient(low = "#f0f9e8", high = "#0868ac",
                      limits = c(min_acc, max_acc),
                      breaks = seq(min_acc, max_acc, length.out = 5),
                      labels = percent_format(accuracy = 0.01)) +
  labs(title = "Accuracy heatmap (temperature × top_p, facet by top_k)",
       x = "temperature", y = "top_p", fill = "Accuracy") +
  theme_bw(base_size = 12)
# add global best points
global_best <- metrics_df %>%
  filter(accuracy == max(accuracy, na.rm = TRUE))

p_hm_acc_best <- p_hm_acc +
  geom_point(data = global_best,
             aes(factor(temperature), factor(top_p)),
             shape = 21, size = 4,
             fill = "#E69F00", colour = "#333333", stroke = 1.2) +
  ggrepel::geom_text_repel(
    data = global_best,
    # data = best_acc,
    aes(factor(temperature), factor(top_p),
        label = sprintf("Acc=%.1f%%", accuracy*100)),
    size = 4, fontface = "bold",
    colour = "white",               
    box.padding = 0.25, segment.color = "grey30",
    min.segment.length = 0.05, max.overlaps = 10,
    bg.color = "black", bg.r = 0.15 
  ) +
  theme(
    axis.title.x = element_text(size = 20, face = "bold"),
    axis.title.y = element_text(size = 20, face = "bold"),
    axis.text.x  = element_text(size = 18),
    axis.text.y  = element_text(size = 18),
    strip.text   = element_text(size = 18, face = "bold"),
    legend.title = element_text(size = 18, face = "bold"),
    legend.text  = element_text(size = 16)
  )
p_hm_acc_best 
```

```{r Table1, echo=FALSE, eval=FALSE}
library(gridExtra)

# Table 1: Best combos by Accuracy (for p1) 
best_acc <- metrics_df %>%
  slice_max(order_by = accuracy, n = 1, with_ties = TRUE) %>%
  select(top_k, temperature, top_p, accuracy, macro_f1) %>%
  arrange(top_k, temperature, top_p)

table_acc <- best_acc %>%
  mutate(
    Accuracy = sprintf("%.3f", accuracy),
    MacroF1  = sprintf("%.3f", macro_f1)
  ) %>%
  select(top_k, temperature, top_p, Accuracy, MacroF1)

p_table_acc <- tableGrob(table_acc, rows = NULL, theme = ttheme_default(
  core = list(fg_params = list(cex = 0.9)),       
  colhead = list(fg_params = list(cex = 1.2, fontface = "bold")) 
))
grid::grid.newpage()
grid::grid.draw(p_table_acc)
```

### 2) Barcharts: frequency of global best points by temp., top_p, top_k

```{r p2, fig.width=14, fig.height=8}
# 2) barcharts: the distribution of global best points
bar_temp  <- global_best %>% count(temperature)
bar_top_p <- global_best %>% count(top_p)
bar_top_k <- global_best %>% count(top_k)

common_theme <- theme(
  axis.title.x = element_text(size = 16, face = "bold"),
  axis.title.y = element_text(size = 16, face = "bold"),
  axis.text.x  = element_text(size = 14),
  axis.text.y  = element_text(size = 14),
  strip.text   = element_text(size = 14, face = "bold"),
  legend.title = element_text(size = 14, face = "bold"),
  legend.text  = element_text(size = 12)
  )

p_bar_temp <- ggplot(bar_temp, aes(x=factor(temperature), y=n)) +
  geom_text(aes(label = n), vjust = -0.5, size = 5) +
  geom_col(fill="darkorange", alpha = 0.7) +
  labs(x="Temperature", y="Count", title="Frequency of global best points by Temperature") +
  theme_minimal(base_size=12) +
  common_theme 
# p_bar_temp

p_bar_top_p <- ggplot(bar_top_p, aes(x=factor(top_p), y=n)) +
  geom_text(aes(label = n), vjust = -0.5, size = 5) +
  geom_col(fill="#41b6c4", alpha = 0.8) +
  labs(x="Top_p", y="Count", title="Frequency of global best points by Top_p") +
  theme_minimal(base_size=12) +
  common_theme 
# p_bar_top_p

p_bar_top_k <- ggplot(bar_top_k, aes(x=factor(top_k), y=n)) +
  geom_col(fill="#225ea8", alpha = 0.8) +
  geom_text(aes(label = n), vjust = -0.5, size = 5) +
  labs(x="Top_k", y="Count", title="Frequency of global best points by Top_k") +
  theme_minimal(base_size=12) +
  common_theme 
# p_bar_top_k

bars_combined <- p_bar_temp | p_bar_top_p | p_bar_top_k 
bars_combined
```

### 3) save plots

```{r save, eval=FALSE}
# save plots
plot_dir <- file.path(base_dir, "plots", "binary_restaurant_plots_1000grid")
dir.create(plot_dir, recursive = TRUE, showWarnings = FALSE)
ggsave(file.path(plot_dir, "p1_HEATMAP_Accuracy_restaurant.png"), plot = p_hm_acc_best, width = 14, height = 10, dpi = 300)
ggsave(file.path(plot_dir, "p2_BARCHART_frequency_best_points_by_param_restaurant.png"), plot = bars_combined, width = 14, height = 8, dpi = 300)
```


