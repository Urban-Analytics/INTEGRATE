{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94edbe3a-69e4-421c-bb8b-6c0d83a313f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import folium\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import sys\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "import html\n",
    "    \n",
    "from together import Together  # pip install together\n",
    "sys.path.insert(1, '../')\n",
    "from Functions import get_gentrification_scores, map_static, get_gentrification_scores_categorical\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)  # Convert things like &amp; to &\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)  # Remove <br> or <br /> tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove other HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse repeated whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def deduplicate_with_logging(df, lsoa_name):\n",
    "    if len(df) < 2:\n",
    "        return df  # Nothing to compare\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df.index)\n",
    "\n",
    "    for i, j in combinations(df.index, 2):\n",
    "        score = fuzz.token_set_ratio(df.at[i, 'text'], df.at[j, 'text'])\n",
    "        if score >= 75:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "    groups = list(nx.connected_components(G))\n",
    "    unique_indices = [sorted(group)[0] for group in groups]\n",
    "\n",
    "    # LOG: Print duplicates\n",
    "    for group in groups:\n",
    "        if len(group) > 1:\n",
    "            # print(f\"\\nDuplicates in {lsoa_name}:\")\n",
    "            for idx in sorted(group):\n",
    "                text_snippet = df.at[idx, 'text'][:120].replace('\\n', ' ')\n",
    "                # print(f\" - {text_snippet}\")\n",
    "\n",
    "    return df.loc[unique_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "341e74f0-6330-46fe-a9b2-a8726906c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'manchester'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43733656-cd47-41fb-99f7-121ce290fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = gpd.read_file(f\"../../../data/AirbnbData/airbnb-{city}/neighbourhoods.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db0cc48-0ce8-422d-904c-29e543d96fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file('../../../data/SpatialData/LSOAs_2011/LSOA_2011_EW_BSC_V4.shp')\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "pattern = '|'.join(manc_lads)\n",
    "manchester_lsoas =lsoas[lsoas['LSOA11NMW'].str.contains(pattern)]\n",
    "\n",
    "# Reproject to a CRS with meters (British National Grid)\n",
    "manchester_lsoas = manchester_lsoas.to_crs(epsg=27700)\n",
    "\n",
    "# Define Manchester city centre point (in WGS84, then project)\n",
    "city_centre_wgs84 = Point(-2.2426, 53.4808)  # approx lat/lon of Manchester city centre\n",
    "city_centre_point = gpd.GeoSeries([city_centre_wgs84], crs='EPSG:4326').to_crs(epsg=27700).iloc[0]\n",
    "\n",
    "# Filter polygons within a buffer distance (e.g., 3 km radius)\n",
    "buffer = city_centre_point.buffer(6000)  # 3000 meters\n",
    "central_lsoas = manchester_lsoas[manchester_lsoas.intersects(buffer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba28e8-e0fb-454d-b554-61e70612d14f",
   "metadata": {},
   "source": [
    "### Join together 4 sets of listings, removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e574c970-cd94-4a3a-8fe2-160e049f6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_mar = pd.read_csv(f\"../../../data/AirbnbData/airbnb-{city}/listings_mar24.csv.gz\")\n",
    "listings_jun = pd.read_csv(f\"../../../data/AirbnbData/airbnb-{city}/listings_jun24.csv.gz\")\n",
    "listings_sept = pd.read_csv(f\"../../../data/AirbnbData/airbnb-{city}/listings_sept24.csv.gz\")\n",
    "listings_dec = pd.read_csv(f\"../../../data/AirbnbData/airbnb-{city}/listings_dec24.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "251e51b9-7e12-497e-ae94-f99e6b7187fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26192\n",
      "10109\n",
      "9668\n",
      "8102\n"
     ]
    }
   ],
   "source": [
    "# Add a column indicating the source dataframe\n",
    "listings_mar[\"source\"] = \"Mar\"\n",
    "listings_jun[\"source\"] = \"Jun\"\n",
    "listings_sept[\"source\"] = \"Sept\"\n",
    "listings_dec[\"source\"] = \"Dec\"\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "combined = pd.concat([listings_mar, listings_jun, listings_sept, listings_dec], ignore_index=True)\n",
    "\n",
    "# Group by listing ID and collect sources\n",
    "# combined[\"source\"] = combined.groupby([\"id\", \"listing_url\", 'name'])[\"source\"].transform(lambda x: \", \".join(sorted(set(x))))\n",
    "print(len(combined))\n",
    "\n",
    "# Drop duplicates based on listing ID (keeping the first occurrence)\n",
    "unique_listings = combined.drop_duplicates(subset=['id'], keep='first').copy()\n",
    "print(len(unique_listings))\n",
    "unique_listings = unique_listings.drop_duplicates(subset=['latitude', 'longitude'], keep='first').copy()\n",
    "print(len(unique_listings))\n",
    "unique_listings = unique_listings.drop_duplicates(subset=['neighborhood_overview', 'description'], keep='first').copy()\n",
    "print(len(unique_listings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8445d-25c7-4b8c-9899-359e17f50d0d",
   "metadata": {},
   "source": [
    "### Add text columns combining description and neighbourhood overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf3bb744-e40a-469a-9884-27e6241b46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_listings['text'] = np.where(\n",
    "    unique_listings[['description', 'neighborhood_overview']].isna().all(axis=1),  # Check if both are NaN\n",
    "    np.nan,  # Assign NaN if both are NaN\n",
    "    unique_listings['description'].fillna('') + \" \" + unique_listings['neighborhood_overview'].fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f486521-f8ed-47b0-9db7-072e1fa7c6e7",
   "metadata": {},
   "source": [
    "### Delete listings with no textual description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63daabaf-a36d-44c0-b76d-46a9b426c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8101\n"
     ]
    }
   ],
   "source": [
    "unique_listings[unique_listings['text'].isna()][[\"description\", \"neighborhood_overview\",\"text\"]]\n",
    "unique_listings = unique_listings[unique_listings['text'].notnull()]\n",
    "print(len(unique_listings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717c1ac-4a5f-49c2-aff7-c37ad6b65ad4",
   "metadata": {},
   "source": [
    "### Add spatial information to listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8fb02ce-bcd0-4b04-ba29-9b4524b9c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8101\n"
     ]
    }
   ],
   "source": [
    "unique_listings['geometry'] = unique_listings.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "unique_listings_gdf = gpd.GeoDataFrame(unique_listings, geometry='geometry', crs=\"EPSG:4326\")\n",
    "unique_listings_gdf = unique_listings_gdf.to_crs(manchester_lsoas.crs)\n",
    "unique_listings_gdf = gpd.sjoin(unique_listings_gdf, manchester_lsoas[['LSOA11NM', 'geometry']], how='left', predicate='within')\n",
    "print(len(unique_listings_gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6942652-ecbc-4157-bca5-26076ee8cd39",
   "metadata": {},
   "source": [
    "### Keep only those in central Manchester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d37a4fd5-d433-48de-a18a-f0bfb8e04be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5262\n"
     ]
    }
   ],
   "source": [
    "buffer = city_centre_point.buffer(6000)  # 3000 meters\n",
    "unique_listings_gdf = unique_listings_gdf[unique_listings_gdf.intersects(buffer)]\n",
    "print(len(unique_listings_gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912024a6-a4ce-4a8a-961b-c0da8cc4f0e0",
   "metadata": {},
   "source": [
    "### Get a datarame containing just one entry per LSOA (with textual descriptions combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0007647-d8a6-45d6-8763-9f3488c18e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for lsoa, group in unique_listings_gdf.groupby(\"LSOA11NM\"):\n",
    "    # check for duplicaed listings\n",
    "    deduped = deduplicate_with_logging(group, lsoa)\n",
    "    # Combine all text entries into one string\n",
    "    combined_text = \" \".join(deduped['text'].dropna().astype(str))\n",
    "    \n",
    "    # Take LSOA11NM from any row (they should all be the same)\n",
    "    lsoa_name = deduped['LSOA11NM'].iloc[0]\n",
    "    \n",
    "    # Create the new single-row dataframe\n",
    "    single_row_df = pd.DataFrame({\n",
    "        'LSOA11NM': [lsoa_name],\n",
    "        'text': [combined_text]})\n",
    "    results.append(single_row_df)\n",
    "\n",
    "# Combine results\n",
    "deduplicated_listings = pd.concat(results).reset_index(drop=True)\n",
    "print(len(deduplicated_listings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bba1a-69ce-4de1-85ab-6556bdc0a4e3",
   "metadata": {},
   "source": [
    "### Clean weird characters out of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2f2dc5-2fb1-44ab-ae73-f7392e31f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples before and after cleaning\n",
    "# for i in range(10,20):\n",
    "    #print(\"ORIGINAL:\")\n",
    "    #print(deduplicated_listings.loc[i, 'text'])\n",
    "    #print(\"CLEANED:\")\n",
    "    #print(clean_text(deduplicated_listings.loc[i, 'text']))\n",
    "    #print(\"=\"*40)\n",
    "\n",
    "deduplicated_listings['text'] = deduplicated_listings['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7800f-23e5-43a5-bbdb-60637282d82d",
   "metadata": {},
   "source": [
    "## The prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ab8edb-fdd1-44bc-a4a6-79e85e5e1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_explanatory = f\"\"\"\n",
    "You are an expert in urban studies with a deep understanding of gentrification and its portrayal in public discourse. I will provide you with Airbnb \n",
    "listings, including a description of the property and a neighborhood overview. Your task is to analyze these texts and assess the gentrification status \n",
    "of the area based on how the neighborhood is presented.\n",
    "\n",
    "Focus primarily on the neighborhood overview and description, ignoring property-specific details like the number of bedrooms, amenities, or decor. \n",
    "\n",
    "Consider the following:\n",
    "- Direct mentions of local attractions, businesses, or community features that suggest development or revitalization.\n",
    "- Language that highlights cultural hotspots, boutique shops, trendy cafes, or artisanal markets.\n",
    "- Descriptions that emphasize diversity, safety, or the presence of creative communities, as these can signal gentrification dynamics.\n",
    "- Listings that avoid mentioning the neighborhood or speak only broadly about the city may imply that the immediate area lacks desirable features or is \n",
    "not a selling point. This absence of detail should inform your assessment.\n",
    "\n",
    "Assign one of the following categories:\n",
    "- \"Established\": A well-known, desirable area with stable appeal and little active change.\n",
    "- \"Gentrifying\": Signs of recent or ongoing transformation, such as new businesses or cultural shifts.\n",
    "- \"Emerging\": Early indicators of gentrification potential, like creative spaces or gradual commercial interest.\n",
    "- \"Undeveloped\": Lacking indicators of gentrification, often reflected in vague or absent neighborhood descriptions.\n",
    "\n",
    "If a listing genuinely lacks sufficient information to make any assessment (e.g., the text is too sparse), assign a score of 'NA' and briefly explain.\n",
    "\n",
    "Provide your answer strictly in the format:\n",
    "'1. Category. Reasoning.', without any additional explanation or commentary. Only provide one score per listing'\n",
    "\"\"\"\n",
    "prompt = prompt_explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daae082-ac21-42c7-b57a-668c8eef5a40",
   "metadata": {},
   "source": [
    "## Decide whether to run LLM\n",
    "Decide whether to run the LLM or load a file of scores that has been previously calculated and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5ebf02-b541-43a4-8400-aaf174051021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for existing files\n",
    "base_dir = os.path.expanduser(f\"../../../data/AirbnbData/airbnb-{city}/\")\n",
    "base_filename = \"airbnb_gentrification_scores_one_per_lsoa\"\n",
    "file_extension = \".gpkg\"\n",
    "\n",
    "# List all matching files in the directory\n",
    "matching_files = [\n",
    "    f for f in sorted(os.listdir(base_dir))\n",
    "    if re.match(f\"{base_filename}_\\\\d{{2}}{file_extension}$\", f)]\n",
    "matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504b0b89-e70e-405f-9f58-a6e819ff5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching files found. 'df' will not be loaded.\n"
     ]
    }
   ],
   "source": [
    "# If there are matching files, find the most recent one\n",
    "if matching_files:\n",
    "    df = gpd.read_file(base_dir+matching_files[-1])\n",
    "    print(f\"Loaded file: {matching_files[-1]}\")\n",
    "else:\n",
    "    # If no matching files are found\n",
    "    print(\"No matching files found. 'df' will not be loaded.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a67f879-ac32-402b-b6b1-46c39252637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log with the current time\n",
    "LOG_FILE = os.path.join(\"../logs\", datetime.now().strftime(\"%Y-%m-%d-%H%M%S.log\"))\n",
    "def log(msg):\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bfa2e81-6362-4d84-bf75-0b4b5df6cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have already loaded a gentrification file, will not re-run the LLM.\n",
      "Running LLM\n",
      "Will query the LM for 309 items\n",
      "Submitting batch 1 of 309...\n",
      "Submitting batch 2 of 309...\n",
      "Submitting batch 3 of 309...\n",
      "Submitting batch 4 of 309...\n",
      "Submitting batch 5 of 309...\n",
      "Submitting batch 6 of 309...\n",
      "Submitting batch 7 of 309...\n",
      "Submitting batch 8 of 309...\n",
      "Submitting batch 9 of 309...\n",
      "Submitting batch 10 of 309...\n",
      "Submitting batch 11 of 309...\n",
      "Submitting batch 12 of 309...\n",
      "Submitting batch 13 of 309...\n",
      "Submitting batch 14 of 309...\n",
      "Submitting batch 15 of 309...\n",
      "Submitting batch 16 of 309...\n",
      "Submitting batch 17 of 309...\n",
      "Submitting batch 18 of 309...\n",
      "Submitting batch 19 of 309...\n",
      "Submitting batch 20 of 309...\n",
      "Submitting batch 21 of 309...\n",
      "Submitting batch 22 of 309...\n",
      "Submitting batch 23 of 309...\n",
      "Submitting batch 24 of 309...\n",
      "Submitting batch 25 of 309...\n",
      "Submitting batch 26 of 309...\n",
      "Submitting batch 27 of 309...\n",
      "Submitting batch 28 of 309...\n",
      "Submitting batch 29 of 309...\n",
      "Submitting batch 30 of 309...\n",
      "Submitting batch 31 of 309...\n",
      "Submitting batch 32 of 309...\n",
      "Submitting batch 33 of 309...\n",
      "Submitting batch 34 of 309...\n",
      "Submitting batch 35 of 309...\n",
      "Submitting batch 36 of 309...\n",
      "Submitting batch 37 of 309...\n",
      "Submitting batch 38 of 309...\n",
      "Submitting batch 39 of 309...\n",
      "Submitting batch 40 of 309...\n",
      "Submitting batch 41 of 309...\n",
      "Submitting batch 42 of 309...\n",
      "Submitting batch 43 of 309...\n",
      "Submitting batch 44 of 309...\n",
      "Submitting batch 45 of 309...\n",
      "Submitting batch 46 of 309...\n",
      "Submitting batch 47 of 309...\n",
      "Submitting batch 48 of 309...\n",
      "Submitting batch 49 of 309...\n",
      "Submitting batch 50 of 309...\n",
      "Submitting batch 51 of 309...\n",
      "Submitting batch 52 of 309...\n",
      "Submitting batch 53 of 309...\n",
      "Submitting batch 54 of 309...\n",
      "Submitting batch 55 of 309...\n",
      "Submitting batch 56 of 309...\n",
      "Submitting batch 57 of 309...\n",
      "Submitting batch 58 of 309...\n",
      "Submitting batch 59 of 309...\n",
      "Submitting batch 60 of 309...\n",
      "Submitting batch 61 of 309...\n",
      "Submitting batch 62 of 309...\n",
      "Submitting batch 63 of 309...\n",
      "Submitting batch 64 of 309...\n",
      "Submitting batch 65 of 309...\n",
      "Submitting batch 66 of 309...\n",
      "Submitting batch 67 of 309...\n",
      "Submitting batch 68 of 309...\n",
      "Submitting batch 69 of 309...\n",
      "Submitting batch 70 of 309...\n",
      "Submitting batch 71 of 309...\n",
      "Submitting batch 72 of 309...\n",
      "Submitting batch 73 of 309...\n",
      "Submitting batch 74 of 309...\n",
      "Submitting batch 75 of 309...\n",
      "Submitting batch 76 of 309...\n",
      "Submitting batch 77 of 309...\n",
      "Submitting batch 78 of 309...\n",
      "Submitting batch 79 of 309...\n",
      "Submitting batch 80 of 309...\n",
      "Submitting batch 81 of 309...\n",
      "Submitting batch 82 of 309...\n",
      "Submitting batch 83 of 309...\n",
      "Submitting batch 84 of 309...\n",
      "Submitting batch 85 of 309...\n",
      "Submitting batch 86 of 309...\n",
      "Submitting batch 87 of 309...\n",
      "Submitting batch 88 of 309...\n",
      "Submitting batch 89 of 309...\n",
      "Submitting batch 90 of 309...\n",
      "Submitting batch 91 of 309...\n",
      "Submitting batch 92 of 309...\n",
      "Submitting batch 93 of 309...\n",
      "Submitting batch 94 of 309...\n",
      "Submitting batch 95 of 309...\n",
      "Submitting batch 96 of 309...\n",
      "Submitting batch 97 of 309...\n",
      "Submitting batch 98 of 309...\n",
      "Submitting batch 99 of 309...\n",
      "Submitting batch 100 of 309...\n",
      "Submitting batch 101 of 309...\n",
      "Submitting batch 102 of 309...\n",
      "Submitting batch 103 of 309...\n",
      "Submitting batch 104 of 309...\n",
      "Submitting batch 105 of 309...\n",
      "Submitting batch 106 of 309...\n",
      "Submitting batch 107 of 309...\n",
      "Submitting batch 108 of 309...\n",
      "Submitting batch 109 of 309...\n",
      "Submitting batch 110 of 309...\n",
      "Submitting batch 111 of 309...\n",
      "Submitting batch 112 of 309...\n",
      "Submitting batch 113 of 309...\n",
      "Submitting batch 114 of 309...\n",
      "Submitting batch 115 of 309...\n",
      "Submitting batch 116 of 309...\n",
      "Submitting batch 117 of 309...\n",
      "Submitting batch 118 of 309...\n",
      "Submitting batch 119 of 309...\n",
      "Submitting batch 120 of 309...\n",
      "Submitting batch 121 of 309...\n",
      "Submitting batch 122 of 309...\n",
      "Submitting batch 123 of 309...\n",
      "Submitting batch 124 of 309...\n",
      "Submitting batch 125 of 309...\n",
      "Submitting batch 126 of 309...\n",
      "Submitting batch 127 of 309...\n",
      "Submitting batch 128 of 309...\n",
      "Submitting batch 129 of 309...\n",
      "Submitting batch 130 of 309...\n",
      "Submitting batch 131 of 309...\n",
      "Submitting batch 132 of 309...\n",
      "Submitting batch 133 of 309...\n",
      "Submitting batch 134 of 309...\n",
      "Submitting batch 135 of 309...\n",
      "Submitting batch 136 of 309...\n",
      "Submitting batch 137 of 309...\n",
      "Submitting batch 138 of 309...\n",
      "Submitting batch 139 of 309...\n",
      "Submitting batch 140 of 309...\n",
      "Submitting batch 141 of 309...\n",
      "Submitting batch 142 of 309...\n",
      "Submitting batch 143 of 309...\n",
      "Submitting batch 144 of 309...\n",
      "Submitting batch 145 of 309...\n",
      "Submitting batch 146 of 309...\n",
      "Submitting batch 147 of 309...\n",
      "Submitting batch 148 of 309...\n",
      "Submitting batch 149 of 309...\n",
      "Submitting batch 150 of 309...\n",
      "Submitting batch 151 of 309...\n",
      "Submitting batch 152 of 309...\n",
      "Submitting batch 153 of 309...\n",
      "Submitting batch 154 of 309...\n",
      "Submitting batch 155 of 309...\n",
      "Submitting batch 156 of 309...\n",
      "Submitting batch 157 of 309...\n",
      "Submitting batch 158 of 309...\n",
      "Submitting batch 159 of 309...\n",
      "Submitting batch 160 of 309...\n",
      "Submitting batch 161 of 309...\n",
      "Submitting batch 162 of 309...\n",
      "Submitting batch 163 of 309...\n",
      "Submitting batch 164 of 309...\n",
      "Submitting batch 165 of 309...\n",
      "Submitting batch 166 of 309...\n",
      "Submitting batch 167 of 309...\n",
      "Submitting batch 168 of 309...\n",
      "Submitting batch 169 of 309...\n",
      "Submitting batch 170 of 309...\n",
      "Submitting batch 171 of 309...\n",
      "Submitting batch 172 of 309...\n",
      "Submitting batch 173 of 309...\n",
      "Submitting batch 174 of 309...\n",
      "Submitting batch 175 of 309...\n",
      "Submitting batch 176 of 309...\n",
      "Submitting batch 177 of 309...\n",
      "Submitting batch 178 of 309...\n",
      "Submitting batch 179 of 309...\n",
      "Submitting batch 180 of 309...\n",
      "Submitting batch 181 of 309...\n",
      "Submitting batch 182 of 309...\n",
      "Submitting batch 183 of 309...\n",
      "Submitting batch 184 of 309...\n",
      "Submitting batch 185 of 309...\n",
      "Submitting batch 186 of 309...\n",
      "Submitting batch 187 of 309...\n",
      "Submitting batch 188 of 309...\n",
      "Submitting batch 189 of 309...\n",
      "Submitting batch 190 of 309...\n",
      "Submitting batch 191 of 309...\n",
      "Submitting batch 192 of 309...\n",
      "Submitting batch 193 of 309...\n",
      "Submitting batch 194 of 309...\n",
      "Submitting batch 195 of 309...\n",
      "Submitting batch 196 of 309...\n",
      "Submitting batch 197 of 309...\n",
      "Submitting batch 198 of 309...\n",
      "Submitting batch 199 of 309...\n",
      "Submitting batch 200 of 309...\n",
      "Submitting batch 201 of 309...\n",
      "Submitting batch 202 of 309...\n",
      "Submitting batch 203 of 309...\n",
      "Submitting batch 204 of 309...\n",
      "Submitting batch 205 of 309...\n",
      "Submitting batch 206 of 309...\n",
      "Submitting batch 207 of 309...\n",
      "Submitting batch 208 of 309...\n",
      "Submitting batch 209 of 309...\n",
      "Submitting batch 210 of 309...\n",
      "Submitting batch 211 of 309...\n",
      "Submitting batch 212 of 309...\n",
      "Submitting batch 213 of 309...\n",
      "Submitting batch 214 of 309...\n",
      "Submitting batch 215 of 309...\n",
      "Submitting batch 216 of 309...\n",
      "Submitting batch 217 of 309...\n",
      "Submitting batch 218 of 309...\n",
      "Submitting batch 219 of 309...\n",
      "Submitting batch 220 of 309...\n",
      "Submitting batch 221 of 309...\n",
      "Submitting batch 222 of 309...\n",
      "Submitting batch 223 of 309...\n",
      "Submitting batch 224 of 309...\n",
      "Submitting batch 225 of 309...\n",
      "Submitting batch 226 of 309...\n",
      "Submitting batch 227 of 309...\n",
      "Submitting batch 228 of 309...\n",
      "Submitting batch 229 of 309...\n",
      "Submitting batch 230 of 309...\n",
      "Submitting batch 231 of 309...\n",
      "Submitting batch 232 of 309...\n",
      "Submitting batch 233 of 309...\n",
      "Submitting batch 234 of 309...\n",
      "Submitting batch 235 of 309...\n",
      "Submitting batch 236 of 309...\n",
      "Submitting batch 237 of 309...\n",
      "Submitting batch 238 of 309...\n",
      "Submitting batch 239 of 309...\n",
      "Submitting batch 240 of 309...\n",
      "Submitting batch 241 of 309...\n",
      "Submitting batch 242 of 309...\n",
      "Submitting batch 243 of 309...\n",
      "Submitting batch 244 of 309...\n",
      "Submitting batch 245 of 309...\n",
      "Submitting batch 246 of 309...\n",
      "Submitting batch 247 of 309...\n",
      "Submitting batch 248 of 309...\n",
      "Submitting batch 249 of 309...\n",
      "Submitting batch 250 of 309...\n",
      "Submitting batch 251 of 309...\n",
      "Submitting batch 252 of 309...\n",
      "Submitting batch 253 of 309...\n",
      "Submitting batch 254 of 309...\n",
      "Submitting batch 255 of 309...\n",
      "Submitting batch 256 of 309...\n",
      "Submitting batch 257 of 309...\n",
      "Submitting batch 258 of 309...\n",
      "Submitting batch 259 of 309...\n",
      "Submitting batch 260 of 309...\n",
      "Submitting batch 261 of 309...\n",
      "Submitting batch 262 of 309...\n",
      "Submitting batch 263 of 309...\n",
      "Submitting batch 264 of 309...\n",
      "Submitting batch 265 of 309...\n",
      "Submitting batch 266 of 309...\n",
      "Submitting batch 267 of 309...\n",
      "Submitting batch 268 of 309...\n",
      "Submitting batch 269 of 309...\n",
      "Submitting batch 270 of 309...\n",
      "Submitting batch 271 of 309...\n",
      "Submitting batch 272 of 309...\n",
      "Submitting batch 273 of 309...\n",
      "Submitting batch 274 of 309...\n",
      "Submitting batch 275 of 309...\n",
      "Submitting batch 276 of 309...\n",
      "Submitting batch 277 of 309...\n",
      "Submitting batch 278 of 309...\n",
      "Submitting batch 279 of 309...\n",
      "Submitting batch 280 of 309...\n",
      "Submitting batch 281 of 309...\n",
      "Submitting batch 282 of 309...\n",
      "Submitting batch 283 of 309...\n",
      "Submitting batch 284 of 309...\n",
      "Submitting batch 285 of 309...\n",
      "Submitting batch 286 of 309...\n",
      "Submitting batch 287 of 309...\n",
      "Submitting batch 288 of 309...\n",
      "Submitting batch 289 of 309...\n",
      "Submitting batch 290 of 309...\n",
      "Submitting batch 291 of 309...\n",
      "Submitting batch 292 of 309...\n",
      "Submitting batch 293 of 309...\n",
      "Submitting batch 294 of 309...\n",
      "Submitting batch 295 of 309...\n",
      "Submitting batch 296 of 309...\n",
      "Submitting batch 297 of 309...\n",
      "Submitting batch 298 of 309...\n",
      "Submitting batch 299 of 309...\n",
      "Submitting batch 300 of 309...\n",
      "Submitting batch 301 of 309...\n",
      "Submitting batch 302 of 309...\n",
      "Submitting batch 303 of 309...\n",
      "Submitting batch 304 of 309...\n",
      "Submitting batch 305 of 309...\n",
      "Submitting batch 306 of 309...\n",
      "Submitting batch 307 of 309...\n",
      "Submitting batch 308 of 309...\n",
      "Submitting batch 309 of 309...\n",
      "Finished querying LLM. Now saving file\n",
      "File saved as: ../../../data/AirbnbData/airbnb-manchester/airbnb_gentrification_scores_one_per_lsoa_10.csv\n"
     ]
    }
   ],
   "source": [
    "RUN_LLM = True\n",
    "if df is not None:\n",
    "    print(\"Have already loaded a gentrification file, will not re-run the LLM.\")\n",
    "    RUN_LLM = False\n",
    "\n",
    "RUN_LLM = True  # Optionally override\n",
    "\n",
    "if RUN_LLM:\n",
    "    print(\"Running LLM\")\n",
    "    # Get the API key from a file\n",
    "    with open('../together.ai_key.txt', 'r') as f:\n",
    "        api_key = f.readline().strip()\n",
    "\n",
    "    client = Together(api_key=api_key)\n",
    "\n",
    "    # Sample for now?\n",
    "    df = deduplicated_listings.copy()\n",
    "    # df = deduplicated_listings.copy()\n",
    "\n",
    "    print(f\"Will query the LM for {len(df)} items\")\n",
    "\n",
    "    assert len(df) < 11000, \"Too many tweets to process in one go. Please reduce the number of tweets.\"\n",
    "\n",
    "    # Ensure the index is consecutive and ascending\n",
    "    df = df.reset_index(drop=True)\n",
    "    # To store the results\n",
    "    df['gentrification_prediction'] = None\n",
    "\n",
    "    # Batch processing\n",
    "    batch_size = 1\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Get the batch of tweets\n",
    "        batch_tweets = df.loc[i:i + batch_size - 1, :]\n",
    "\n",
    "        # Get sentiments using the function\n",
    "        print(f\"Submitting batch {i//len(batch_tweets)+1} of {len(df)//len(batch_tweets)}...\")\n",
    "        #print(\"-----\")\n",
    "        #print(\"Sending to LLM:\", batch_tweets['text'].tolist())\n",
    "        #print(\"-----\")\n",
    "        ids, sentiments, explanations = get_gentrification_scores_categorical_one_per_lsoa(\n",
    "            batch_tweets, prompt, client, batch_index=i, max_tokens=15000)\n",
    "        #print(\"----------------\")\n",
    "        # Update the DataFrame with the predictions\n",
    "        df.loc[ids, 'gentrification_prediction'] = sentiments\n",
    "        df.loc[ids, 'explanation'] = explanations\n",
    "\n",
    "        # Predictions should be integers\n",
    "        # df.gentrification_prediction = df.gentrification_prediction.astype('Int64')\n",
    "\n",
    "    print(\"Finished querying LLM. Now saving file\")\n",
    "\n",
    "    # Initialize counter and check for existing files\n",
    "    counter = 1\n",
    "    while True:\n",
    "        filename = f\"{base_filename}_{counter:02d}.csv\"\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            break\n",
    "        counter += 1\n",
    "    df.to_csv(f\"{filepath}\")    \n",
    "    # df.set_crs(epsg=4326, inplace=True)\n",
    "    # df.to_file(filepath, layer=\"data\", driver=\"GPKG\")\n",
    "    print(f\"File saved as: {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
