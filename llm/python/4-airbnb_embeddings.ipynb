{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757f12481dc91b71",
   "metadata": {},
   "source": [
    "# Analyse Embeddings from AirBnB data to see if they can predict gentrification (using the Manchester Gentrification Index)\n",
    "\n",
    "  - Data from [Inside AirBnB](https://insideairbnb.com/)\n",
    "  - [Manchester gentrification index](https://www.common-wealth.org/interactive/the-greater-manchester-gentrification-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf0ba36f26207f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5e5d213c632f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T06:08:08.406326Z",
     "start_time": "2025-06-12T06:08:08.403672Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import time\n",
    "import folium\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import branca\n",
    "import branca.colormap as cm\n",
    "from shapely.geometry import Point\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import folium, rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323d61a1ce053e7",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data for the Greater Manchester Gentrification Index were provided privately. They have not been included in the github repository.\n",
    "\n",
    "Airbnb data for Manchester are [Inside AirBnB](https://insideairbnb.com/get-the-data/). Specifically four different snapshots:\n",
    "  - 2004-03-28\n",
    "  - 2024-06-26\n",
    "  - 2024-09-23\n",
    "  - 2024-12-25\n",
    "\n",
    "The code will try to download the ata if they don't exist.\n",
    "\n",
    "From each snapshot, we use three files:\n",
    "   - `listings.csv.gz`\n",
    "   - `reviews.csv.gz`\n",
    "   - `neighbourhoods.geojson`\n",
    "\n",
    "The downloaded files need to be placed in a directory called [../data/airbnb-manchester](../data/airbnb-manchester) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441eb711ef8c92c",
   "metadata": {},
   "source": [
    "### Download and prepare AirBnB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0b6d6fffbec0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:47.498304Z",
     "start_time": "2025-06-11T13:45:47.494753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try to download the data, checking if it exists already\n",
    "dates = [ '2024-03-28', '2024-06-26', '2024-09-23', '2024-12-25']\n",
    "files = [\"listings.csv.gz\", \"reviews.csv.gz\"]\n",
    "data_dir = os.path.join(\"..\", \"data\", \"airbnb-manchester\")\n",
    "root_url = \"https://data.insideairbnb.com/united-kingdom/england/greater-manchester/\"\n",
    "neighbourhoods_url = \"https://data.insideairbnb.com/united-kingdom/england/greater-manchester/2024-12-25/visualisations/neighbourhoods.geojson\"\n",
    "\n",
    "for d in dates:\n",
    "    # Check if d is already a directory\n",
    "    if os.path.isdir(os.path.join(data_dir, d)):\n",
    "        print(f\"Directory {d} already exists.\")\n",
    "        continue\n",
    "    # It isn't, so create it and download the data\n",
    "    os.makedirs(os.path.join(data_dir, d), exist_ok=True)\n",
    "    print(f\"Downloading data for {d} ...\")\n",
    "    for f in files:\n",
    "        url = f\"{root_url}{d}/data/{f}\"\n",
    "        try:\n",
    "            print(f\"\\tDownloading {f} from {url} \", end=\"\")\n",
    "            urllib.request.urlretrieve(url, os.path.join(data_dir, d, f))\n",
    "            print(\"...done.\")\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"Error downloading {f} for {d}: {e}\")\n",
    "        # Sleep for a few seconds so not to abuse their server\n",
    "        time.sleep(3)\n",
    "\n",
    "# Get the neighbourhoods first, if we haven't done so already\n",
    "if not os.path.isfile(os.path.join(data_dir, \"neighbourhoods.geojson\")):\n",
    "    try:\n",
    "        print(f\"Downloading neighbourhoods from {neighbourhoods_url} \", end=\"\")\n",
    "        urllib.request.urlretrieve(neighbourhoods_url, os.path.join(data_dir, \"neighbourhoods.geojson\") )\n",
    "        print(\" ...done.\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"Error downloading neighbourhoods: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafc7f634971b1e",
   "metadata": {},
   "source": [
    "Open the listings files and create a single pandas dataframe.\n",
    "\n",
    "Note that I drop rows that are identical, but there are still some properties that are duplicated because the desciption, name, or neighbourhood overview change. I leave them in for now but could just drop those with identical `host_id`, keeping only the most recent one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4fb22f092b4b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:48.143364Z",
     "start_time": "2025-06-11T13:45:47.521012Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for d in dates:\n",
    "    dfs.append(pd.read_csv(os.path.join(data_dir, d, \"listings.csv.gz\")))\n",
    "full_listings_df = pd.concat(dfs)\n",
    "\n",
    "# Only interested in some columns\n",
    "listings_df = full_listings_df.loc[:,['id', 'name', 'description', 'neighborhood_overview', 'host_id', 'latitude', 'longitude']]\n",
    "\n",
    "# Drop rows that are identical\n",
    "listings_df = listings_df.drop_duplicates()\n",
    "\n",
    "# Drop rows that have no property or neighbourhood description\n",
    "listings_df = listings_df.dropna(subset=['description', 'neighborhood_overview'], how=\"any\")\n",
    "\n",
    "# Concatenate the neighbourhood and property descriptions into single text\n",
    "listings_df['text'] = listings_df['description'] + \" \" + listings_df['neighborhood_overview']\n",
    "\n",
    "listings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8acb37cb0a850c",
   "metadata": {},
   "source": [
    "Useful to see the full set of listings data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901a62175641fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:48.197076Z",
     "start_time": "2025-06-11T13:45:48.178050Z"
    }
   },
   "outputs": [],
   "source": [
    "full_listings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfb69e957bed9f",
   "metadata": {},
   "source": [
    "## Read and prepare the Greater Manchester Gentrification Index data and LSOA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a1df89552e45b",
   "metadata": {},
   "source": [
    "GMGI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b77b67eb8c1e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:48.485770Z",
     "start_time": "2025-06-11T13:45:48.463633Z"
    }
   },
   "outputs": [],
   "source": [
    "gmgi = pd.read_csv(\"../data/gmgi_data/lsoa_summary_jan25.csv\")\n",
    "gmgi = gmgi.iloc[:,1:]  # Drop the first column\n",
    "gmgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55115077dd09c1b",
   "metadata": {},
   "source": [
    "RLSOA data (from the [ONS Open Geography portal](https://geoportal.statistics.gov.uk/)).\n",
    "Annoyingly can't find data just for GM, so extract from larger E&W set. Only run this once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c233b0df260b1e",
   "metadata": {},
   "source": [
    "Read the LSOA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620527dc05299a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:49.286770Z",
     "start_time": "2025-06-11T13:45:48.736955Z"
    }
   },
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file('../data/LSOAs_2011/LSOA_2011_EW_BSC_V4.shp')\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA11NMW'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "gm_lsoa.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0b5f68b4262e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:49.554727Z",
     "start_time": "2025-06-11T13:45:49.552028Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NO LONGER RUNNING THIS, BUT CAN'T FIGURE OUT HOW TO MAKE IT raw ON PYCHARM\n",
    "\n",
    "# ChatGPD says these are the Local Authority District codes for all LADs in Greater Manchester\n",
    "greater_manchester_lads = [\n",
    "    \"E08000001\",  # Bolton\n",
    "    \"E08000002\",  # Bury\n",
    "    \"E08000003\",  # Manchester\n",
    "    \"E08000004\",  # Oldham\n",
    "    \"E08000005\",  # Rochdale\n",
    "    \"E08000006\",  # Salford\n",
    "    \"E08000007\",  # Stockport\n",
    "    \"E08000008\",  # Tameside\n",
    "    \"E08000009\",  # Trafford\n",
    "    \"E08000010\",  # Wigan\n",
    "]\n",
    "# Lookup\n",
    "lookup = pd.read_csv(\"/Users/geonsm/research/projects/current/integrate/data/Lower_Layer_Super_Output_Area_(2021)_to_Ward_(2023)_to_LAD_(2023)_Lookup_in_England_and_Wales.csv\")\n",
    "\n",
    "# Need to extract Greater Manchester from E&W\n",
    "ew_lsoa = gpd.read_file(\"/Users/geonsm/research/projects/current/integrate/data/Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BSC_V4_-4299016806856585929.geojson\")\n",
    "# Merge LSOA geometries with LAD codes\n",
    "ew_lsoa_with_lad = ew_lsoa.merge(lookup[['LSOA21CD', 'LAD23CD']], on='LSOA21CD')\n",
    "# Filter for Greater Manchester LADs\n",
    "gm_lsoa = ew_lsoa_with_lad[ew_lsoa_with_lad['LAD23CD'].isin(greater_manchester_lads)]\n",
    "\n",
    "# Plot to check it looks OK\n",
    "ax = ew_lsoa.plot(color='lightgrey', linewidth=0.1, edgecolor='white', figsize=(10, 10))\n",
    "gm_lsoa.plot(ax=ax, color='red', linewidth=0.2, edgecolor='black')\n",
    "plt.show()\n",
    "\n",
    "# It's fine, save it\n",
    "gm_lsoa.to_file(os.path.join(data_dir, \"greater_manchester_lsoas.geojson\"), driver=\"GeoJSON\")\n",
    "\n",
    "gm_lsoa = gpd.read_file(os.path.join(data_dir, \"greater_manchester_lsoas.geojson\"))\n",
    "gm_lsoa\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3fe4280365960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:49.661629Z",
     "start_time": "2025-06-11T13:45:49.658925Z"
    }
   },
   "outputs": [],
   "source": [
    "gm_lsoa.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537797d6c1a41cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:49.843917Z",
     "start_time": "2025-06-11T13:45:49.702206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach GMGI columns to lsoa data\n",
    "gm_gmgi_lsoa = pd.merge(left=gm_lsoa, right=gmgi, left_on=\"LSOA11CD\", right_on=\"LSOA11CD\")\n",
    "# Map the gentrification index (sanity check)\n",
    "gm_gmgi_lsoa.plot(column=\"gi_n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38844adbdbf64a09",
   "metadata": {},
   "source": [
    "Attach GI to the airbnb data based on spatial location of the parent LSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabf5e4d8f1f1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:50.036218Z",
     "start_time": "2025-06-11T13:45:49.870115Z"
    }
   },
   "outputs": [],
   "source": [
    "listings_gdf = gpd.GeoDataFrame(\n",
    "    listings_df,\n",
    "    geometry=[Point(xy) for xy in zip(listings_df['longitude'], listings_df['latitude'])],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "listings_gdf.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e5cc453d58cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:50.374935Z",
     "start_time": "2025-06-11T13:45:50.057706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spatial join: adds LSOA info to each point based on which polygon it falls in\n",
    "listings_gdf_gmgi = gpd.sjoin(\n",
    "    listings_gdf,\n",
    "    gm_gmgi_lsoa,\n",
    "    how=\"left\",        # keep all listings, add LSOA where matched\n",
    "    predicate=\"within\" # or \"intersects\" if you prefer\n",
    ")\n",
    "\n",
    "# Plot with the origina LSOA index scores too\n",
    "ax = gm_gmgi_lsoa.plot(column=\"gi_n\")\n",
    "listings_gdf_gmgi.plot(\n",
    "    ax=ax,\n",
    "    column=\"gi_n\",\n",
    "    markersize=15,          # smaller points\n",
    "    edgecolor=\"black\",      # black outline\n",
    "    linewidth=0.2,          # outline thickness\n",
    "    legend=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507311b657442ad7",
   "metadata": {},
   "source": [
    "gInteractive plot to check it is sensible (thanks chatgpt, I've not even read this code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4e5f016960916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:53.579471Z",
     "start_time": "2025-06-11T13:45:50.412273Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Interactive GI‑index map for Greater Manchester — Folium version\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 1.  Clean & re‑project ------------------------------------------------------\n",
    "# ‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑‑\n",
    "gm_gmgi_lsoa_temp      = gm_gmgi_lsoa.dropna(subset=[\"gi_n\"]).to_crs(4326)\n",
    "listings_gdf_gmgi_temp = listings_gdf_gmgi.dropna(subset=[\"gi_n\"]).to_crs(4326)\n",
    "\n",
    "# 2.  Base map centred on the study area -------------------------------------\n",
    "centre = gm_gmgi_lsoa_temp.unary_union.centroid\n",
    "m = folium.Map(\n",
    "    location=[centre.y, centre.x],\n",
    "    zoom_start=10,\n",
    "    tiles=\"cartodbpositron\"\n",
    ")\n",
    "\n",
    "# 3.  Continuous colour scale (monotonic index avoids the ValueError) ---------\n",
    "min_gi, max_gi = gm_gmgi_lsoa_temp[\"gi_n\"].min(), gm_gmgi_lsoa_temp[\"gi_n\"].max()\n",
    "cmap = cm.LinearColormap(\n",
    "    colors=[\"#fee8c8\", \"#a1dab4\", \"#41b6c4\", \"#2c7fb8\", \"#253494\"],\n",
    "    index=np.linspace(min_gi, max_gi, 5),   # strictly increasing\n",
    "    vmin=min_gi,\n",
    "    vmax=max_gi,\n",
    "    caption=\"Green‑Infrastructure index (GI_n)\"\n",
    ")\n",
    "\n",
    "# 4.  LSOA polygons layer -----------------------------------------------------\n",
    "folium.GeoJson(\n",
    "    gm_gmgi_lsoa_temp,\n",
    "    style_function=lambda feat: {\n",
    "        \"fillColor\"   : cmap(feat[\"properties\"][\"gi_n\"]),\n",
    "        \"color\"       : \"black\",\n",
    "        \"weight\"      : 0.3,\n",
    "        \"fillOpacity\" : 0.7\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"LSOA11CD\", \"gi_n\"],\n",
    "        aliases=[\"LSOA\", \"GI_n\"],\n",
    "        sticky=False\n",
    "    ),\n",
    "    name=\"LSOA GI scores\"\n",
    ").add_to(m)\n",
    "\n",
    "# 5.  Listing point markers ---------------------------------------------------\n",
    "for _, row in listings_gdf_gmgi_temp.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.geometry.y, row.geometry.x],\n",
    "        radius      = 4,\n",
    "        color       = \"black\",\n",
    "        weight      = 0.4,\n",
    "        fill        = True,\n",
    "        fill_color  = cmap(row[\"gi_n\"]),\n",
    "        fill_opacity= 0.9,\n",
    "        popup       = folium.Popup(f\"GI_n = {row['gi_n']:.2f}\", show=False)\n",
    "    ).add_to(m)\n",
    "\n",
    "# 6.  Legend & controls -------------------------------------------------------\n",
    "cmap.add_to(m)\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "# cleanup\n",
    "del gm_gmgi_lsoa_temp, listings_gdf_gmgi_temp\n",
    "\n",
    "# 7.  Display inline in Jupyter / IPython ------------------------------------\n",
    "m\n",
    "\n",
    "# If you want a standalone file instead (or as well):\n",
    "# m.save(\"gm_gmgi_interactive_map.html\")\n",
    "# display(m._repr_html_())   # show inline *and* write file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6d77ce7e01a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29520f88ecc2ed04",
   "metadata": {},
   "source": [
    "## Calculate the Airbnb embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d46a8372fb13",
   "metadata": {},
   "source": [
    "TODO: have a look for the most appropriate embeddings model, this is just a ChatGPT recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80267c92d17ab724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:56.589819Z",
     "start_time": "2025-06-11T13:45:54.283707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a long-text-capable model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "embeddings_cache = \"../data/airbnb-manchester/cached_embeddings.npz\"\n",
    "if os.path.exists(embeddings_cache):\n",
    "    print(f\"Loading embeddings from {embeddings_cache}\")\n",
    "    embeddings = np.load(embeddings_cache)[\"embeddings\"]\n",
    "else:\n",
    "    print(f\"Calculating embeddings\")\n",
    "    embeddings = model.encode(listings_df['text'].tolist(), show_progress_bar=True)\n",
    "    np.savez_compressed(embeddings_cache, embeddings=embeddings)\n",
    "\n",
    "assert len(embeddings) == len(listings_df), \"The number of embeddings does not match the number of listings.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633cf99fc6e1c71",
   "metadata": {},
   "source": [
    "Run a PCA to convert these embeddings into size 3, so that they can be mapped to RGB colour space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa81248f2d3ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:56.680035Z",
     "start_time": "2025-06-11T13:45:56.616283Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA → 3 components\n",
    "X = np.asarray(embeddings)\n",
    "xyz = PCA(n_components=3, random_state=0).fit_transform(X)\n",
    "\n",
    "# Scale to [0, 1] then [0, 255] for RGB\n",
    "xyz_min, xyz_max = xyz.min(axis=0), xyz.max(axis=0)\n",
    "rgb_255 = ((xyz - xyz_min) / (xyz_max - xyz_min + 1e-9) * 255).astype(int)\n",
    "\n",
    "# Hex colors\n",
    "hex_colors = [f'#{r:02x}{g:02x}{b:02x}' for r, g, b in rgb_255]\n",
    "\n",
    "# Attach to DataFrame\n",
    "listings_df = listings_df.copy()\n",
    "listings_df['color'] = hex_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a047eb78bb98dea",
   "metadata": {},
   "source": [
    "Static map of the PCA embedding colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d37044ec7a326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:56.809601Z",
     "start_time": "2025-06-11T13:45:56.807778Z"
    }
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.scatter(listings_df['longitude'], listings_df['latitude'],\n",
    "#            c=listings_df['color'], s=10)\n",
    "#plt.xlabel('Longitude')\n",
    "#plt.ylabel('Latitude')\n",
    "#plt.title('Listings coloured by embedding (PCA→RGB)')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c97ff65e6a5abf",
   "metadata": {},
   "source": [
    "Interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5188b8f2b80edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:57.913018Z",
     "start_time": "2025-06-11T13:45:56.873765Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[listings_df['latitude'].mean(),\n",
    "                         listings_df['longitude'].mean()],\n",
    "               zoom_start=10)\n",
    "for lat, lon, col in zip(listings_df['latitude'],\n",
    "                         listings_df['longitude'],\n",
    "                         listings_df['color']):\n",
    "    folium.CircleMarker([lat, lon], radius=3,\n",
    "                        color=col, fill=True,\n",
    "                        fill_color=col, fill_opacity=0.8).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cb618c8998fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:58.301559Z",
     "start_time": "2025-06-11T13:45:58.298748Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5cb9b5998bac9d",
   "metadata": {},
   "source": [
    "## Random forest to predict gentrification from the embeddings\n",
    "\n",
    "  - Listings data with GMGI: `listings_gdf_gmgi`\n",
    "  - Associated matrix of embeddings: `embeddings`\n",
    "\n",
    "Use k-fold crosss validation and tune the hyper parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0305aa2c-0d63-4a88-85f1-1524a39a46b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T08:52:22.972751Z",
     "start_time": "2025-05-15T08:52:05.391919Z"
    }
   },
   "source": [
    "# --- Prepare data ---\n",
    "\n",
    "# There are na values (problem with LSOAs above) so filter these out temporarily\n",
    "valid_mask = ~listings_gdf_gmgi[\"gi_n\"].isna()\n",
    "print(f\"{len(listings_gdf_gmgi[valid_mask])} valid listings found\")\n",
    "\n",
    "X = embeddings[valid_mask.values]  # shape: (n_samples, n_features)\n",
    "y = listings_gdf_gmgi.loc[valid_mask, \"gi_n\"].values  # target: gentrification score\n",
    "assert len(X) == len(y) # checked earlier, just making sure\n",
    "\n",
    "# --- K-Fold Cross-Validation ---\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    print(f\"Fold {fold} - MSE: {mse:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage MSE over {k} folds: {np.mean(mse_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73939f6b-5087-4aa6-9a9e-13f11e9251d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:58.441237Z",
     "start_time": "2025-06-11T13:45:58.360013Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Prepare data ---\n",
    "\n",
    "## There are na values (problem with LSOAs above) so filter these out temporarily\n",
    "#valid_mask = ~listings_gdf_gmgi[\"gi_n\"].isna()\n",
    "#print(f\"{len(listings_gdf_gmgi[valid_mask])} valid listings found\")\n",
    "\n",
    "#X = embeddings[valid_mask.values]  # shape: (n_samples, n_features)\n",
    "X = embeddings\n",
    "#y = listings_gdf_gmgi.loc[valid_mask, \"gi_n\"].values  # target: gentrification score\n",
    "y = listings_gdf_gmgi.loc[:, \"gi_n\"].values  # target: gentrification score\n",
    "assert len(X) == len(y) # checked earlier, just making sure\n",
    "\n",
    "# Split data into training (used for paramter tuning) and hold-out test sets (for testing afterwards)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Find the best model after hyper parameter tuning ---\n",
    "\n",
    "model_path = os.path.join(\"..\", \"data\", \"airbnb-manchester\", \"cached_rf_gridsearch.pkl\")  # Optionally cache the final model\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading cached model...\")\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        grid_search = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"Training model, this will take some time\")          \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],         # Number of trees in the forest. More trees = better performance, but slower.\n",
    "        'max_depth': [None, 10, 20],        # Maximum depth of each tree. None allows nodes to expand until all leaves are pure.\n",
    "        'min_samples_split': [2, 5],        # Minimum number of samples required to split an internal node.\n",
    "        'min_samples_leaf': [1, 2],         # Minimum number of samples required to be at a leaf node.\n",
    "        'max_features': ['sqrt', 'log2'],   # Number of features to consider when looking for the best split.\n",
    "        'bootstrap': [True, False]          # Whether bootstrap samples are used when building trees.\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf,\n",
    "        param_grid,\n",
    "        cv=5,  # k for cross-validation\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,  # Run on all cores\n",
    "        verbose = 1  # Show progress (not as good as a progress bar, but much simpler)\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(grid_search, f)\n",
    "    print(\"Model trained and saved to cache.\")\n",
    "\n",
    "print(\"Param grid: \", grid_search.param_grid)  # To check the cached one is correct (compare to grid defined above)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best score (neg MSE):\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55cac596c21af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:58.518827Z",
     "start_time": "2025-06-11T13:45:58.474687Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Predict and evaluate ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bb747b2-4219-4c08-ba87-fdafe09fd423",
   "metadata": {},
   "source": [
    "TEMP: Previous scores (before hyper parameter tuning but i)\n",
    "Mean Squared Error: 86.0931\n",
    "R² Score: 0.7383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa2eba1eb53825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:58.613945Z",
     "start_time": "2025-06-11T13:45:58.553496Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot predicted vs actual ---\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "plt.xlabel(\"Actual gi_n\")\n",
    "plt.ylabel(\"Predicted gi_n\")\n",
    "plt.title(\"Random Forest Predictions\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc5800fd658022",
   "metadata": {},
   "source": [
    "Cool. Now map the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8639f63c08c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:58.766402Z",
     "start_time": "2025-06-11T13:45:58.646788Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X)  # Predict over the whole dataset (not just the test)\n",
    "listings_gdf_gmgi['gi_pred'] = predictions\n",
    "listings_gdf_gmgi['gi_pred_error'] = predictions - listings_gdf_gmgi['gi_n']\n",
    "\n",
    "listings_gdf_gmgi['gi_pred_error'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513aba0e18964c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:45:59.003827Z",
     "start_time": "2025-06-11T13:45:58.796159Z"
    }
   },
   "outputs": [],
   "source": [
    "listings_gdf_gmgi.plot(\n",
    "    column=\"gi_pred_error\",\n",
    "    markersize=2,          # smaller points\n",
    "    legend=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2aa80452f6d50a",
   "metadata": {},
   "source": [
    "I'm concerned that because the embedding matrix is so large that the model may be over-specified.\n",
    "\n",
    "Sanity check: can I fit any RF to a dataset with a large number of parameters (embedding dimension) and a relatively small number of observations (listings)?\n",
    "\n",
    "To test, create a random embeding matrix and re-calculate the model.\n",
    "\n",
    "ChatGPT generated the code below. Seems like the model doesn't work, which is good news!\n",
    "\n",
    "_The random model is very poor, so the gentrification model may be OK_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98fb1d06b6abb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee8bc4a56813e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T13:46:36.422054Z",
     "start_time": "2025-06-11T13:45:59.032793Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1.  Create a reproducible random‑embedding matrix (same shape)\n",
    "# ------------------------------------------------------------------\n",
    "rng = np.random.RandomState(42)\n",
    "random_embeddings = rng.normal(size=embeddings.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Prepare data (only rows with a valid GI score)\n",
    "# ------------------------------------------------------------------\n",
    "X_rand = random_embeddings\n",
    "#X_rand = random_embeddings[valid_mask.values]\n",
    "y_rand = listings_gdf_gmgi.loc[:, \"gi_n\"].values\n",
    "#y_rand = listings_gdf_gmgi.loc[valid_mask, \"gi_n\"].values\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_rand, y_rand, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Train a random‑forest model\n",
    "# ------------------------------------------------------------------\n",
    "rf_rand = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_rand.fit(X_train_r, y_train_r)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Evaluate performance\n",
    "# ------------------------------------------------------------------\n",
    "y_pred_r = rf_rand.predict(X_test_r)\n",
    "mse_r = mean_squared_error(y_test_r, y_pred_r)\n",
    "r2_r = r2_score(y_test_r, y_pred_r)\n",
    "\n",
    "print(f\"Random embeddings baseline — MSE: {mse_r:.4f}\")\n",
    "print(f\"Random embeddings baseline — R²:  {r2_r:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Plot predicted vs. actual\n",
    "# ------------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_r, y_pred_r, alpha=0.5)\n",
    "plt.plot(\n",
    "    [y_rand.min(), y_rand.max()],\n",
    "    [y_rand.min(), y_rand.max()],\n",
    "    linestyle=\"--\", linewidth=1\n",
    ")\n",
    "plt.xlabel(\"Actual gi_n\")\n",
    "plt.ylabel(\"Predicted gi_n (random)\")\n",
    "plt.title(\"Random‑embedding baseline\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7868cb70b29a0",
   "metadata": {},
   "source": [
    "## NN to predict gentriciation\n",
    "\n",
    "Repeat with a neural network to see whether it is any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f161da4d123de2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:45.475478Z",
     "start_time": "2025-06-11T13:46:36.461211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model cache path\n",
    "model_path = os.path.join(\"..\", \"data\", \"airbnb-manchester\", \"cached_nn_gridsearch.pkl\")\n",
    "\n",
    "# Load or train model\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading cached model...\")\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        grid_search_nn = pickle.load(f)\n",
    "else:\n",
    "    print(\"Training neural network...\")\n",
    "\n",
    "    # Grid search parameters.\n",
    "    # (The first version was when we used a NN directly, rather than in a pipeline\n",
    "    # with a standard scaler)\n",
    "    #param_grid = {\n",
    "    #    'hidden_layer_sizes': [\n",
    "    #        (50,), (100,), (100, 50), (200,), (200, 100), (100, 100)\n",
    "    #    ],\n",
    "    #    'activation': ['relu', 'tanh'],\n",
    "    #    'alpha': [1e-5, 1e-4, 1e-3],  # L2 regularization\n",
    "    #    'learning_rate_init': [0.0001, 0.001, 0.01],\n",
    "    #    'solver': ['adam', 'lbfgs'],  # Try both optimizers (lbfgs often works better for smaller datasets)\n",
    "    #}\n",
    "    param_grid = {\n",
    "        'mlp__hidden_layer_sizes': [\n",
    "            (50,), (100,), (100, 50), (200,), (200, 100), (100, 100)\n",
    "        ],\n",
    "        'mlp__activation': ['relu', 'tanh'],\n",
    "        'mlp__alpha': [1e-5, 1e-4, 1e-3],\n",
    "        'mlp__learning_rate_init': [0.0001, 0.001, 0.01],\n",
    "        'mlp__solver': ['adam', 'lbfgs'],\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create the NN. Run the data through a standard scaler.\n",
    "    #nn = MLPRegressor(max_iter=1000, random_state=42)\n",
    "    nn = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPRegressor(max_iter=2000, random_state=42))\n",
    "    ])\n",
    "\n",
    "    grid_search_nn = GridSearchCV(\n",
    "        nn,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search_nn.fit(X_train, y_train)\n",
    "\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(grid_search_nn, f)\n",
    "    print(\"Model trained and saved to cache.\")\n",
    "\n",
    "# Print best model info\n",
    "print(\"Best params:\", grid_search_nn.best_params_)\n",
    "print(\"Best score (neg MSE):\", grid_search_nn.best_score_)\n",
    "\n",
    "best_model_nn = grid_search_nn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ddf1c6922aff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:45.579752Z",
     "start_time": "2025-06-11T15:24:45.572248Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Predict and evaluate ---\n",
    "best_model_nn = grid_search_nn.best_estimator_\n",
    "y_pred_nn = best_model_nn.predict(X_test)\n",
    "\n",
    "mse_nn = mean_squared_error(y_test, y_pred)\n",
    "r2_nn = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse_nn:.4f}\")\n",
    "print(f\"R² Score: {r2_nn:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44031da30d8e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:45.967845Z",
     "start_time": "2025-06-11T15:24:45.906936Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot predicted vs actual ---\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_nn, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "plt.xlabel(\"Actual gi_n\")\n",
    "plt.ylabel(\"Predicted gi_n\")\n",
    "plt.title(\"Neural Network Predictions\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486615f08dfdc3",
   "metadata": {},
   "source": [
    "It doesn't work as well as the random forest, lets use that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c27634ae182046",
   "metadata": {},
   "source": [
    "# Apply the Manchester Model to San Francisco\n",
    "\n",
    "The model that predicts a gentrification in Manchester works very well. Lets see if it works in San Francicso (a place with a history of gentrification-related research and abundant data).\n",
    "\n",
    "Method:\n",
    "\n",
    " - Download AirBnB data for San Francisco\n",
    " - Calculate the embeddings\n",
    " - Use the previous model to predict gentrification\n",
    " - Compare to San Francisco gentrification data, i.e. Landis, UDP and Freeman classifications published [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC6616964/). (_Note that those are categorical classification so will need to convert model output scores to categories_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22b3fbb04a4ecc",
   "metadata": {},
   "source": [
    "## San Francisco AirBnB data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c331ecc4d9b732f",
   "metadata": {},
   "source": [
    "Download the data from Inside Airbnb (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c368bde8b26191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:46.389788Z",
     "start_time": "2025-06-11T15:24:46.384468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same code as used to download / obtain the Manchester data\n",
    "\n",
    "dates = [ '2025-03-01', '2024-12-04', '2024-09-04', '2024-06-04' ]\n",
    "files = [\"listings.csv.gz\", \"reviews.csv.gz\"]\n",
    "data_dir = os.path.join(\"..\", \"data\", \"airbnb-sanfrancisco\")\n",
    "root_url = \"https://data.insideairbnb.com/united-states/ca/san-francisco/\"\n",
    "neighbourhoods_url = \"https://data.insideairbnb.com/united-states/ca/san-francisco/2025-03-01/visualisations/neighbourhoods.geojson\"\n",
    "\n",
    "\n",
    "\n",
    "for d in dates:\n",
    "    # Check if d is already a directory\n",
    "    if os.path.isdir(os.path.join(data_dir, d)):\n",
    "        print(f\"Directory {d} already exists.\")\n",
    "        continue\n",
    "    # It isn't, so create it and download the data\n",
    "    os.makedirs(os.path.join(data_dir, d), exist_ok=True)\n",
    "    print(f\"Downloading data for {d} ...\")\n",
    "    for f in files:\n",
    "        url = f\"{root_url}{d}/data/{f}\"\n",
    "        try:\n",
    "            print(f\"\\tDownloading {f} from {url} \", end=\"\")\n",
    "            urllib.request.urlretrieve(url, os.path.join(data_dir, d, f))\n",
    "            print(\"...done.\")\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"Error downloading {f} for {d}: {e}\")\n",
    "        # Sleep for a few seconds so not to abuse their server\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "# Get the neighbourhoods first, if we haven't done so already\n",
    "if not os.path.isfile(os.path.join(data_dir, \"neighbourhoods.geojson\")):\n",
    "    try:\n",
    "        print(f\"Downloading neighbourhoods from {neighbourhoods_url} \", end=\"\")\n",
    "        urllib.request.urlretrieve(neighbourhoods_url, os.path.join(data_dir, \"neighbourhoods.geojson\") )\n",
    "        print(\" ...done.\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"Error downloading neighbourhoods: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550955f3d8bd81cc",
   "metadata": {},
   "source": [
    "Read the data\n",
    "\n",
    "https://data.insideairbnb.com/united-states/ca/san-francisco/2024-09-04/data/listings.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8388f4f4280d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:47.315085Z",
     "start_time": "2025-06-11T15:24:46.542059Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for d in dates:\n",
    "    dfs.append(pd.read_csv(os.path.join(data_dir, d, \"listings.csv.gz\")))\n",
    "sf_full_listings_df = pd.concat(dfs)\n",
    "\n",
    "# Only interested in some columns\n",
    "sf_listings_df = sf_full_listings_df.loc[:,['id', 'name', 'description', 'neighborhood_overview', 'host_id', 'latitude', 'longitude']].drop_duplicates().dropna(subset=['description', 'neighborhood_overview'], how=\"any\")\n",
    "\n",
    "# Concatenate the neighbourhood and property descriptions into single text\n",
    "sf_listings_df['text'] = sf_listings_df['description'] + \" \" + sf_listings_df['neighborhood_overview']\n",
    "\n",
    "sf_listings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ba190b110a601",
   "metadata": {},
   "source": [
    "Spatialise the listings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa83086dc3b403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:47.636474Z",
     "start_time": "2025-06-11T15:24:47.424249Z"
    }
   },
   "outputs": [],
   "source": [
    "sf_listings_gdf = gpd.GeoDataFrame(\n",
    "    sf_listings_df,\n",
    "    geometry=[Point(xy) for xy in zip(sf_listings_df['longitude'], sf_listings_df['latitude'])],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "sf_listings_gdf.plot(markersize=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df78be0888c9829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:48.138513Z",
     "start_time": "2025-06-11T15:24:48.137044Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ae784b949b8c8",
   "metadata": {},
   "source": [
    "## Calculate the SF embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6bd59dcfa233e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T20:49:13.371776Z",
     "start_time": "2025-06-11T20:49:13.233159Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Using model {model}\")\n",
    "\n",
    "embeddings_cache = \"../data/airbnb-sanfrancisco/cached_embeddings.npz\"\n",
    "if os.path.exists(embeddings_cache):\n",
    "    print(f\"Loading embeddings from {embeddings_cache}\")\n",
    "    sf_embeddings = np.load(embeddings_cache)[\"embeddings\"]\n",
    "else:\n",
    "    print(f\"Calculating embeddings\")\n",
    "    sf_embeddings = model.encode(sf_listings_df['text'].tolist(), show_progress_bar=True)\n",
    "    np.savez_compressed(embeddings_cache, embeddings=sf_embeddings)\n",
    "\n",
    "assert len(sf_embeddings) == len(sf_listings_df), \\\n",
    "    \"The number of embeddings does not match the number of listings. Do you need to recreate the cache?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e52fa1aeb64e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:49.252288Z",
     "start_time": "2025-06-04T10:06:59.372671Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2820f5845077d49a",
   "metadata": {},
   "source": [
    "Visualise the embeddings (map PCA reduction to three components: R, G, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e2aa6bb136fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T20:49:19.421967Z",
     "start_time": "2025-06-11T20:49:19.277160Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA → 3 components\n",
    "X = np.asarray(sf_embeddings)\n",
    "xyz = PCA(n_components=3, random_state=0).fit_transform(X)\n",
    "\n",
    "# Scale to [0, 1] then [0, 255] for RGB\n",
    "xyz_min, xyz_max = xyz.min(axis=0), xyz.max(axis=0)\n",
    "rgb_255 = ((xyz - xyz_min) / (xyz_max - xyz_min + 1e-9) * 255).astype(int)\n",
    "\n",
    "# Hex colors\n",
    "hex_colors = [f'#{r:02x}{g:02x}{b:02x}' for r, g, b in rgb_255]\n",
    "\n",
    "# Attach to DataFrame\n",
    "sf_listings_df['color'] = hex_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91504eaf540cf311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:50.171868Z",
     "start_time": "2025-06-04T10:06:59.553096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Static map\n",
    "\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#plt.scatter(sf_listings_df['longitude'], sf_listings_df['latitude'],\n",
    "#            c=sf_listings_df['color'], s=10)\n",
    "#plt.xlabel('Longitude')\n",
    "#plt.ylabel('Latitude')\n",
    "#plt.title('Listings coloured by embedding (PCA→RGB)')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7c0ec65adb67b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T20:49:24.133461Z",
     "start_time": "2025-06-11T20:49:22.645180Z"
    }
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[sf_listings_df['latitude'].mean(),\n",
    "                         sf_listings_df['longitude'].mean()],\n",
    "               zoom_start=13)\n",
    "for lat, lon, col in zip(sf_listings_df['latitude'],\n",
    "                         sf_listings_df['longitude'],\n",
    "                         sf_listings_df['color']):\n",
    "    folium.CircleMarker([lat, lon], radius=2,\n",
    "                        color=col, fill=True,\n",
    "                        fill_color=col, fill_opacity=0.8).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78578393fbf877f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:51.100294Z",
     "start_time": "2025-06-04T10:07:00.796804Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186fca63f5dc4813",
   "metadata": {},
   "source": [
    "## Predict SF gentrification\n",
    "\n",
    "Start by predicting scores, then convert these into categories so that they can be compared to the published data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d46c954cf6ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T06:05:26.377889Z",
     "start_time": "2025-06-12T06:05:26.284637Z"
    }
   },
   "outputs": [],
   "source": [
    "sf_gentrification_pred = best_model.predict(sf_embeddings)\n",
    "\n",
    "sf_listings_df['gentrification_pred'] = sf_gentrification_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb541fa7f63906ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T06:05:47.547474Z",
     "start_time": "2025-06-12T06:05:46.077636Z"
    }
   },
   "outputs": [],
   "source": [
    "var_to_map = 'gentrification_pred'  # To make less repetition later\n",
    "\n",
    "# Create a linear colormap\n",
    "colormap = cm.linear.viridis.scale(\n",
    "    min(sf_listings_df[var_to_map]), max(sf_listings_df[var_to_map])\n",
    ")\n",
    "\n",
    "# Create map\n",
    "m = folium.Map(location=[sf_listings_df['latitude'].mean(),\n",
    "                         sf_listings_df['longitude'].mean()],\n",
    "               zoom_start=13)\n",
    "\n",
    "# Add CircleMarkers with mapped colors\n",
    "for lat, lon, val in zip(sf_listings_df['latitude'],\n",
    "                         sf_listings_df['longitude'],\n",
    "                         sf_listings_df[var_to_map]):\n",
    "    folium.CircleMarker([lat, lon], radius=2,\n",
    "                        color=colormap(val),\n",
    "                        fill=True, fill_color=colormap(val),\n",
    "                        fill_opacity=0.8).add_to(m)\n",
    "\n",
    "# Optionally add the colormap legend to the map\n",
    "colormap.caption = 'Gentrification Score'\n",
    "colormap.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611e858d421ad71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:51.346590Z",
     "start_time": "2025-06-11T12:34:54.244842Z"
    }
   },
   "outputs": [],
   "source": [
    "sf_gentrification_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42b24d6fa37c71",
   "metadata": {},
   "source": [
    "## Compare SF gentrification categories to real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c824fd90f9e3d19",
   "metadata": {},
   "source": [
    "XXXX HERE Try to get the data from this publication and somehow(?) compare it to these scores\n",
    "\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC6616964/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24da5bdbc0a32f8",
   "metadata": {},
   "source": [
    "In the [Mujahid paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC6616964/), the Landis and Freeman are largely consistent (or at least somewhat consistent) whereas the UDP is quite different. So we compare our SF prediction to Landis and Freeman. This isn't ideal because those are categorical classifications whereas the Manchester one is numeric, so we simply divide our data into decile ranges that loosely represent the categories.\n",
    "\n",
    "![Gentrification classifications (from Mojahid et al. (2019))](img/sf_gentrification.png)\n",
    "\n",
    "The following table shows the Landis and Freeman classification categories, and how we divide the Manchester index to losely correspond to those:\n",
    "\n",
    "| Name | Landis      | Freeman     | Manchester Index Decile Range |\n",
    "|------|-------------|-------------|-------------------------------|\n",
    "| B    | Stable      | Stable      | 30-59                         |\n",
    "| A    | Declining   | Excluded    | 0-29                          |\n",
    "| C    | Gentrifying | Gentrifying | 59-100                        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475c13c4221779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T06:13:04.629665Z",
     "start_time": "2025-06-12T06:13:04.622262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorise depending on prediction\n",
    "\n",
    "# Define thresholds and labels\n",
    "thresholds = [0, 30, 70, 100]  # Adjust these as needed\n",
    "labels = ['A', 'B', 'C']\n",
    "\n",
    "# Convert deciles to quantile values\n",
    "quantile_values = [t / 100 for t in thresholds]\n",
    "\n",
    "# Compute the actual threshold values from the data\n",
    "cut_points = sf_listings_df['gentrification_pred'].quantile(quantile_values).values\n",
    "print(f\"Cut points: {cut_points}\")\n",
    "\n",
    "# Assign categories\n",
    "sf_listings_df['gentrification_cat'] = pd.cut(\n",
    "    sf_listings_df['gentrification_pred'],\n",
    "    bins=cut_points,\n",
    "    labels=labels,\n",
    "    include_lowest=True,\n",
    "    right=False  # to match \"0-29\", \"30-69\", \"70-100\"\n",
    ")\n",
    "sf_listings_df.loc[:,['gentrification_pred', 'gentrification_cat', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe528b72429c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T06:13:12.107507Z",
     "start_time": "2025-06-12T06:13:10.285075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map the categories (thanks ChatGPT)\n",
    "\n",
    "var_to_map = 'gentrification_cat'\n",
    "\n",
    "# Get unique categories and assign colors\n",
    "categories = sorted(sf_listings_df[var_to_map].dropna().unique())\n",
    "colormap = branca.colormap.linear.Set1_03.scale(0, len(categories) - 1)\n",
    "color_dict = {cat: colormap(i) for i, cat in enumerate(categories)}\n",
    "\n",
    "# Create the map\n",
    "m = folium.Map(location=[sf_listings_df['latitude'].mean(),\n",
    "                         sf_listings_df['longitude'].mean()],\n",
    "               zoom_start=13)\n",
    "\n",
    "# Add circle markers\n",
    "for _, row in sf_listings_df.iterrows():\n",
    "    val = row[var_to_map]\n",
    "    color = color_dict.get(val, 'gray')\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.8\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add a simple HTML legend\n",
    "legend_items = ''.join(\n",
    "    f'<i style=\"background:{color_dict[c]};width:10px;height:10px;display:inline-block;margin-right:5px;\"></i> {c}<br>'\n",
    "    for c in categories\n",
    ")\n",
    "legend_html = f\"\"\"\n",
    "<div style=\"position: fixed; bottom: 30px; left: 30px; width: 160px;\n",
    "            background-color: white; padding: 10px; border: 1px solid grey; z-index:9999;\">\n",
    "<b>Gentrification Category</b><br>{legend_items}\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf838a15268c2a",
   "metadata": {},
   "source": [
    "I don't think it's working. I.e. the Manchester prediction model doesn't work well in San Francisco.\n",
    "\n",
    "One last thing to try is make a kernel-weighted moving-average map from the raw prediction values to see if this shows a trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39de01c0a43846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T11:27:15.468515Z",
     "start_time": "2025-06-12T11:26:53.787728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Thanks ChatGPT!\n",
    "\n",
    "# --- Convert DataFrame → GeoDataFrame (lat/lon → geometry) ---\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    sf_listings_df,\n",
    "    geometry=gpd.points_from_xy(sf_listings_df['longitude'],\n",
    "                                sf_listings_df['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(3857)  # Need a metric CRS (metres)\n",
    "\n",
    "# --- Kernel-weighted mean surface of gentrification_pred ---\n",
    "coords   = np.stack([gdf.geometry.x, gdf.geometry.y], axis=1)\n",
    "values   = gdf['gentrification_pred'].to_numpy()\n",
    "bw       = 400                                    # metres\n",
    "kde_den  = KernelDensity(bandwidth=bw).fit(coords)\n",
    "kde_num  = KernelDensity(bandwidth=bw).fit(coords, sample_weight=values)\n",
    "\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "#n = 400                                           # grid size\n",
    "n = 100                                           # grid size\n",
    "xx, yy = np.meshgrid(np.linspace(xmin, xmax, n),\n",
    "                     np.linspace(ymin, ymax, n))\n",
    "grid_pts = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "avg = np.exp(kde_num.score_samples(grid_pts) -\n",
    "             kde_den.score_samples(grid_pts)).reshape(n, n)\n",
    "\n",
    "# --- Render RGBA raster in memory ---\n",
    "cmap, norm = plt.cm.viridis, plt.Normalize(avg.min(), avg.max())\n",
    "rgba = (cmap(norm(avg)) * 255).astype(np.uint8)\n",
    "\n",
    "# --- Save PNG with geotransform ---\n",
    "out_png = \"/tmp/gentri_surface.png\"\n",
    "transform = from_bounds(xmin, ymin, xmax, ymax, n, n)\n",
    "with rasterio.open(out_png, \"w\", driver=\"PNG\",\n",
    "                   width=n, height=n, count=4, dtype=rgba.dtype,\n",
    "                   transform=transform, crs=\"EPSG:3857\") as dst:\n",
    "    for i in range(4):\n",
    "        dst.write(rgba[:, :, i], i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8010cd299ac1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T11:28:17.535029Z",
     "start_time": "2025-06-12T11:28:17.496563Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Folium overlay ---\n",
    "m = folium.Map(location=[sf_listings_df['latitude'].mean(),\n",
    "                         sf_listings_df['longitude'].mean()],\n",
    "               zoom_start=13, tiles='CartoDB positron')\n",
    "\n",
    "folium.raster_layers.ImageOverlay(\n",
    "    image=out_png,\n",
    "    bounds=[[gdf.to_crs(4326).geometry.y.min(), gdf.to_crs(4326).geometry.x.min()],\n",
    "            [gdf.to_crs(4326).geometry.y.max(), gdf.to_crs(4326).geometry.x.max()]],\n",
    "    opacity=0.5,\n",
    "    name='Avg gentrification_pred',\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ae2a85171f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab5cf709cc56cf5",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac96da78f6e63fa",
   "metadata": {},
   "source": [
    "TODO:\n",
    " - [X] Tune the model\n",
    " - [X] k-Fold test/validation\n",
    " - [X] Ranger RF (implementation in scikit-learn)\n",
    " - [ ] Convert above to hex heat map\n",
    "\n",
    "Apply elsewhere - San Francisco (and/or UK)\n",
    " - [London](https://trustforlondon.org.uk/data/gentrification-across-london/) (and through [CDRC>](https://www.cdrc.ac.uk/quantifying-state-led-gentrification-in-london/))\n",
    "\n",
    "Apply to other text (i.e. recalculate embeddings and apply trained RF)\n",
    "  - Twitter? Reddit? Insta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb74cc4d499bbd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:51.771932Z",
     "start_time": "2025-06-04T10:07:02.976154Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2d2f0bad7e2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:52.268106Z",
     "start_time": "2025-06-04T10:07:03.020621Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266aebae505dfb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T15:24:52.278039Z",
     "start_time": "2025-06-04T10:07:03.033019Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
