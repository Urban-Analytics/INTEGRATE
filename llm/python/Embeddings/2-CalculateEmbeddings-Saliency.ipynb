{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import random\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# # Hugging Face Transformers for image embedding\n",
    "from transformers import AutoImageProcessor, AutoModel, ViTModel, ViTImageProcessor, ViTConfig\n",
    "\n",
    "# # ----------------- Configuration -----------------\n",
    "# #np.random.seed(42)\n",
    "# density_per_km = 0.3  # number of points to sample per km of road\n",
    "# #density_per_km = 0.1  # VERY FEW WHILE TESTING\n",
    "DOWNLOAD_IMAGES = False  # If false then don't download any images, just load those that have been cached\n",
    "\n",
    "data_dir = os.path.join(\"../../data/embeddings/\")\n",
    "boundary_file = os.path.join(data_dir, \"greater_manchester_lsoas.geojson\")  # Path to boundary polygon file\n",
    "lsoas_file = os.path.join(\"../../data/SpatialData/\", \"LSOAs_2011\", \"LSOA_2011_EW_BSC_V4.shp\")\n",
    "n_directions = 4         # number of street view images per point (e.g., 4 cardinal directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a13cd0696105",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a11b33a5e00526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.661138Z",
     "start_time": "2025-09-04T09:36:29.529524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged neighbourhoods into single study-area boundary.\n",
      "Bounds: (-2.73052481406758, 53.3281053809015, -1.90962093258169, 53.6857339236763)\n"
     ]
    }
   ],
   "source": [
    "# --- Load neighbourhood polygons and dissolve to one study-area boundary ---\n",
    "boundary_neighs = gpd.read_file(boundary_file)\n",
    "# Make sure we're in WGS84 (lat/lon) for OSM and APIs\n",
    "boundary_neighs = boundary_neighs.to_crs(epsg=4326)\n",
    "# Dissolve: merge all geometries into one polygon (MultiPolygon possible)\n",
    "boundary_polygon = boundary_neighs.union_all()  # shapely (multi)polygon\n",
    "boundary_gdf = gpd.GeoDataFrame(data={'name': ['study_area']},\n",
    "    geometry=[boundary_polygon], crs=boundary_neighs.crs)\n",
    "print(\"Merged neighbourhoods into single study-area boundary.\")\n",
    "print(\"Bounds:\", boundary_polygon.bounds)\n",
    "# boundary_gdf.plot(color='lightblue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c742d-721d-41d7-8287-027c8d4bc523",
   "metadata": {},
   "source": [
    "## Read the LSOA boundary data\n",
    "\n",
    "(later it will be joined to the Greater Manchester Gentrification Index and IMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dc5e31eb629a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA11NMW'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "# gm_lsoa.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached point data â€¦\n",
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir + \"points_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if os.path.isfile(points_data_cache):\n",
    "    print(\"Loading cached point data â€¦\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba1f32-9f79-4985-a01f-6de3b3a0a942",
   "metadata": {},
   "source": [
    "### Map of the full sample (cache + any others just downlaoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b952715d-e889-404b-96be-72e783920876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:17:15.525112Z",
     "start_time": "2025-09-04T11:17:15.239938Z"
    }
   },
   "outputs": [],
   "source": [
    "# if point_records:    \n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "#     # Plot roads first (thin gray lines)\n",
    "#     # edges_gdf.plot(ax=ax, linewidth=0.1, color=\"gray\")\n",
    "\n",
    "#     # Plot the sample points\n",
    "#     gpd.GeoDataFrame(point_records, \n",
    "#                      geometry=[Point(rec[\"longitude\"], rec[\"latitude\"]) for rec in point_records], \n",
    "#                      crs=\"EPSG:4326\").plot(ax=ax, color=\"blue\", markersize=0.2, label=\"Sample Points\")\n",
    "\n",
    "#     # Plot the study-area outline on top (thicker red line)\n",
    "#     boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "#     ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "#     ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "#     ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fe2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True).eval()\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "config = ViTConfig.from_pretrained(model_name, output_attentions=True)\n",
    "model = AutoModel.from_pretrained(model_name, config=config).eval()\n",
    "\n",
    "# Directory for saving heatmaps\n",
    "heatmap_dir = \"saliency_heatmaps\"\n",
    "os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def generate_attention_map(img_path, layer=11):\n",
    "    \"\"\"Return embedding and attention map for a single image.\"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()\n",
    "\n",
    "    # Attention tensor: (num_layers, batch, num_heads, tokens, tokens)\n",
    "    attn = outputs.attentions[layer][0]  # take selected layer, first batch\n",
    "    attn_cls = attn[:, 0, 1:]  # attention from [CLS] to all other patches\n",
    "    attn_mean = attn_cls.mean(0).reshape(14, 14).cpu().numpy()  # 14x14 patches\n",
    "\n",
    "    # Normalize\n",
    "    attn_norm = (attn_mean - attn_mean.min()) / (attn_mean.max() - attn_mean.min())\n",
    "\n",
    "    return cls_embedding, attn_norm\n",
    "\n",
    "\n",
    "def save_and_show_heatmap(img_path, attn_map):\n",
    "    \"\"\"Overlay attention heatmap on original image and save.\"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "    attn_resized = np.array(Image.fromarray((attn_map * 255).astype(np.uint8)).resize((224, 224)))\n",
    "    attn_colored = plt.cm.jet(attn_resized / 255.0)[..., :3]\n",
    "\n",
    "    overlay = 0.6 * np.array(img) / 255.0 + 0.4 * attn_colored\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    # Save to file\n",
    "    base = os.path.basename(img_path).replace(\"/\", \"_\")\n",
    "    out_path = os.path.join(heatmap_dir, f\"attn_{base}.png\")\n",
    "    plt.imsave(out_path, overlay)\n",
    "\n",
    "    # Display in notebook\n",
    "    #plt.figure(figsize=(4, 4))\n",
    "    #plt.imshow(overlay)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.title(base)\n",
    "    # plt.show()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def embed_image_with_saliency(img_path):\n",
    "    \"\"\"Compute embedding + attention stats + heatmap.\"\"\"\n",
    "    cls_embedding, attn_map = generate_attention_map(img_path)\n",
    "    \n",
    "    # Save and display\n",
    "    heatmap_path = save_and_show_heatmap(img_path, attn_map)\n",
    "\n",
    "    # Stats of attention\n",
    "    stats = {\n",
    "        \"mean\": float(attn_map.mean()),\n",
    "        \"std\": float(attn_map.std()),\n",
    "        \"entropy\": float(-np.sum(attn_map * np.log(attn_map + 1e-9))),}\n",
    "    \n",
    "    cy, cx = np.unravel_index(attn_map.argmax(), attn_map.shape)\n",
    "    stats[\"max_y\"], stats[\"max_x\"] = cy, cx\n",
    "\n",
    "    return cls_embedding, stats, heatmap_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1096687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 11805/18897 [2:09:19<1:36:16,  1.23point/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Error processing ../../data/embeddings/street_images/point11958_heading270.jpg: [Errno 13] Permission denied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18897/18897 [3:26:22<00:00,  1.53point/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Created 18897 new embeddings. 0 points had existing embeddings.\n",
      "ðŸ’¾ 18897 embeddings computed and cached.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Embed all images ------------------\n",
    "already_have_embedding = 0\n",
    "\n",
    "for rec in tqdm(point_records, desc=\"Embedding points\", unit=\"point\"):\n",
    "#     if rec.get(\"embedding\") is not None:\n",
    "#         already_have_embedding += 1\n",
    "#         continue\n",
    "\n",
    "    embeds = []\n",
    "    attn_stats_last = None  # store last successful attention stats\n",
    "\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "        img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            embedding, attn_stats, heatmap_path = embed_image_with_saliency(img_path)\n",
    "            attn_stats_last = attn_stats  # remember last good stats\n",
    "            if embedding is not None:\n",
    "                embeds.append(embedding)\n",
    "        except Exception as e:\n",
    "            # tqdm.write prevents interfering with the progress bar\n",
    "            tqdm.write(f\"âš ï¸ Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Assign results\n",
    "    rec[\"embedding\"] = None if not embeds else np.mean(embeds, axis=0)\n",
    "\n",
    "    if attn_stats_last:\n",
    "        rec[\"attn_mean\"] = attn_stats_last.get(\"mean\")\n",
    "        rec[\"attn_entropy\"] = attn_stats_last.get(\"entropy\")\n",
    "        rec[\"attn_std\"] = attn_stats_last.get(\"std\")\n",
    "        rec[\"max_x\"] = attn_stats_last.get(\"max_x\")\n",
    "        rec[\"max_y\"] = attn_stats_last.get(\"max_y\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nâœ… Created {len(point_records)-already_have_embedding} new embeddings. \"\n",
    "      f\"{already_have_embedding} points had existing embeddings.\")\n",
    "\n",
    "# Cache\n",
    "points_data_cache_with_embeddings = \"points_data_cache_with_embeddings_and_attn_stats.pkl\"\n",
    "with open(points_data_cache_with_embeddings, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"ðŸ’¾ {len(point_records)} embeddings computed and cached.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557a20b7d1e3b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all records have an embeding\n",
    "invalid_records = [rec for rec in point_records if rec.get('embedding') is None]\n",
    "assert len(invalid_records)==0, f\"Found {len(invalid_records)} invalid points\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
