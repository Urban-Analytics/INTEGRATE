{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import random\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "import clip\n",
    "\n",
    "# # Hugging Face Transformers for image embedding\n",
    "from transformers import AutoImageProcessor, AutoModel, ViTModel, ViTImageProcessor, ViTConfig\n",
    "\n",
    "# # ----------------- Configuration -----------------\n",
    "# #np.random.seed(42)\n",
    "# density_per_km = 0.3  # number of points to sample per km of road\n",
    "# #density_per_km = 0.1  # VERY FEW WHILE TESTING\n",
    "DOWNLOAD_IMAGES = False  # If false then don't download any images, just load those that have been cached\n",
    "\n",
    "data_dir = os.path.join(\"../../data/embeddings/\")\n",
    "boundary_file = os.path.join(data_dir, \"greater_manchester_lsoas.geojson\")  # Path to boundary polygon file\n",
    "lsoas_file = os.path.join(\"../../data/SpatialData/\", \"LSOAs_2021\", \"LSOA_2021_EW_BSC_V4.shp\")\n",
    "n_directions = 4         # number of street view images per point (e.g., 4 cardinal directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a13cd0696105",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a11b33a5e00526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.661138Z",
     "start_time": "2025-09-04T09:36:29.529524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged neighbourhoods into single study-area boundary.\n",
      "Bounds: (-2.73052481406758, 53.3281053809015, -1.90962093258169, 53.6857339236763)\n"
     ]
    }
   ],
   "source": [
    "# --- Load neighbourhood polygons and dissolve to one study-area boundary ---\n",
    "boundary_neighs = gpd.read_file(boundary_file)\n",
    "# Make sure we're in WGS84 (lat/lon) for OSM and APIs\n",
    "boundary_neighs = boundary_neighs.to_crs(epsg=4326)\n",
    "# Dissolve: merge all geometries into one polygon (MultiPolygon possible)\n",
    "boundary_polygon = boundary_neighs.union_all()  # shapely (multi)polygon\n",
    "boundary_gdf = gpd.GeoDataFrame(data={'name': ['study_area']},\n",
    "    geometry=[boundary_polygon], crs=boundary_neighs.crs)\n",
    "print(\"Merged neighbourhoods into single study-area boundary.\")\n",
    "print(\"Bounds:\", boundary_polygon.bounds)\n",
    "# boundary_gdf.plot(color='lightblue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c742d-721d-41d7-8287-027c8d4bc523",
   "metadata": {},
   "source": [
    "## Read the LSOA boundary data\n",
    "\n",
    "(later it will be joined to the Greater Manchester Gentrification Index and IMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dc5e31eb629a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA21NM'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "# gm_lsoa.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached point data â€¦\n",
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if os.path.isfile(points_data_cache):\n",
    "    print(\"Loading cached point data â€¦\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba1f32-9f79-4985-a01f-6de3b3a0a942",
   "metadata": {},
   "source": [
    "### Map of the full sample (cache + any others just downlaoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952715d-e889-404b-96be-72e783920876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:17:15.525112Z",
     "start_time": "2025-09-04T11:17:15.239938Z"
    }
   },
   "outputs": [],
   "source": [
    "# if point_records:    \n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "#     # Plot roads first (thin gray lines)\n",
    "#     # edges_gdf.plot(ax=ax, linewidth=0.1, color=\"gray\")\n",
    "\n",
    "#     # Plot the sample points\n",
    "#     gpd.GeoDataFrame(point_records, \n",
    "#                      geometry=[Point(rec[\"longitude\"], rec[\"latitude\"]) for rec in point_records], \n",
    "#                      crs=\"EPSG:4326\").plot(ax=ax, color=\"blue\", markersize=0.2, label=\"Sample Points\")\n",
    "\n",
    "#     # Plot the study-area outline on top (thicker red line)\n",
    "#     boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "#     ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "#     ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "#     ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07afd2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18897/18897 [52:46<00:00,  5.97point/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saved embeddings + category scores for 18897 points.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. Load CLIP\n",
    "# ------------------------------\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Define categories\n",
    "# ------------------------------\n",
    "categories = [\n",
    "    \"a photo taken indoors, inside a building or a car\",\n",
    "    \"a photo primarily of a terraced house\",\n",
    "    \"a photo primarily of a detached or semi detached house\",\n",
    "    \"a photo primarily of a road\",\n",
    "    \"a photo of a shop\",\n",
    "    \"a photo dominated by the outside of a car\",\n",
    "    \"a photo of an industrial building\",\n",
    "    \"a photo of wasteland, empty space\",\n",
    "    \"a photo of nice green space, e.g. parks, gardens, nice trees and flowers\"]\n",
    "\n",
    "text_tokens = clip.tokenize(categories).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)  # normalize once\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3. CLIP embedding + scoring\n",
    "# ------------------------------\n",
    "def embed_and_score_clip(img_path):\n",
    "    \"\"\"Return CLIP embedding (512d) + category similarity scores.\"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_feat = model.encode_image(image_tensor)\n",
    "        image_feat /= image_feat.norm(dim=-1, keepdim=True)  # normalize for cosine sim\n",
    "\n",
    "        # similarities â†’ shape (1, num_categories)\n",
    "        sims = (image_feat @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    return (\n",
    "        image_feat.cpu().numpy()[0],      # 512-dim embedding\n",
    "        sims.cpu().numpy()[0]             # category probs (len 9)\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Dummy fields for compatibility\n",
    "# ------------------------------\n",
    "def empty_attention_stats():\n",
    "    return {\"mean\": None, \"std\": None, \"entropy\": None, \"max_y\": None, \"max_x\": None}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed + score all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records, desc=\"Embedding points\", unit=\"point\"):\n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []      # <â€”â€” NEW\n",
    "    rec[\"attn_stats\"] = []           # empty placeholders\n",
    "    rec[\"heatmap_paths\"] = []        # always None\n",
    "\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "        img_path = (\n",
    "            img_path.replace(\"airbnb-manchester/\", \"embeddings/\")\n",
    "                    .replace(\"../\", \"../../\")\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            embedding, scores = embed_and_score_clip(img_path)\n",
    "\n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            rec[\"category_scores\"].append(scores)\n",
    "            rec[\"attn_stats\"].append(empty_attention_stats())\n",
    "            rec[\"heatmap_paths\"].append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âš ï¸ Error processing {img_path}: {e}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Save cache\n",
    "# ------------------------------\n",
    "output_file = (\n",
    "    data_dir + \"sample_points_cache/points_data_cache_with_CLIP_embeddings_and_scores.pkl\"\n",
    ")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved embeddings + category scores for {len(point_records)} points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557a20b7d1e3b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all records have an embeding\n",
    "invalid_records = [rec for rec in point_records if rec.get('embedding') is None]\n",
    "assert len(invalid_records)==0, f\"Found {len(invalid_records)} invalid points\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
