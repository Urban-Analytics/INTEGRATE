{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8cf3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import random \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f89e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"../../data/embeddings/\")\n",
    "lsoas_file = os.path.join(\"../../data/SpatialData/\", \"LSOAs_2011\", \"LSOA_2011_EW_BSC_V4.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e33fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_k(all_embeddings):\n",
    "    # --- Range of cluster counts to test ---\n",
    "    n_samples = len(all_embeddings)\n",
    "    max_k = min(20, n_samples - 1)\n",
    "    k_values = range(2, max_k + 1)\n",
    "\n",
    "    inertias = []\n",
    "    sil_scores = []\n",
    "\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(all_embeddings)\n",
    "\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        sil_scores.append(silhouette_score(all_embeddings, labels))\n",
    "\n",
    "    best_k = k_values[np.argmax(sil_scores)]\n",
    "    return best_k\n",
    "\n",
    "def plot_example_images_per_cluster(df, k, n_show=5):\n",
    "    fixed_rows, fixed_cols = 2, 6  # always 6 spaces total\n",
    "\n",
    "    for c in range(k):\n",
    "        cluster_imgs = df.loc[df['scene_cluster'] == c, 'image_files']\n",
    "        if len(cluster_imgs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Sample up to n_show images\n",
    "        sample_imgs = random.sample(list(cluster_imgs), min(n_show, len(cluster_imgs)))\n",
    "\n",
    "        fig, axes = plt.subplots(fixed_rows, fixed_cols, figsize=(fixed_cols * 3, fixed_rows * 3))\n",
    "        axes = np.array(axes).reshape(-1)  # flatten axes for easy indexing\n",
    "\n",
    "        for ax, img_path in zip(axes, sample_imgs):\n",
    "            adj_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../\")\n",
    "            img = plt.imread(adj_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # Hide any unused subplot spaces\n",
    "        for ax in axes[len(sample_imgs):]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"Global Cluster {c}\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "        \n",
    "def lsoa_within_between_summary(expanded_gdf):\n",
    "    results = []\n",
    "\n",
    "    for lsoa, df_lsoa in expanded_gdf.groupby('LSOA11CD'):\n",
    "        embeddings = np.stack(df_lsoa['embedding'].values)\n",
    "        clusters = df_lsoa['scene_cluster'].values\n",
    "        n_total = len(embeddings)\n",
    "\n",
    "        if n_total < 2:\n",
    "            results.append({\n",
    "                'LSOA11CD': lsoa,\n",
    "                'mean_within': np.nan,\n",
    "                'mean_between': np.nan,\n",
    "                'within_minus_between': np.nan,\n",
    "                'n_images': n_total\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "        within_sims = []\n",
    "        between_sims = []\n",
    "\n",
    "        for i, j in combinations(range(n_total), 2):\n",
    "            sim = sim_matrix[i, j]\n",
    "            if clusters[i] == clusters[j]:\n",
    "                within_sims.append(sim)\n",
    "            else:\n",
    "                between_sims.append(sim)\n",
    "\n",
    "        mean_within = np.mean(within_sims) if within_sims else np.nan\n",
    "        mean_between = np.mean(between_sims) if between_sims else np.nan\n",
    "        within_minus_between = mean_within - mean_between if mean_within is not np.nan and mean_between is not np.nan else np.nan\n",
    "\n",
    "        results.append({\n",
    "            'LSOA11CD': lsoa,\n",
    "            'mean_within': mean_within,\n",
    "            'mean_between': mean_between,\n",
    "            'within_minus_between': within_minus_between,\n",
    "            'n_images': n_total})\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Check for LSOAs where between > within\n",
    "    anomalies = df_results[df_results['within_minus_between'] < 0]\n",
    "    n_anomalies = len(anomalies)\n",
    "    if n_anomalies > 0:\n",
    "        print(f\"Warning: {n_anomalies} LSOAs have higher between-cluster similarity than within-cluster similarity.\")\n",
    "    else:\n",
    "        print(\"All LSOAs have higher within-cluster similarity than between-cluster similarity.\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def mean_embedding_per_cluster(expanded_gdf, K, embedding_col):\n",
    "    \"\"\"\n",
    "    Compute mean embedding per cluster for each LSOA.\n",
    "    \n",
    "    Parameters:\n",
    "    - expanded_gdf: DataFrame with columns 'LSOA11CD', 'embedding', 'scene_cluster'\n",
    "    - K: total number of clusters (0 to K-1)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with one row per LSOA, columns: 'cluster_0', 'cluster_1', ..., 'cluster_{K-1}'\n",
    "      Each cell contains the mean embedding vector for that cluster, or np.nan if no images in cluster.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for lsoa, df_lsoa in expanded_gdf.groupby('LSOA11CD'):\n",
    "        row = {'LSOA11CD': lsoa}\n",
    "        for k in range(K):\n",
    "            cluster_embeddings = df_lsoa.loc[df_lsoa['scene_cluster'] == k, embedding_col].values\n",
    "            if len(cluster_embeddings) == 0:\n",
    "                row[f'cluster_{k}'] = np.nan  # no images in this cluster\n",
    "            else:\n",
    "                row[f'cluster_{k}'] = np.mean(np.stack(cluster_embeddings), axis=0)\n",
    "        results.append(row)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a64fe",
   "metadata": {},
   "source": [
    "### Get spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8395946f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA11NMW'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2331d9",
   "metadata": {},
   "source": [
    "### Get embeddings (four per location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e3b8511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points_data_cache = data_dir + \"points_with_embeddings.pkl\"\n",
    "with open(points_data_cache, \"rb\") as f:\n",
    "    point_records = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4899715",
   "metadata": {},
   "source": [
    "### Join image embeddings points to gentrification LSOAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5af05acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points after spatial join: 18897 / 18897 `(some points may lie outside the label polygons and were dropped)\n"
     ]
    }
   ],
   "source": [
    "point_coords = [Point(rec['longitude'], rec['latitude']) for rec in point_records]\n",
    "points_labels_gdf = gpd.GeoDataFrame(point_records, geometry=point_coords, crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to get gentrification label for each point\n",
    "points_labels_gdf = gpd.sjoin(points_labels_gdf, gm_lsoa, how='inner', predicate='within')\n",
    "# sjoin may add an index from the polygon ('index_right'); we can drop it\n",
    "if 'index_right' in points_labels_gdf.columns:\n",
    "    points_labels_gdf = points_labels_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(f\"Points after spatial join: {len(points_labels_gdf)} / {len(point_records)}\"\n",
    "      f\" `(some points may lie outside the label polygons and were dropped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41217c68",
   "metadata": {},
   "source": [
    "### Create dataframe with one row per embedding (Instead of four embeddings per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff481082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 9158: 3 embeddings, 4 images\n",
      "Skipping row 18664: 3 embeddings, 4 images\n",
      "Original rows: 18897, Expanded rows: 75580\n"
     ]
    }
   ],
   "source": [
    "expanded_rows = []\n",
    "\n",
    "for _, row in points_labels_gdf.iterrows():\n",
    "    embeddings = row['embedding']      # list of 4 embeddings\n",
    "    images = row['image_files']        # list of 4 image paths\n",
    "\n",
    "    # Skip if lengths don't match\n",
    "    if len(embeddings) != len(images):\n",
    "        print(f\"Skipping row {row.name}: {len(embeddings)} embeddings, {len(images)} images\")\n",
    "        continue\n",
    "\n",
    "    for emb, img in zip(embeddings, images):\n",
    "        new_row = row.to_dict()        # copy all other columns\n",
    "        new_row['embedding'] = emb     # single embedding\n",
    "        new_row['image_files'] = img    # single image\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create new DataFrame\n",
    "expanded_gdf = pd.DataFrame(expanded_rows)\n",
    "print(f\"Original rows: {len(points_labels_gdf)}, Expanded rows: {len(expanded_gdf)}\")\n",
    "expanded_gdf = pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743259c5",
   "metadata": {},
   "source": [
    "# Run cluster analysis over the entire set of images\n",
    "### Run a PCA to reduce embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25532c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance by 20 components: 0.65\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = np.stack(expanded_gdf['embedding'].values)\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "print(f\"Explained variance by 20 components: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "# set back on dataframe\n",
    "expanded_gdf['embedding_reduced'] = list(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e6e7b",
   "metadata": {},
   "source": [
    "### Run cluster analysis on reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de30fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 5  # e.g., parks, houses, roads, industrial, water\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "all_embeddings = np.stack(expanded_gdf['embedding_reduced'].values)\n",
    "expanded_gdf['scene_cluster'] = kmeans.fit_predict(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da7b6d",
   "metadata": {},
   "source": [
    "### Check images to see whether clustering is logical (to me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f6e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_example_images_per_cluster(expanded_gdf, k, n_show=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c0ac6",
   "metadata": {},
   "source": [
    "### For each LSOA, compare between and within cluster group similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8120a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 7 LSOAs have higher between-cluster similarity than within-cluster similarity.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSOA11CD</th>\n",
       "      <th>mean_within</th>\n",
       "      <th>mean_between</th>\n",
       "      <th>within_minus_between</th>\n",
       "      <th>n_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E01004766</td>\n",
       "      <td>0.513102</td>\n",
       "      <td>0.349985</td>\n",
       "      <td>0.163117</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E01004767</td>\n",
       "      <td>0.472873</td>\n",
       "      <td>0.323371</td>\n",
       "      <td>0.149502</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E01004768</td>\n",
       "      <td>0.602358</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.224358</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E01004769</td>\n",
       "      <td>0.478979</td>\n",
       "      <td>0.280748</td>\n",
       "      <td>0.198231</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E01004770</td>\n",
       "      <td>0.586373</td>\n",
       "      <td>0.331539</td>\n",
       "      <td>0.254834</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LSOA11CD  mean_within  mean_between  within_minus_between  n_images\n",
       "0  E01004766     0.513102      0.349985              0.163117        64\n",
       "1  E01004767     0.472873      0.323371              0.149502        72\n",
       "2  E01004768     0.602358      0.378000              0.224358        56\n",
       "3  E01004769     0.478979      0.280748              0.198231        36\n",
       "4  E01004770     0.586373      0.331539              0.254834        40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoa_sim_df = lsoa_within_between_summary(expanded_gdf)\n",
    "lsoa_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd6748",
   "metadata": {},
   "source": [
    "### For each LSOA, calculate mean embedding in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cdfb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = expanded_gdf['scene_cluster'].max() + 1  # number of clusters\n",
    "lsoa_cluster_means_df = mean_embedding_per_cluster(expanded_gdf, K, 'embedding_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3733ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 1308 LSOAs out of 1669 that have all clusters present\n"
     ]
    }
   ],
   "source": [
    "# Keep only LSOAs where no cluster embedding is missing\n",
    "cluster_cols = [f'cluster_{k}' for k in range(K)]\n",
    "complete_lsoas_df = lsoa_cluster_means_df.dropna(subset=cluster_cols)\n",
    "\n",
    "print(f\"Kept {len(complete_lsoas_df)} LSOAs out of {len(lsoa_cluster_means_df)} that have all clusters present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e93953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "lsoa_cluster_means_df.to_pickle(data_dir + \"global_clusters_mean_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a5b2d",
   "metadata": {},
   "source": [
    "# Who knows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcdc8afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(points_labels_gdf['embedding'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13d631b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18897, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.stack(points_labels_gdf[\"embedding\"].values)     # shape (n_points, embed_dim)\n",
    "# y = points_labels_gdf[label_col].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2620ec",
   "metadata": {},
   "source": [
    "# What is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "428c9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list with mean embeddings\n",
    "point_records_mean = []\n",
    "\n",
    "for record in point_records:\n",
    "    # Convert the list of embeddings to a NumPy array (shape: 4 x embedding_dim)\n",
    "    embeddings_array = np.stack(record['embedding'])  # shape (4, D)\n",
    "    \n",
    "    # Take the mean along axis 0 to get a single embedding (shape: D)\n",
    "    mean_embedding = np.mean(embeddings_array, axis=0)\n",
    "    \n",
    "    # Copy the record and replace 'embedding' with mean\n",
    "    new_record = record.copy()\n",
    "    new_record['embedding'] = mean_embedding\n",
    "    point_records_mean.append(new_record)\n",
    "    \n",
    "points_data_cache_mean = data_dir + \"points_with_embeddings_mean.pkl\"  \n",
    "with open(points_data_cache_mean, \"wb\") as f:\n",
    "    pickle.dump(point_records_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "412661e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points after spatial join: 18897 / 18897 `(some points may lie outside the label polygons and were dropped)\n"
     ]
    }
   ],
   "source": [
    "point_coords = [Point(rec['longitude'], rec['latitude']) for rec in point_records_mean]\n",
    "points_labels_gdf = gpd.GeoDataFrame(point_records_mean, geometry=point_coords, crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to get gentrification label for each point\n",
    "points_labels_gdf = gpd.sjoin(points_labels_gdf, gm_lsoa, how='inner', predicate='within')\n",
    "# sjoin may add an index from the polygon ('index_right'); we can drop it\n",
    "if 'index_right' in points_labels_gdf.columns:\n",
    "    points_labels_gdf = points_labels_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(f\"Points after spatial join: {len(points_labels_gdf)} / {len(point_records_mean)}\"\n",
    "      f\" `(some points may lie outside the label polygons and were dropped)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "381c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embed(x):\n",
    "    return np.mean(np.vstack(x), axis=0)\n",
    "\n",
    "avg_embeddings = (\n",
    "    points_labels_gdf.groupby(\"LSOA11CD\")[\"embedding\"]\n",
    "      .apply(mean_embed)\n",
    "      .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31676214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "avg_embeddings.to_pickle(data_dir + \"mean_embeddings_per_lsoa.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
