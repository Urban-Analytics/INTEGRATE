{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import random\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "import clip\n",
    "import pandas as pd\n",
    "\n",
    "# # Hugging Face Transformers for image embedding\n",
    "from transformers import AutoImageProcessor, AutoModel, ViTModel, ViTImageProcessor, ViTConfig\n",
    "\n",
    "# # ----------------- Configuration -----------------\n",
    "# #np.random.seed(42)\n",
    "# density_per_km = 0.3  # number of points to sample per km of road\n",
    "# #density_per_km = 0.1  # VERY FEW WHILE TESTING\n",
    "DOWNLOAD_IMAGES = False  # If false then don't download any images, just load those that have been cached\n",
    "\n",
    "data_dir = os.path.join(\"../../../data/embeddings/\")\n",
    "boundary_file = os.path.join(data_dir, \"greater_manchester_lsoas.geojson\")  # Path to boundary polygon file\n",
    "lsoas_file = os.path.join(\"../../../data/SpatialData/\", \"LSOAs_2021\", \"LSOA_2021_EW_BSC_V4.shp\")\n",
    "n_directions = 4         # number of street view images per point (e.g., 4 cardinal directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a13cd0696105",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a11b33a5e00526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.661138Z",
     "start_time": "2025-09-04T09:36:29.529524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged neighbourhoods into single study-area boundary.\n",
      "Bounds: (-2.73052481406758, 53.3281053809015, -1.90962093258169, 53.6857339236763)\n"
     ]
    }
   ],
   "source": [
    "# --- Load neighbourhood polygons and dissolve to one study-area boundary ---\n",
    "boundary_neighs = gpd.read_file(boundary_file)\n",
    "# Make sure we're in WGS84 (lat/lon) for OSM and APIs\n",
    "boundary_neighs = boundary_neighs.to_crs(epsg=4326)\n",
    "# Dissolve: merge all geometries into one polygon (MultiPolygon possible)\n",
    "boundary_polygon = boundary_neighs.union_all()  # shapely (multi)polygon\n",
    "boundary_gdf = gpd.GeoDataFrame(data={'name': ['study_area']},\n",
    "    geometry=[boundary_polygon], crs=boundary_neighs.crs)\n",
    "print(\"Merged neighbourhoods into single study-area boundary.\")\n",
    "print(\"Bounds:\", boundary_polygon.bounds)\n",
    "# boundary_gdf.plot(color='lightblue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c742d-721d-41d7-8287-027c8d4bc523",
   "metadata": {},
   "source": [
    "## Read the LSOA boundary data\n",
    "\n",
    "(later it will be joined to the Greater Manchester Gentrification Index and IMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dc5e31eb629a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA21NM'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "# gm_lsoa.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached point data â€¦\n",
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if os.path.isfile(points_data_cache):\n",
    "    print(\"Loading cached point data â€¦\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b82fb7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built improved category text embeddings: torch.Size([11, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [13:04<00:00,  6.38point/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saved embeddings + category scores for 18897 points.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# ---- Create final text embeddings ----\n",
    "final_text_features = []\n",
    "category_names = []  # so your categories keep their correct order\n",
    "\n",
    "# multi_prompts = {\n",
    "#     \"modern\": [\n",
    "#         \"a photo showing a modern style\",\n",
    "#         \"a clean contemporary aesthetic\",\n",
    "#         \"modern architecture or interior design\",\n",
    "#         \"a sleek, newly renovated modern look\"\n",
    "#     ],\n",
    "#     \"traditional\": [\n",
    "#         \"a photo with a traditional style\",\n",
    "#         \"classic older architecture or interiors\",\n",
    "#         \"traditional British housing or decor\",\n",
    "#         \"a place with a historic or vintage aesthetic\"\n",
    "#     ],\n",
    "#     \"affluent\": [\n",
    "#         \"a photo that looks affluent or wealthy\",\n",
    "#         \"a well-maintained high-end environment\",\n",
    "#         \"a stylish, expensive-looking area\",\n",
    "#         \"a photo suggesting wealth or prosperity\"\n",
    "#     ],\n",
    "#     \"run_down\": [\n",
    "#         \"a photo showing a run-down environment\",\n",
    "#         \"signs of wear, damage, or poor maintenance\",\n",
    "#         \"a neglected or deteriorating place\",\n",
    "#         \"a photo suggesting disrepair or decay\"\n",
    "#     ],\n",
    "#     \"minimalist\": [\n",
    "#         \"a minimalist design style\",\n",
    "#         \"clean simple lines and sparse decoration\",\n",
    "#         \"a clutter-free modern interior\",\n",
    "#         \"a calm minimalist aesthetic\"\n",
    "#     ],\n",
    "#     \"cluttered\": [\n",
    "#         \"a cluttered messy-looking scene\",\n",
    "#         \"an interior with many scattered objects\",\n",
    "#         \"a busy, chaotic environment\",\n",
    "#         \"a photo showing visual clutter\"\n",
    "#     ],\n",
    "#     \"renovated\": [\n",
    "#         \"a newly renovated or refurbished interior\",\n",
    "#         \"recently upgraded modern features\",\n",
    "#         \"a fresh high-quality renovation\",\n",
    "#         \"a renewed modernised environment\"\n",
    "#     ],\n",
    "#     \"dated\": [\n",
    "#         \"a dated or old-fashioned decor style\",\n",
    "#         \"an interior with older outdated features\",\n",
    "#         \"a place that looks stuck in the past\",\n",
    "#         \"a photo with an out-of-date aesthetic\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "\n",
    "multi_prompts = {\n",
    "    \"street_trees\": [\n",
    "        \"a street scene with visible street trees\",\n",
    "        \"a road lined with trees\",\n",
    "        \"trees planted along an urban street\",\n",
    "        \"a city street with noticeable tree canopy\"\n",
    "    ],\n",
    "\n",
    "    \"greenery_quality\": [\n",
    "        \"a scene with healthy green vegetation\",\n",
    "        \"a photo showing well-maintained grass or plants\",\n",
    "        \"lush greenery in an urban environment\",\n",
    "        \"green plants or trees that appear healthy and vibrant\"\n",
    "    ],\n",
    "\n",
    "    \"building_condition\": [\n",
    "        \"a photo of a building with visible wear or decay\",\n",
    "        \"a street scene showing weathered or poorly maintained buildings\",\n",
    "        \"a building with cracked paint, damage, or ageing materials\",\n",
    "        \"an older or run-down building facade\"\n",
    "    ],\n",
    "\n",
    "    \"surface_quality\": [\n",
    "        \"a street or pavement with potholes or cracks\",\n",
    "        \"a damaged sidewalk or road surface\",\n",
    "        \"a photo of worn asphalt or uneven pavement\",\n",
    "        \"poorly maintained street or footpath surfaces\"\n",
    "    ],\n",
    "\n",
    "    \"street_cleanliness\": [\n",
    "        \"a street scene with visible litter or rubbish\",\n",
    "        \"an outdoor area that looks unclean or poorly maintained\",\n",
    "        \"a city street with scattered trash\",\n",
    "        \"an urban scene showing litter or waste on the ground\"\n",
    "    ],\n",
    "\n",
    "    \"pedestrian_infrastructure\": [\n",
    "        \"a street scene with clear pedestrian infrastructure\",\n",
    "        \"a road with sidewalks, footpaths or crossings\",\n",
    "        \"a city street designed for pedestrians\",\n",
    "        \"an urban pedestrian-friendly environment\"\n",
    "    ],\n",
    "\n",
    "    \"lighting_and_visibility\": [\n",
    "        \"a street with streetlights or lighting infrastructure\",\n",
    "        \"a scene showing lamps or artificial lighting\",\n",
    "        \"urban streetlight poles visible in the environment\",\n",
    "        \"a road at dusk with visible lighting structures\"\n",
    "    ],\n",
    "\n",
    "    \"parking_pressure\": [\n",
    "        \"a street scene dominated by parked cars\",\n",
    "        \"cars tightly parked along a residential street\",\n",
    "        \"a street with heavy on-street parking\",\n",
    "        \"roadside parking filling most of the street view\"\n",
    "    ],\n",
    "\n",
    "    \"green_open_space\": [\n",
    "        \"a public green open space such as a park or field\",\n",
    "        \"a large grassy area in a neighbourhood\",\n",
    "        \"an open green parkland scene\",\n",
    "        \"a public field or recreational green area\"\n",
    "    ],\n",
    "\n",
    "    \"street_furniture\": [\n",
    "        \"a street scene with benches, bins or bollards\",\n",
    "        \"urban street furniture visible on a sidewalk\",\n",
    "        \"public seating or bins along a footpath\",\n",
    "        \"a street with bollards, benches or other fixtures\"\n",
    "    ],\n",
    "\n",
    "    \"graffiti_and_vandalism\": [\n",
    "        \"a wall or structure with graffiti or vandalism\",\n",
    "        \"a building or street surface marked with graffiti\",\n",
    "        \"tagged walls or vandalised public infrastructure\",\n",
    "        \"a scene showing graffiti or street tagging\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cat, prompts in multi_prompts.items():\n",
    "        category_names.append(cat)\n",
    "\n",
    "        # Encode all prompts for this category\n",
    "        tokenized = clip.tokenize(prompts).to(device)\n",
    "        txt_feats = model.encode_text(tokenized)\n",
    "\n",
    "        # Normalise each embedding\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Average across prompts\n",
    "        avg_feat = txt_feats.mean(dim=0)\n",
    "\n",
    "        # Normalise again (important!)\n",
    "        avg_feat = avg_feat / avg_feat.norm()\n",
    "\n",
    "        final_text_features.append(avg_feat.cpu())\n",
    "\n",
    "# Convert to tensor of shape (num_categories, 512)\n",
    "final_text_features = torch.stack(final_text_features)\n",
    "print(\"Built improved category text embeddings:\", final_text_features.shape)\n",
    "\n",
    "\n",
    "def embed_and_score_clip(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Image embedding\n",
    "        image_feat = model.encode_image(image_tensor)\n",
    "        image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Cosine similarity to averaged text embeddings\n",
    "        sims = image_feat @ final_text_features.to(device).T\n",
    "\n",
    "        # Optional: scale from [-1, 1] â†’ [0, 1]\n",
    "        sims = (sims + 1) / 2  \n",
    "\n",
    "    return (\n",
    "        image_feat.cpu().numpy()[0],   # 512D image embedding\n",
    "        sims.cpu().numpy()[0]          # independent environmental cue scores\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Placeholder stats\n",
    "# ------------------------------\n",
    "def empty_attention_stats():\n",
    "    return {\"mean\": None, \"std\": None, \"entropy\": None, \"max_y\": None, \"max_x\": None}\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records[:5000], desc=\"Embedding points\", unit=\"point\"):\n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []\n",
    "    rec[\"attn_stats\"] = []\n",
    "    rec[\"heatmap_paths\"] = []\n",
    "\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "\n",
    "        # Use ORIGINAL images\n",
    "        img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../../\")\n",
    "\n",
    "        try:\n",
    "            embedding, scores = embed_and_score_clip(img_path)\n",
    "\n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            rec[\"category_scores\"].append(scores)\n",
    "            rec[\"attn_stats\"].append(empty_attention_stats())\n",
    "            rec[\"heatmap_paths\"].append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âš ï¸ Error: {e}\")\n",
    "\n",
    "output_file = (data_dir + \"sample_points_cache/points_data_cache_with_CLIP_embeddings_and_scores_v3.pkl\")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved embeddings + category scores for {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22656cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18897"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(point_records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
