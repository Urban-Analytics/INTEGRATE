{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af254837",
   "metadata": {},
   "source": [
    "## CalculateEmbeddings\n",
    "\n",
    "This script:\n",
    "- Loads list of sample points, each with 4 associated image files\n",
    "- Calculates an image embedding for each image files\n",
    "- Calculates text embeddings from a list of prompts\n",
    "- Calculates a similarity score between each image embedding and each text embedding\n",
    "- Saves a pickle file containig each sample point, the list of image files, the list of embeddings and the list of similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "data_dir = os.path.join(\"../../../../data/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.iypnb  \n",
    "Each point has a latitude, a longitude, and 4 image files associated with it (these are sampled in each of the 4 cardinal directions from the sample point)  \n",
    "It also contains an 'embedding' slot, which this script will fill with a list of embeddings for each of the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78ce937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache.pkl\"\n",
    "with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "print(f\"Cache currently has {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "# Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82fb7d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc589",
   "metadata": {},
   "source": [
    "## Create text embedding for categories we want to match image embeddings to\n",
    "\n",
    "- For each headline category, define several different prompts\n",
    "- Convert each of the subprompts into a text embedding\n",
    "- For each headling category, find the mean text embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54037c49",
   "metadata": {},
   "source": [
    "### Define the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b45473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# These prompts are based on the planning use classes: https://www.planningportal.co.uk/permission/common-projects/change-of-use/use-classes \n",
    "multi_prompts = {\n",
    "    \"C ‚Äì Accommodation\": [\n",
    "        \"a photo of a house or home\",\n",
    "        \"an apartment building on a street\",\n",
    "        \"houses in a residential neighborhood\",\n",
    "        \"front view of a suburban house\",\n",
    "        \"a cozy home exterior with a garden\"],\n",
    "    \"B ‚Äì Industrial / Storage\": [\n",
    "        \"a warehouse or big industrial building\",\n",
    "        \"a factory with chimneys or machinery\",\n",
    "        \"storage containers outside a building\",\n",
    "        \"a logistics yard with trucks and crates\",\n",
    "        \"industrial buildings in an urban area\"],\n",
    "    \"E ‚Äì Commercial / Business / Service\": [\n",
    "        \"a shop or cafe on the street\",\n",
    "        \"a busy high street with stores\",\n",
    "        \"a restaurant or small business front\",\n",
    "        \"office building in the city\",\n",
    "        \"people outside a retail store or service\"],\n",
    "    \"F ‚Äì Local Community / Learning\": [\n",
    "        \"a school or university building\",\n",
    "        \"library or community centre\",\n",
    "        \"children playing at a sports field\",\n",
    "        \"outdoor playground or swimming pool\",\n",
    "        \"museum, gallery or exhibition space\"]}\n",
    "\n",
    "# Option 2\n",
    "# These prompts are based on trial and error refinement of what seem to be the most common scene types\n",
    "multi_prompts = {\n",
    "# 1. Indoor / interior scenes (to exclude non-street content)\n",
    "    \"indoor\": [\n",
    "        \"an indoor photo inside a building\",\n",
    "        \"an interior room scene\",\n",
    "        \"a photo taken indoors under artificial lighting\",\n",
    "        \"inside a residential home\",\n",
    "        \"inside an office or workspace\",\n",
    "        \"an indoor hallway or corridor\",\n",
    "        \"inside a car interior\",\n",
    "        \"a room with walls and furniture visible\",\n",
    "        \"an indoor photo with no outdoor scenery\",\n",
    "        \"an interior living space\"],\n",
    "\n",
    "    # 2. Single residential house (any detached, semi-detached, terraced)\n",
    "    \"single_house\": [\n",
    "        \"a single residential house\",\n",
    "        \"a standalone house on a street\",\n",
    "        \"a terraced or semi-detached home\",\n",
    "        \"a suburban residential house\",\n",
    "        \"a UK-style house with a front door and windows\",\n",
    "        \"a residential house with a garden or driveway\",\n",
    "        \"a home with a pitched roof\",\n",
    "        \"a brick residential house\",\n",
    "        \"a typical British house exterior\",\n",
    "        \"a single-family home on a quiet street\"],\n",
    "\n",
    "    # 3. Residential street (multiple houses visible)\n",
    "    \"residential_street\": [\n",
    "        \"a residential street with multiple houses\",\n",
    "        \"a row of houses along a street\",\n",
    "        \"a street lined with terraced homes\",\n",
    "        \"a suburban neighbourhood street view\",\n",
    "        \"a street with parked cars and houses\",\n",
    "        \"a residential road with houses on both sides\",\n",
    "        \"a quiet housing neighbourhood\",\n",
    "        \"a UK residential area with multiple homes visible\",\n",
    "        \"a typical street of British housing\",\n",
    "        \"a suburban housing street scene\"],\n",
    "\n",
    "    # 4. Shops and cafes (retail frontage)\n",
    "    \"shops_and_cafes\": [\n",
    "        \"a shop or retail storefront\",\n",
    "        \"a cafe or coffee shop entrance\",\n",
    "        \"a commercial high street with shops\",\n",
    "        \"a small business storefront\",\n",
    "        \"a row of shops on a street\",\n",
    "        \"a trendy cafe or coffee shop\",\n",
    "        \"a street with commercial signage\",\n",
    "        \"a food takeaway or retail frontage\",\n",
    "        \"a boutique store or independent shop\",\n",
    "        \"a street with restaurants or cafes\"],\n",
    "\n",
    "    # 5. Road / street without strong residential context\n",
    "    \"road\": [\n",
    "        \"a road with cars and traffic\",\n",
    "        \"a street with no houses visible\",\n",
    "        \"an asphalt road outdoors\",\n",
    "        \"a roadway with vehicles\",\n",
    "        \"a simple road scene with pavement\",\n",
    "        \"a street-level view of a road\",\n",
    "        \"a road with buildings far away\",\n",
    "        \"a road with painted lane markings\",\n",
    "        \"an intersection or junction\",\n",
    "        \"a wide street with traffic flow\"],\n",
    "\n",
    "    # 6. Highways / motorways (new category)\n",
    "    \"highway\": [\n",
    "        \"a large highway with multiple lanes\",\n",
    "        \"a motorway with fast-moving traffic\",\n",
    "        \"a dual carriageway\",\n",
    "        \"a high-speed road with no houses nearby\",\n",
    "        \"a major road with overpasses or slip roads\",\n",
    "        \"a highway with barriers and signage\",\n",
    "        \"a wide road with multiple lanes of traffic\",\n",
    "        \"a motorway with cars travelling at speed\",\n",
    "        \"a road with green verges and no buildings\",\n",
    "        \"a large transportation corridor\" ],\n",
    "\n",
    "    # 7. Car-dominated / close-up vehicles\n",
    "    \"car\": [\n",
    "        \"a photo dominated by a car exterior\",\n",
    "        \"a close-up of a vehicle\",\n",
    "        \"a parked car in the foreground\",\n",
    "        \"a photo where a car fills most of the frame\",\n",
    "        \"a car on the street close-up\",\n",
    "        \"a vehicle photographed from the side\",\n",
    "        \"a car front or headlight close-up\",\n",
    "        \"a parked vehicle dominating the scene\",\n",
    "        \"a photo focused mainly on a car body\",\n",
    "        \"a street scene with a car extremely close\"],\n",
    "\n",
    "    # 8. Industrial buildings / warehouses\n",
    "    \"industrial\": [\n",
    "        \"an industrial building such as a warehouse\",\n",
    "        \"a factory or manufacturing facility\",\n",
    "        \"an industrial estate building\",\n",
    "        \"a warehouse with metal siding\",\n",
    "        \"a large industrial structure\",\n",
    "        \"a factory exterior with chimneys\",\n",
    "        \"an industrial unit with shutters\",\n",
    "        \"a building used for manufacturing or storage\",\n",
    "        \"an industrial complex\",\n",
    "        \"a photo of a warehouse yard\"],\n",
    "\n",
    "    # 9. Wasteland / derelict land\n",
    "    \"wasteland\": [\n",
    "        \"a derelict vacant lot\",\n",
    "        \"an empty or abandoned outdoor area\",\n",
    "        \"unused land with no buildings\",\n",
    "        \"a vacant plot with rubble or overgrowth\",\n",
    "        \"an open space with signs of neglect\",\n",
    "        \"a barren ground area\",\n",
    "        \"an outdoor area with debris and no structures\",\n",
    "        \"a neglected or rundown open space\",\n",
    "        \"a disused land area\",\n",
    "        \"a wasteland with weeds or dirt\"],\n",
    "\n",
    "    # 10. Greenspace / parks\n",
    "    \"greenspace\": [\n",
    "        \"a public park with grass and trees\",\n",
    "        \"an urban greenspace or recreation area\",\n",
    "        \"a landscaped public park\",\n",
    "        \"a community park with greenery\",\n",
    "        \"a garden or park with plants\",\n",
    "        \"a green open space with trees\",\n",
    "        \"a park pathway with vegetation\",\n",
    "        \"a natural outdoor green area\",\n",
    "        \"a public lawn or green field\",\n",
    "        \"an urban park with trees and grass\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147e0b3",
   "metadata": {},
   "source": [
    "### Calculate the embeddings from the text categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40a5856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built improved category text embeddings: torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# List of embeddings for each of the 4 headline categories\n",
    "final_text_features = []\n",
    "category_names = []  \n",
    "\n",
    "# This line tells PyTorch that we're not training CLIP (just using to calculate embeddings), so don't need to compute gradients\n",
    "# This makes the computation faster\n",
    "with torch.no_grad():\n",
    "    # Loop through each category and its list of text prompts\n",
    "    for cat, prompts in multi_prompts.items():\n",
    "\n",
    "        # Add the category name to your list (used later for plotting or indexing)\n",
    "        category_names.append(cat)\n",
    "\n",
    "        # Convert all textual prompts into CLIP token IDs \n",
    "        # Token IDs are numerical codes that represent words or sub-words\n",
    "        tokenized = clip.tokenize(prompts).to(device)\n",
    "\n",
    "        # Encode all the token IDs into CLIP text embeddings\n",
    "        txt_feats = model.encode_text(tokenized)\n",
    "\n",
    "        # Normalise each prompt embedding to unit length\n",
    "        # (CLIP uses cosine similarity, so normalisation matters)\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute the mean embedding across all prompts for this category\n",
    "        # This creates a single \"category embedding\" representing all its prompts\n",
    "        avg_feat = txt_feats.mean(dim=0)\n",
    "\n",
    "        # Normalise the averaged embedding again\n",
    "        # This ensures it remains a proper CLIP embedding for cosine similarity\n",
    "        avg_feat = avg_feat / avg_feat.norm()\n",
    "\n",
    "        # Save this averaged category embedding\n",
    "        final_text_features.append(avg_feat.cpu())\n",
    "\n",
    "# Convert to tensor of shape (num_categories, 512)\n",
    "# A tensor is a multi-dimensional array, and is the format expected by PyTorch\n",
    "final_text_features = torch.stack(final_text_features)\n",
    "print(\"Built improved category text embeddings:\", final_text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcc42b",
   "metadata": {},
   "source": [
    "## Create embedding for each image and find similarity to categories \n",
    "- Create embedding for image\n",
    "- Find similarity score to text embedding for each category\n",
    "- Convert similarity score to a \"probability-like number\" using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5f237c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def embed_and_score_clip(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image, computes its CLIP embedding, \n",
    "    and calculates similarity-based category probabilities.\n",
    "\n",
    "    Returns:\n",
    "        image_embedding (np.array): 512-dim CLIP image embedding\n",
    "        category_probabilities (np.array): Probability for each category\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. LOAD AND PREPROCESS THE IMAGE\n",
    "    # -----------------------------------------------------------\n",
    "    # Load image using PIL and convert to 3-channel RGB\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Apply CLIP preprocessing:\n",
    "    # - resize/crop to 224x224\n",
    "    # - convert to torch tensor\n",
    "    # - normalise pixels with CLIP‚Äôs mean/std\n",
    "    # This produces a tensor of shape (3, 224, 224)\n",
    "    image_tensor = preprocess(pil_image)\n",
    "\n",
    "    # Add a batch dimension ‚Üí (1, 3, 224, 224)\n",
    "    # Required because CLIP expects a batch\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move tensor to CPU or GPU depending on device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. RUN CLIP TO GET IMAGE EMBEDDING\n",
    "    # -----------------------------------------------------------\n",
    "    # Disable gradient tracking \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encode the image ‚Üí produces a 512-dim CLIP embedding\n",
    "        raw_image_embedding = model.encode_image(image_tensor)\n",
    "\n",
    "        # Normalise embedding to unit length (important for cosine similarity)\n",
    "        image_embedding = raw_image_embedding / raw_image_embedding.norm(\n",
    "            dim=-1, keepdim=True)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 3. COMPUTE SIMILARITIES TO TEXT CATEGORY EMBEDDINGS\n",
    "        # -----------------------------------------------------------\n",
    "        # Returns similarity of the 1 image embedding to N text embeddings\n",
    "        # These are dot products, representing how close the image is to each category in embedding space\n",
    "        similarity_scores = (image_embedding @ final_text_features.to(device).T)\n",
    "\n",
    "        ######## Convert raw similarities to probabilities\n",
    "        # Softmax is a function that turns a set of numbers into a probability-like distribution\n",
    "        # However, the numbers do not represent true probabilities\n",
    "        # e.g. Scores of Indoor: 0.75, Greenery: 0.18, Terraced house: 0.04, Road: 0.02, Shop: 0.01\n",
    "        # Mean that The \"indoor\" text embedding was much closer to the image embedding than the others.\n",
    "        # And NOT that the true probability that the scene is indoors is 75%.\n",
    "        category_probabilities = similarity_scores.softmax(dim=-1)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. RETURN CLEAN CPU NUMPY ARRAYS\n",
    "    # -----------------------------------------------------------\n",
    "    return (image_embedding.cpu().numpy()[0],       # shape (512,)\n",
    "        similarity_scores.cpu().numpy()[0],\n",
    "        category_probabilities.cpu().numpy()[0])\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records, desc=\"Embedding points\", unit=\"point\"):\n",
    "# for rec in point_records[200:300]:    \n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []\n",
    "    rec[\"category_probs\"] = []\n",
    "    \n",
    "    for img_path in rec[\"image_files\"]:\n",
    "\n",
    "        try:\n",
    "            embedding, scores, probabilities = embed_and_score_clip(img_path)\n",
    "            \n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            rec[\"category_scores\"].append(scores)\n",
    "            rec[\"category_probs\"].append(probabilities)\n",
    "            \n",
    "            # Testing\n",
    "#             img = Image.open(img_path)\n",
    "#             fig,ax=plt.subplots(figsize=(2,2))\n",
    "#             ax.axis(\"off\")\n",
    "#             plt.imshow(img)\n",
    "#             plt.show()\n",
    "#             argmax = scores.argmax()\n",
    "#             print(list(multi_prompts.keys())[argmax])\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37771f37",
   "metadata": {},
   "source": [
    "## Save outputs to pickle file\n",
    "\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.iypnb.    \n",
    "Each point has a latitude, a longitude, and 4 image files associated with it.  \n",
    "It also contains an 'embedding' slot, with a list of embeddings for each of the 4 images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0bfb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved embeddings + category scores for 18897 points.\n"
     ]
    }
   ],
   "source": [
    "output_file = (data_dir + \"sample_points_cache/points_data_cache_with_CLIP_embeddings_and_scores_userdefinedclasses.pkl\")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nüíæ Saved embeddings + category scores for {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746178c",
   "metadata": {},
   "source": [
    "## Testing how CLIP works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cad3740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_path = point_records[2][\"image_files\"][3]\n",
    "# img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../../\")\n",
    "# img = Image.open(img_path)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bc68b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# from PIL import Image\n",
    "\n",
    "# #Load CLIP model\n",
    "# model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901f0578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encode text descriptions\n",
    "# text_emb = model.encode(['Two dogs in the snow', \"a pizza\", 'A cat on a table', 'A picture of a road, with cars and trees'])\n",
    "# text_emb = model.encode([\"a cucumber\", \"semi-detached house\", \"a highway with few cars and grass embankments\", \"a car\", \"a view down a road\", \"a park\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718df4da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Encode an image:\n",
    "# img_emb = model.encode(Image.open(img_path))\n",
    "\n",
    "# #Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f5b5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text_emb = model.encode([\"a house\", \"a shop\", \"a car\", \"a road\", \"a park\"])\n",
    "\n",
    "# #Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
