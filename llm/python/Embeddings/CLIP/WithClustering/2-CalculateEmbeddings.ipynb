{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af254837",
   "metadata": {},
   "source": [
    "## CalculateEmbeddings\n",
    "\n",
    "This script:\n",
    "- Loads list of sample points, each with 4 associated image files from an H5 data store\n",
    "- Calculates an image embedding for each image file\n",
    "- Saves the embeddings with the other metadata in the H5 store."
   ]
  },
  {
   "cell_type": "code",
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-02-25T11:52:23.104491Z",
     "start_time": "2026-02-25T11:52:20.909718Z"
    }
   },
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "#os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import h5py"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ad3f525e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:52:23.126632Z",
     "start_time": "2026-02-25T11:52:23.118023Z"
    }
   },
   "source": [
    "from directory_filepaths import *\n",
    "\n",
    "print(\"Directory paths are:\")\n",
    "print(\"data_dir: \", data_dir)\n",
    "print(\"lsoas_file: \", lsoas_file)\n",
    "print(\"imd_file: \", imd_file)\n",
    "print(\"h5_filename: \", h5_filename)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory paths are:\n",
      "data_dir:  ../../../../data/embeddings/\n",
      "lsoas_file:  ../../../../data/SpatialData/LSOAs_2021/LSOA_2021_EW_BSC_V4.shp\n",
      "imd_file:  /Users/geonsm/gp/INTEGRATE/llm/data/imd/File_2_-_IoD2025_Domains_of_Deprivation.xlsx\n",
      "h5_filename:  ../../../../data/embeddings/sample_points_cache/street_data.h5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points\n",
    "\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.ipynb  \n",
    "\n",
    "Each point has an ID, a latitude, a longitude, and 4 image files associated with it (these are sampled in each of the 4 cardinal directions from the sample point)  \n",
    "\n",
    "This script will also create an 'embeddings' slot that it will fill with a list of embeddings for each of the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "id": "b78ce937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:52:23.139888Z",
     "start_time": "2026-02-25T11:52:23.132344Z"
    }
   },
   "source": [
    "with h5py.File(h5_filename, \"r\") as f:\n",
    "    print(f\"H5 file contains {len(f[\"point_id\"])} items\")\n",
    "    print(\"Keys are: \", list(f.keys()))\n",
    "\n",
    "#points_data_cache = data_dir + \"sample_points_cache/points_data_cache.pkl\"\n",
    "#with open(points_data_cache, \"rb\") as f:\n",
    "#        point_records = pickle.load(f)\n",
    "#print(f\"Cache currently has {len(point_records)} points.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 file contains 18897 items\n",
      "Keys are:  ['date', 'embeddings_clip', 'image_paths', 'images_jpeg', 'images_present', 'latitude', 'longitude', 'point_id']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "# Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "b82fb7d2",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-02-25T11:52:25.133372Z",
     "start_time": "2026-02-25T11:52:23.145345Z"
    }
   },
   "source": [
    "# Define model and device to run it\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():  # macs\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():  # for completeness if you ever run on CUDA\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "91bcc42b",
   "metadata": {},
   "source": [
    "## Create embedding for each image and find similarity to categories \n",
    "- Create embedding for image\n",
    "- Find similarity score to text embedding for each category\n",
    "- Convert similarity score to a \"probability-like number\" using softmax"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare the database (we need an embeddings column)",
   "id": "b622ac3b0737f2ad"
  },
  {
   "cell_type": "code",
   "id": "d6230310-cba1-40ae-b4fb-d50b921c2ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T11:52:25.177048Z",
     "start_time": "2026-02-25T11:52:25.155362Z"
    }
   },
   "source": [
    "dim = 512  # Length of CLIP embeddings\n",
    "overwrite_embeddings = False  # Set to True to recalculate and overwrite existing embeddings\n",
    "calculate_embeddings = True  # Will be set to False if we skip (i.e. if embeddins have already been calculated)\n",
    "\n",
    "with h5py.File(h5_filename, \"a\") as f:\n",
    "    N = f[\"point_id\"].shape[0]\n",
    "\n",
    "    if \"embeddings_clip\" in f:\n",
    "        if not overwrite_embeddings:\n",
    "            existing = f[\"embeddings_clip\"][:]\n",
    "            n_populated = np.count_nonzero(~np.all(np.isnan(existing), axis=-1)) // 4\n",
    "            print(f\"embeddings_clip already exists with {n_populated}/{N} points populated.\")\n",
    "            print(\"Skipping. Set overwrite_embeddings = True to recalculate.\")\n",
    "            calculate_embeddings = False\n",
    "        else:\n",
    "            print(\"overwrite_embeddings is True, replacing existing embeddings.\")\n",
    "            del f[\"embeddings_clip\"]\n",
    "            f.create_dataset(\n",
    "                \"embeddings_clip\",\n",
    "                shape=(N, 4, dim),\n",
    "                dtype=\"float32\",\n",
    "                fillvalue=np.nan\n",
    "            )\n",
    "            print(f\"Created embeddings_clip dataset with shape ({N}, 4, {dim})\")\n",
    "    else:\n",
    "        f.create_dataset(\n",
    "            \"embeddings_clip\",\n",
    "            shape=(N, 4, dim),\n",
    "            dtype=\"float32\",\n",
    "            fillvalue=np.nan\n",
    "        )\n",
    "        print(f\"Created embeddings_clip dataset with shape ({N}, 4, {dim})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite_embeddings is True, replacing existing embeddings.\n",
      "Created embeddings_clip dataset with shape (18897, 4, 512)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate the embeddings (unless they have already been calculated)",
   "id": "e43348e7b13edc81"
  },
  {
   "cell_type": "code",
   "id": "c5f237c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T12:21:37.787858Z",
     "start_time": "2026-02-25T11:52:25.181739Z"
    }
   },
   "source": "def load_pil_from_h5(f, row_idx, slot):\n    \"\"\"\n    Returns a PIL.Image for the given row and slot, reading 'images_jpeg' bytes.\n    Raises FileNotFoundError if images_present is False.\n    \"\"\"\n    if not bool(f[\"images_present\"][row_idx, slot]):\n        raise FileNotFoundError(f\"No image stored at row {row_idx}, slot {slot}\")\n\n    jpeg_bytes = f[\"images_jpeg\"][row_idx, slot].tobytes()\n    return Image.open(io.BytesIO(jpeg_bytes)).convert(\"RGB\")\n\n\ndef embed_clip_pil(pil_image):\n    \"\"\"\n    Compute a CLIP embedding from a PIL image.\n    Returns a (D,) float32 numpy array (unit-normalized).\n    \"\"\"\n    image_tensor = preprocess(pil_image).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        raw = model.encode_image(image_tensor)\n        emb = raw / raw.norm(dim=-1, keepdim=True)\n\n    return emb.squeeze(0).detach().cpu().numpy().astype(\"float32\")\n\n\nif not calculate_embeddings:\n    print(\"Skipping embedding calculation (embeddings already exist).\")\nelse:\n    with h5py.File(h5_filename, \"a\") as f:\n        if \"embeddings_clip\" not in f:\n            raise KeyError(\"No embeddings file, this should have been created above.\")\n\n        emb_ds = f[\"embeddings_clip\"]    # shape: (N, 4, D)\n        N, _, D = emb_ds.shape\n        print(f\"Embeddings shape: {emb_ds.shape}\")\n\n        for i in tqdm(range(N), desc=\"Embedding CLIP from HDF5\", unit=\"point\"):\n            for j in range(4):\n                try:\n                    # Decode image from HDF5\n                    pil_img = load_pil_from_h5(f, i, j)\n\n                    # Compute embedding and write it\n                    emb = embed_clip_pil(pil_img)  # (D,)\n                    if emb.shape[0] != D:\n                        raise ValueError(f\"Embedding dim mismatch: got {emb.shape[0]}, expected {D}\")\n                    emb_ds[i, j, :] = emb\n                except FileNotFoundError:\n                    # Missing image -> leave NaNs\n                    tqdm.write(f\"Missing image at {i}, {j}, not calculating the embedding\")\n                    emb_ds[i, j, :] = np.nan\n                except Exception as e:\n                    # Any other error -> write NaNs to mark as missing/bad\n                    emb_ds[i, j, :] = np.nan\n                    tqdm.write(f\"Error at row {i}, slot {j}: {e}\")\n\n    print(\"Finished\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (18897, 4, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding CLIP from HDF5:  48%|████▊     | 9160/18897 [14:12<14:21, 11.31point/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing image at 9158, 1, not calculating the embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding CLIP from HDF5:  99%|█████████▉| 18667/18897 [28:51<00:20, 11.10point/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing image at 18664, 3, not calculating the embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding CLIP from HDF5: 100%|██████████| 18897/18897 [29:12<00:00, 10.78point/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "86463360-171b-40ff-822b-163582627ec7",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "To finish just print a bit of information about what's in the H5 data store"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8a32737-0615-461a-bbbc-c0b5c1ed10b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T13:15:45.571348Z",
     "start_time": "2026-02-25T13:15:45.481051Z"
    }
   },
   "source": "with h5py.File(h5_filename, \"r\") as f:\n    print(f\"H5 data store: {h5_filename}\")\n    print(f\"{'Dataset':<20} {'Shape':<25} {'Dtype'}\")\n    print(\"-\" * 60)\n    for key in sorted(f.keys()):\n        ds = f[key]\n        print(f\"{key:<20} {str(ds.shape):<25} {ds.dtype}\")\n\n    N = f[\"point_id\"].shape[0]\n    n_images = np.count_nonzero(f[\"images_present\"][:])\n    print(f\"\\nTotal points: {N}\")\n    print(f\"Images present: {n_images} / {N * 4}\")\n\n    if \"embeddings_clip\" in f:\n        emb = f[\"embeddings_clip\"][:]\n        n_emb = np.count_nonzero(~np.all(np.isnan(emb), axis=-1))\n        print(f\"Embeddings populated: {n_emb} / {N * 4}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 data store: ../../../../data/embeddings/sample_points_cache/street_data.h5\n",
      "Dataset              Shape                     Dtype\n",
      "------------------------------------------------------------\n",
      "date                 (18897,)                  |S10\n",
      "embeddings_clip      (18897, 4, 512)           float32\n",
      "image_paths          (18897, 4)                |S512\n",
      "images_jpeg          (18897, 4)                object\n",
      "images_present       (18897, 4)                bool\n",
      "latitude             (18897,)                  float64\n",
      "longitude            (18897,)                  float64\n",
      "point_id             (18897,)                  int32\n",
      "\n",
      "Total points: 18897\n",
      "Images present: 75586 / 75588\n",
      "Embeddings populated: 75586 / 75588\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7ca45a69ac46b34a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
