{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af254837",
   "metadata": {},
   "source": [
    "## CalculateEmbeddings\n",
    "\n",
    "This script:\n",
    "- Loads list of sample points, each with 4 associated image files\n",
    "- Calculates an image embedding for each image file\n",
    "- Saves a pickle file containig each sample point, the list of image files and the list of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3f525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directory_filepaths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points\n",
    "\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.ipynb  \n",
    "\n",
    "Each point has an ID, a latitude, a longitude, and 4 image files associated with it (these are sampled in each of the 4 cardinal directions from the sample point)  \n",
    "\n",
    "This script will also create an 'embeddings' slot that it will fill with a list of embeddings for each of the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78ce937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 file contains 18897 items\n",
      "Keys are:  ['date', 'embeddings_clip', 'image_paths', 'images_jpeg', 'images_present', 'latitude', 'longitude', 'point_id']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_filename, \"r\") as f:\n",
    "    print(f\"H5 file contains {len(f[\"point_id\"])} items\")\n",
    "    print(\"Keys are: \", list(f.keys()))\n",
    "\n",
    "#points_data_cache = data_dir + \"sample_points_cache/points_data_cache.pkl\"\n",
    "#with open(points_data_cache, \"rb\") as f:\n",
    "#        point_records = pickle.load(f)\n",
    "#print(f\"Cache currently has {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "# Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82fb7d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define model and device to run it\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():  # macs\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():  # for completeness if you ever run on CUDA\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcc42b",
   "metadata": {},
   "source": [
    "## Create embedding for each image and find similarity to categories \n",
    "- Create embedding for image\n",
    "- Find similarity score to text embedding for each category\n",
    "- Convert similarity score to a \"probability-like number\" using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6230310-cba1-40ae-b4fb-d50b921c2ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings exist, replacing them\n",
      "Created embeddings_clip dataset with shape (18897, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the database (we need an embeddings column)\n",
    "dim = 512  # Length nof CLIP embedings\n",
    "\n",
    "with h5py.File(h5_filename, \"a\") as f:\n",
    "    N = f[\"point_id\"].shape[0]\n",
    "\n",
    "    # If exists, delete it\n",
    "    if \"embeddings_clip\" in f:\n",
    "        print(\"Embeddings exist, replacing them\")\n",
    "        del f[\"embeddings_clip\"]\n",
    "\n",
    "    # Create new dataset\n",
    "    f.create_dataset(\n",
    "        \"embeddings_clip\",\n",
    "        shape=(N, 4, dim),  # 4 embeddings per point\n",
    "        dtype=\"float32\",\n",
    "        fillvalue=np.nan\n",
    "    )\n",
    "\n",
    "print(f\"Created embeddings_clip dataset with shape ({N}, 4, {dim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f237c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (18897, 4, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding CLIP from HDF5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18897/18897 [28:44<00:00, 10.96point/s]\n"
     ]
    }
   ],
   "source": [
    "def load_pil_from_h5(f, row_idx, slot):\n",
    "    \"\"\"\n",
    "    Returns a PIL.Image for the given row and slot, reading 'images_jpeg' bytes.\n",
    "    Raises FileNotFoundError if images_present is False.\n",
    "    \"\"\"\n",
    "    if not bool(f[\"images_present\"][row_idx, slot]):\n",
    "        raise FileNotFoundError(f\"No image stored at row {row_idx}, slot {slot}\")\n",
    "\n",
    "    jpeg_bytes = f[\"images_jpeg\"][row_idx, slot].tobytes()\n",
    "    return Image.open(io.BytesIO(jpeg_bytes)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def embed_clip_pil(pil_image):\n",
    "    \"\"\"\n",
    "    Compute a CLIP embedding from a PIL image.\n",
    "    Returns a (D,) float32 numpy array (unit-normalized).\n",
    "    \"\"\"\n",
    "    image_tensor = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        raw = model.encode_image(image_tensor)\n",
    "        emb = raw / raw.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return emb.squeeze(0).detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "with h5py.File(h5_filename, \"a\") as f:\n",
    "    if \"embeddings_clip\" not in f:\n",
    "        raise KeyError(\"No embeddings file, this should have been created above.\")\n",
    "\n",
    "    emb_ds = f[\"embeddings_clip\"]    # shape: (N, 4, D)\n",
    "    N, _, D = emb_ds.shape\n",
    "    print(f\"Embeddings shape: {emb_ds.shape}\")\n",
    "\n",
    "    for i in tqdm(range(N), desc=\"Embedding CLIP from HDF5\", unit=\"point\"):\n",
    "        for j in range(4):\n",
    "            try:\n",
    "                # Decode image from HDF5\n",
    "                pil_img = load_pil_from_h5(f, i, j)\n",
    "\n",
    "                # Compute embedding and write it\n",
    "                emb = embed_clip_pil(pil_img)  # (D,)\n",
    "                if emb.shape[0] != D:\n",
    "                    raise ValueError(f\"Embedding dim mismatch: got {emb.shape[0]}, expected {D}\")\n",
    "                emb_ds[i, j, :] = emb\n",
    "            except FileNotFoundError:\n",
    "                # Missing image → leave NaNs\n",
    "                tqdm.write(f\"Missing image at {i}, {j}, not calculating the embedding\")\n",
    "                emb_ds[i, j, :] = np.nan\n",
    "            except Exception as e:\n",
    "                # Any other error → write NaNs to mark as missing/bad\n",
    "                emb_ds[i, j, :] = np.nan\n",
    "                tqdm.write(f\"⚠️ Error at row {i}, slot {j}: {e}\")\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86463360-171b-40ff-822b-163582627ec7",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a32737-0615-461a-bbbc-c0b5c1ed10b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
