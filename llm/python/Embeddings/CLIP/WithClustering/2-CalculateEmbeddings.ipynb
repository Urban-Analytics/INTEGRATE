{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af254837",
   "metadata": {},
   "source": [
    "## CalculateEmbeddings\n",
    "\n",
    "This script:\n",
    "- Loads list of sample points, each with 4 associated image files\n",
    "- Calculates an image embedding for each image file\n",
    "- Saves a pickle file containig each sample point, the list of image files and the list of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from directory_filepaths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.iypnb  \n",
    "Each point has a latitude, a longitude, and 4 image files associated with it (these are sampled in each of the 4 cardinal directions from the sample point)  \n",
    "It also contains an 'embedding' slot, which this script will fill with a list of embeddings for each of the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78ce937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache.pkl\"\n",
    "with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "print(f\"Cache currently has {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "# Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82fb7d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcc42b",
   "metadata": {},
   "source": [
    "## Create embedding for each image and find similarity to categories \n",
    "- Create embedding for image\n",
    "- Find similarity score to text embedding for each category\n",
    "- Convert similarity score to a \"probability-like number\" using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f237c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points:   0%|                                                                                                           | 1/18897 [00:07<39:19:15,  7.49s/point]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def embed_clip(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image, computes its CLIP embedding, \n",
    "\n",
    "    Returns:\n",
    "        image_embedding (np.array): 512-dim CLIP image embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. LOAD AND PREPROCESS THE IMAGE\n",
    "    # -----------------------------------------------------------\n",
    "    # Load image using PIL and convert to 3-channel RGB\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Apply CLIP preprocessing:\n",
    "    # - resize/crop to 224x224\n",
    "    # - convert to torch tensor\n",
    "    # - normalise pixels with CLIPâ€™s mean/std\n",
    "    # This produces a tensor of shape (3, 224, 224)\n",
    "    image_tensor = preprocess(pil_image)\n",
    "\n",
    "    # Add a batch dimension â†’ (1, 3, 224, 224)\n",
    "    # Required because CLIP expects a batch\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move tensor to CPU or GPU depending on device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. RUN CLIP TO GET IMAGE EMBEDDING\n",
    "    # -----------------------------------------------------------\n",
    "    # Disable gradient tracking \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encode the image â†’ produces a 512-dim CLIP embedding\n",
    "        raw_image_embedding = model.encode_image(image_tensor)\n",
    "\n",
    "        # Normalise embedding to unit length (important for cosine similarity)\n",
    "        image_embedding = raw_image_embedding / raw_image_embedding.norm(\n",
    "            dim=-1, keepdim=True)\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. RETURN CLEAN CPU NUMPY ARRAYS\n",
    "    # -----------------------------------------------------------\n",
    "    return image_embedding.cpu().numpy()[0]\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records, desc=\"Embedding points\", unit=\"point\"):\n",
    "# for rec in point_records[200:300]:    \n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []\n",
    "    rec[\"category_probs\"] = []\n",
    "    \n",
    "    for img_path in rec[\"image_files\"]:\n",
    "\n",
    "        try:\n",
    "            embedding = embed_clip(img_path)\n",
    "            \n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            \n",
    "            # Testing\n",
    "#             img = Image.open(img_path)\n",
    "#             fig,ax=plt.subplots(figsize=(2,2))\n",
    "#             ax.axis(\"off\")\n",
    "#             plt.imshow(img)\n",
    "#             plt.show()\n",
    "#             argmax = scores.argmax()\n",
    "#             print(list(multi_prompts.keys())[argmax])\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âš ï¸ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37771f37",
   "metadata": {},
   "source": [
    "## Save outputs to pickle file\n",
    "\n",
    "This contains points sampled along the road network in 1-SampleStreetNetwork.iypnb.    \n",
    "Each point has a latitude, a longitude, and 4 image files associated with it.  \n",
    "It also contains an 'embedding' slot, with a list of embeddings for each of the 4 images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bfb2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file = (data_dir + \"embeddings/points_data_cache_with_CLIP_embeddings.pkl\")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved embeddings + category scores for {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746178c",
   "metadata": {},
   "source": [
    "## Testing how CLIP works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad3740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_path = point_records[2][\"image_files\"][3]\n",
    "# img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../../\")\n",
    "# img = Image.open(img_path)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc68b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# from PIL import Image\n",
    "\n",
    "# #Load CLIP model\n",
    "# model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f0578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Encode text descriptions\n",
    "# text_emb = model.encode(['Two dogs in the snow', \"a pizza\", 'A cat on a table', 'A picture of a road, with cars and trees'])\n",
    "# text_emb = model.encode([\"a cucumber\", \"semi-detached house\", \"a highway with few cars and grass embankments\", \"a car\", \"a view down a road\", \"a park\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718df4da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Encode an image:\n",
    "# img_emb = model.encode(Image.open(img_path))\n",
    "\n",
    "# #Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5b5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text_emb = model.encode([\"a house\", \"a shop\", \"a car\", \"a road\", \"a park\"])\n",
    "\n",
    "# #Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
