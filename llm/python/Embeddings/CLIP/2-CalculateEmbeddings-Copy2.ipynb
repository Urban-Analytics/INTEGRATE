{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "data_dir = os.path.join(\"../../../data/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached point data â€¦\n",
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if os.path.isfile(points_data_cache):\n",
    "    print(\"Loading cached point data â€¦\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82fb7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built improved category text embeddings: torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding points: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:20<00:00,  3.84point/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saved embeddings + category scores for 18897 points.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# ---- Create final text embeddings ----\n",
    "final_text_features = []\n",
    "category_names = []  # so your categories keep their correct order\n",
    "\n",
    "multi_prompts = {\n",
    "    \"C â€“ Accommodation\": [\n",
    "        \"a photo of a house or home\",\n",
    "        \"an apartment building on a street\",\n",
    "        \"houses in a residential neighborhood\",\n",
    "        \"front view of a suburban house\",\n",
    "        \"a cozy home exterior with a garden\"\n",
    "    ],\n",
    "    \"B â€“ Industrial / Storage\": [\n",
    "        \"a warehouse or big industrial building\",\n",
    "        \"a factory with chimneys or machinery\",\n",
    "        \"storage containers outside a building\",\n",
    "        \"a logistics yard with trucks and crates\",\n",
    "        \"industrial buildings in an urban area\"\n",
    "    ],\n",
    "    \"E â€“ Commercial / Business / Service\": [\n",
    "        \"a shop or cafe on the street\",\n",
    "        \"a busy high street with stores\",\n",
    "        \"a restaurant or small business front\",\n",
    "        \"office building in the city\",\n",
    "        \"people outside a retail store or service\"\n",
    "    ],\n",
    "    \"F â€“ Local Community / Learning\": [\n",
    "        \"a school or university building\",\n",
    "        \"library or community centre\",\n",
    "        \"children playing at a sports field\",\n",
    "        \"outdoor playground or swimming pool\",\n",
    "        \"museum, gallery or exhibition space\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# multi_prompts = {\n",
    "#     \"indoor\": [\n",
    "#         \"a photo taken indoors\",\n",
    "#         \"an indoor interior scene\",\n",
    "#         \"inside a building\",\n",
    "#         \"an indoor room photo\"\n",
    "#     ],\n",
    "#     \"terraced house\": [\n",
    "#         \"a photo of a terraced house\",\n",
    "#         \"a row of terraced homes\",\n",
    "#         \"a UK terrace housing street\",\n",
    "#         \"a brick terraced house\"\n",
    "#     ],\n",
    "#     \"semi-detached house\": [\n",
    "#         \"a photo of a detached or semi detached house\",\n",
    "#         \"a suburban detached house\",\n",
    "#         \"a single-family home\",\n",
    "#         \"a detached house on a residential street\"\n",
    "#     ],\n",
    "#     \"road\": [\n",
    "#         \"a photo of a road\",\n",
    "#         \"a street with cars or buildings\",\n",
    "#         \"a roadway scene\",\n",
    "#         \"a street-level view outdoors\"\n",
    "#     ],\n",
    "#     \"shop\": [\n",
    "#         \"a photo of a shop\",\n",
    "#         \"a store front\",\n",
    "#         \"a retail business on a street\",\n",
    "#         \"a commercial storefront\"\n",
    "#     ],\n",
    "#     \"car\": [\n",
    "#         \"a photo dominated by the outside of a car\",\n",
    "#         \"a vehicle exterior close-up\",\n",
    "#         \"a photo of a parked car\",\n",
    "#         \"a car on the street\"\n",
    "#     ],\n",
    "#     \"industrial\": [\n",
    "#         \"a photo of an industrial building\",\n",
    "#         \"a warehouse or factory building\",\n",
    "#         \"an industrial site\",\n",
    "#         \"a manufacturing facility\"\n",
    "#     ],\n",
    "#     \"wasteland\": [\n",
    "#         \"a photo of wasteland or empty space\",\n",
    "#         \"an abandoned empty outdoor area\",\n",
    "#         \"a derelict vacant lot\",\n",
    "#         \"an unused or empty land area\"\n",
    "#     ],\n",
    "#     \"greenery\": [\n",
    "#         \"a photo of nice green space\",\n",
    "#         \"a park or garden with trees\",\n",
    "#         \"green plants and nature\",\n",
    "#         \"a photo of natural greenery\"]}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cat, prompts in multi_prompts.items():\n",
    "        category_names.append(cat)\n",
    "\n",
    "        # Encode all prompts for this category\n",
    "        tokenized = clip.tokenize(prompts).to(device)\n",
    "        txt_feats = model.encode_text(tokenized)\n",
    "\n",
    "        # Normalise each embedding\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Average across prompts\n",
    "        avg_feat = txt_feats.mean(dim=0)\n",
    "\n",
    "        # Normalise again (important!)\n",
    "        avg_feat = avg_feat / avg_feat.norm()\n",
    "\n",
    "        final_text_features.append(avg_feat.cpu())\n",
    "\n",
    "# Convert to tensor of shape (num_categories, 512)\n",
    "final_text_features = torch.stack(final_text_features)\n",
    "print(\"Built improved category text embeddings:\", final_text_features.shape)\n",
    "\n",
    "\n",
    "def embed_and_score_clip(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Image embedding\n",
    "        image_feat = model.encode_image(image_tensor)\n",
    "        image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity to averaged text embeddings\n",
    "        sims = (image_feat @ final_text_features.to(device).T).softmax(dim=-1)\n",
    "\n",
    "    return (\n",
    "        image_feat.cpu().numpy()[0],   # 512D image embedding\n",
    "        sims.cpu().numpy()[0]          # category probabilities\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Placeholder stats\n",
    "# ------------------------------\n",
    "def empty_attention_stats():\n",
    "    return {\"mean\": None, \"std\": None, \"entropy\": None, \"max_y\": None, \"max_x\": None}\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records[:1000], desc=\"Embedding points\", unit=\"point\"):\n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []\n",
    "    rec[\"attn_stats\"] = []\n",
    "    rec[\"heatmap_paths\"] = []\n",
    "\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "\n",
    "        # Use ORIGINAL images\n",
    "        img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../../\")\n",
    "\n",
    "        try:\n",
    "            embedding, scores = embed_and_score_clip(img_path)\n",
    "\n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            rec[\"category_scores\"].append(scores)\n",
    "            rec[\"attn_stats\"].append(empty_attention_stats())\n",
    "            rec[\"heatmap_paths\"].append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âš ï¸ Error: {e}\")\n",
    "\n",
    "output_file = (data_dir + \"sample_points_cache/points_data_cache_with_CLIP_embeddings_and_scores_planninguseclasses_short.pkl\")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Saved embeddings + category scores for {len(point_records)} points.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
