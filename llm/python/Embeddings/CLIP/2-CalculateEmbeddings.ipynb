{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/nfs/a319/gy17m2a/scratch/hf_cache\"\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "data_dir = os.path.join(\"../../../data/embeddings/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6363",
   "metadata": {},
   "source": [
    "### Load list of sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache currently has 18897 points.\n"
     ]
    }
   ],
   "source": [
    "points_data_cache = data_dir + \"sample_points_cache/points_data_cache_with_embeddings.pkl\"\n",
    "with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "print(f\"Cache currently has {len(point_records)} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "# Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82fb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc589",
   "metadata": {},
   "source": [
    "## Create text embedding for categories we want to match image embeddings to\n",
    "\n",
    "- For each headline category, define several different prompts\n",
    "- Convert each of the subprompts into a text embedding\n",
    "- For each headling category, find the mean text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b45473",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_prompts = {\n",
    "    \"C ‚Äì Accommodation\": [\n",
    "        \"a photo of a house or home\",\n",
    "        \"an apartment building on a street\",\n",
    "        \"houses in a residential neighborhood\",\n",
    "        \"front view of a suburban house\",\n",
    "        \"a cozy home exterior with a garden\"],\n",
    "    \"B ‚Äì Industrial / Storage\": [\n",
    "        \"a warehouse or big industrial building\",\n",
    "        \"a factory with chimneys or machinery\",\n",
    "        \"storage containers outside a building\",\n",
    "        \"a logistics yard with trucks and crates\",\n",
    "        \"industrial buildings in an urban area\"],\n",
    "    \"E ‚Äì Commercial / Business / Service\": [\n",
    "        \"a shop or cafe on the street\",\n",
    "        \"a busy high street with stores\",\n",
    "        \"a restaurant or small business front\",\n",
    "        \"office building in the city\",\n",
    "        \"people outside a retail store or service\"],\n",
    "    \"F ‚Äì Local Community / Learning\": [\n",
    "        \"a school or university building\",\n",
    "        \"library or community centre\",\n",
    "        \"children playing at a sports field\",\n",
    "        \"outdoor playground or swimming pool\",\n",
    "        \"museum, gallery or exhibition space\"]}\n",
    "\n",
    "multi_prompts = {\n",
    "    \"indoor\": [\"a photo taken indoors\",\n",
    "        \"an indoor interior scene\",\n",
    "        \"inside a building\",\n",
    "        \"an indoor room photo\"],\n",
    "    \"terraced house\": [\"a photo of a terraced house\",\n",
    "        \"a row of terraced homes\",\n",
    "        \"a UK terrace housing street\",\n",
    "        \"a brick terraced house\"],\n",
    "    \"semi-detached house\": [\n",
    "        \"a photo of a detached or semi detached house\",\n",
    "        \"a suburban detached house\",\n",
    "        \"a single-family home\",\n",
    "        \"a detached house on a residential street\"],\n",
    "    \"road\": [\"a photo of a road\",\n",
    "        \"a street with cars or buildings\",\n",
    "        \"a roadway scene\",\n",
    "        \"a street-level view outdoors\"],\n",
    "    \"shop\": [\"a photo of a shop\",\n",
    "        \"a store front\",\n",
    "        \"a retail business on a street\",\n",
    "        \"a commercial storefront\"],\n",
    "    \"car\": [\"a photo dominated by the outside of a car\",\n",
    "        \"a vehicle exterior close-up\",\n",
    "        \"a photo of a parked car\",\n",
    "        \"a car on the street\"],\n",
    "    \"industrial\": [\"a photo of an industrial building\",\n",
    "        \"a warehouse or factory building\",\n",
    "        \"an industrial site\",\n",
    "        \"a manufacturing facility\"],\n",
    "    \"wasteland\": [\"a photo of wasteland or empty space\",\n",
    "        \"an abandoned empty outdoor area\",\n",
    "        \"a derelict vacant lot\",\n",
    "        \"an unused or empty land area\"],\n",
    "    \"greenery\": [\"a photo of nice green space\",\n",
    "        \"a park or garden with trees\",\n",
    "        \"green plants and nature\",\n",
    "        \"a photo of natural greenery\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40a5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of embeddings for each of the 4 headline categories\n",
    "final_text_features = []\n",
    "category_names = []  \n",
    "\n",
    "# This line tells PyTorch that we're not training CLIP (just using to calculate embeddings), so don't need to compute gradients\n",
    "# This makes the computation faster\n",
    "with torch.no_grad():\n",
    "    # Loop through each category and its list of text prompts\n",
    "    for cat, prompts in multi_prompts.items():\n",
    "\n",
    "        # Add the category name to your list (used later for plotting or indexing)\n",
    "        category_names.append(cat)\n",
    "\n",
    "        # Convert all textual prompts into CLIP token IDs \n",
    "        # Token IDs are numerical codes that represent words or sub-words\n",
    "        tokenized = clip.tokenize(prompts).to(device)\n",
    "\n",
    "        # Encode all the token IDs into CLIP text embeddings\n",
    "        txt_feats = model.encode_text(tokenized)\n",
    "\n",
    "        # Normalise each prompt embedding to unit length\n",
    "        # (CLIP uses cosine similarity, so normalisation matters)\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute the mean embedding across all prompts for this category\n",
    "        # This creates a single \"category embedding\" representing all its prompts\n",
    "        avg_feat = txt_feats.mean(dim=0)\n",
    "\n",
    "        # Normalise the averaged embedding again\n",
    "        # This ensures it remains a proper CLIP embedding for cosine similarity\n",
    "        avg_feat = avg_feat / avg_feat.norm()\n",
    "\n",
    "        # Save this averaged category embedding\n",
    "        final_text_features.append(avg_feat.cpu())\n",
    "\n",
    "# Convert to tensor of shape (num_categories, 512)\n",
    "# A tensor is a multi-dimensional array, and is the format expected by PyTorch\n",
    "final_text_features = torch.stack(final_text_features)\n",
    "print(\"Built improved category text embeddings:\", final_text_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcc42b",
   "metadata": {},
   "source": [
    "## Create embedding for each image and find similarity to categories \n",
    "- Create embedding for image\n",
    "- Find similarity score to text embedding for each category\n",
    "- Convert similarity score to a \"probability-like number\" using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f237c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_score_clip(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image, computes its CLIP embedding, \n",
    "    and calculates similarity-based category probabilities.\n",
    "\n",
    "    Returns:\n",
    "        image_embedding (np.array): 512-dim CLIP image embedding\n",
    "        category_probabilities (np.array): Probability for each category\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1. LOAD AND PREPROCESS THE IMAGE\n",
    "    # -----------------------------------------------------------\n",
    "    # Load image using PIL and convert to 3-channel RGB\n",
    "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Apply CLIP preprocessing:\n",
    "    # - resize/crop to 224x224\n",
    "    # - convert to torch tensor\n",
    "    # - normalise pixels with CLIP‚Äôs mean/std\n",
    "    # This produces a tensor of shape (3, 224, 224)\n",
    "    image_tensor = preprocess(pil_image)\n",
    "\n",
    "    # Add a batch dimension ‚Üí (1, 3, 224, 224)\n",
    "    # Required because CLIP expects a batch\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move tensor to CPU or GPU depending on device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. RUN CLIP TO GET IMAGE EMBEDDING\n",
    "    # -----------------------------------------------------------\n",
    "    # Disable gradient tracking \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encode the image ‚Üí produces a 512-dim CLIP embedding\n",
    "        raw_image_embedding = model.encode_image(image_tensor)\n",
    "\n",
    "        # Normalise embedding to unit length (important for cosine similarity)\n",
    "        image_embedding = raw_image_embedding / raw_image_embedding.norm(\n",
    "            dim=-1, keepdim=True)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 3. COMPUTE SIMILARITIES TO TEXT CATEGORY EMBEDDINGS\n",
    "        # -----------------------------------------------------------\n",
    "        # Returns similarity of the 1 image embedding to N text embeddings\n",
    "        # These are dot products, representing how close the image is to each category in embedding space\n",
    "        similarity_scores = (image_embedding @ final_text_features.to(device).T)\n",
    "\n",
    "        ######## Convert raw similarities to probabilities\n",
    "        # Softmax is a function that turns a set of numbers into a probability-like distribution\n",
    "        # However, the numbers do not represent true probabilities\n",
    "        # e.g. Scores of Indoor: 0.75, Greenery: 0.18, Terraced house: 0.04, Road: 0.02, Shop: 0.01\n",
    "        # Mean that The \"indoor\" text embedding was much closer to the image embedding than the others.\n",
    "        # And NOT that the true probability that the scene is indoors is 75%.\n",
    "        category_probabilities = similarity_scores.softmax(dim=-1)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4. RETURN CLEAN CPU NUMPY ARRAYS\n",
    "    # -----------------------------------------------------------\n",
    "    return (\n",
    "        image_embedding.cpu().numpy()[0],       # shape (512,)\n",
    "        category_probabilities.cpu().numpy()[0] # shape (num_categories,)\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Embed all images\n",
    "# ------------------------------\n",
    "for rec in tqdm(point_records, desc=\"Embedding points\", unit=\"point\"):\n",
    "\n",
    "    rec[\"embedding\"] = []\n",
    "    rec[\"category_scores\"] = []\n",
    "\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "\n",
    "        # Use ORIGINAL images\n",
    "        img_path = img_path.replace(\"airbnb-manchester/\", \"embeddings/\").replace(\"../\", \"../../../\")\n",
    "\n",
    "        try:\n",
    "            embedding, scores = embed_and_score_clip(img_path)\n",
    "\n",
    "            rec[\"embedding\"].append(embedding)\n",
    "            rec[\"category_scores\"].append(scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37771f37",
   "metadata": {},
   "source": [
    "## Save outputs to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = (data_dir + \"sample_points_cache/points_data_cache_with_CLIP_embeddings_and_scores_planninguseclasses.pkl\")\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"\\nüíæ Saved embeddings + category scores for {len(point_records)} points.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
