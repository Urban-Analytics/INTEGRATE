{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - _Do some analysis of the embeddings_ (Added by Nick later)\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification (_or, more recently, deprivation) for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import torch\n",
    "from tqdm.auto import tqdm                    # auto picks notebook / console style\n",
    "import multiprocessing\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Spatial stuff\n",
    "import folium\n",
    "import shapely\n",
    "from shapely.geometry import Point, LineString\n",
    "from libpysal.weights import KNN\n",
    "from esda.moran import Moran\n",
    "import contextily as cx\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# For caching models\n",
    "from joblib import dump, load\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Hugging Face Transformers for image embedding\n",
    "from transformers import AutoImageProcessor, AutoModel  # will load a vision model\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_hex  # For creating hex colours\n",
    "\n",
    "\n",
    "# ----------------- Configuration -----------------\n",
    "DOWNLOAD_IMAGES = False  # If false then don't download any images, just load those that have been cached\n",
    "\n",
    "# Can decide, after analysing images, which models we want to run (to predict gentrification and/or deprivation)\n",
    "RUN_GENTRIFICATION_MODEL = False  \n",
    "RUN_IMD_MODEL = False  # (This one is actually more about running a grid search to find the optimal model\n",
    "                      # for the deprivation analysis. WHen False then it still runs models, but doesn't conduct the laborious search) \n",
    "\n",
    "# Data and cache dirs\n",
    "data_dir = Path(os.path.join(\"..\", \"data\", \"airbnb-manchester\"))\n",
    "boundary_file = os.path.join(data_dir, \"greater_manchester_lsoas.geojson\")  # Path to boundary polygon file\n",
    "gentrification_file = os.path.join(\"..\", \"data\", \"gmgi_data\", \"lsoa_summary_jan25.csv\")  # Path to polygons with gentrification index\n",
    "lsoas_file = os.path.join(\"..\", \"data\", \"LSOAs_2011\", \"LSOA_2011_EW_BSC_V4.shp\")\n",
    "imd_file = os.path.join(\"..\", \"data\", \"imd\", \"File_2_-_IoD2019_Domains_of_Deprivation.xlsx\")\n",
    "\n",
    "# For point sampling\n",
    "density_per_km = 0.3  # number of points to sample per km of road\n",
    "#density_per_km = 0.1  # VERY FEW WHILE TESTING\n",
    "sample_spacing = 200.0   # distance in meters between sample points on roads\n",
    "#sample_spacing = 5000.0  # Very large for testing\n",
    "n_directions = 4         # number of street view images per point (e.g., 4 cardinal directions)\n",
    "image_size = \"640x640\"   # requested image resolution from Street View API\n",
    "\n",
    "# Create directories for caching if not exist\n",
    "#Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "image_dir = Path(os.path.join(data_dir, \"street_images\"))\n",
    "image_dir.mkdir(exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a13cd0696105",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "85a11b33a5e00526",
   "metadata": {},
   "source": [
    "# --- Load neighbourhood polygons and dissolve to one study-area boundary ---\n",
    "boundary_neighs = gpd.read_file(boundary_file)\n",
    "# (Optional) keep original neighbourhoods for later mapping/stratified sampling\n",
    "#neighbourhoods_gdf = boundary_neighs.copy()\n",
    "# Make sure we're in WGS84 (lat/lon) for OSM and APIs\n",
    "boundary_neighs = boundary_neighs.to_crs(epsg=4326)\n",
    "# Dissolve: merge all geometries into one polygon (MultiPolygon possible)\n",
    "boundary_polygon = boundary_neighs.unary_union  # shapely (multi)polygon\n",
    "boundary_gdf = gpd.GeoDataFrame(\n",
    "    data={'name': ['study_area']},\n",
    "    geometry=[boundary_polygon],\n",
    "    crs=boundary_neighs.crs\n",
    ")\n",
    "\n",
    "# TEMP! Just keep one LSOA for testing\n",
    "#boundary_gdf = boundary_neighs.sample(1)\n",
    "\n",
    "print(\"Merged neighbourhoods into single study-area boundary.\")\n",
    "print(\"Bounds:\", boundary_polygon.bounds)\n",
    "boundary_gdf.plot(color='lightblue', edgecolor='black')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "179c742d-721d-41d7-8287-027c8d4bc523",
   "metadata": {},
   "source": [
    "## Read the LSOA boundary data\n",
    "\n",
    "(later it will be joined to the Greater Manchester Gentrification Index and IMD)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5dc5e31eb629a1e",
   "metadata": {},
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA11NMW'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "gm_lsoa.plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "932b35733a70d8a7",
   "metadata": {},
   "source": [
    "## Get Road Network for the Area from OSM"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f64dc94ca59522d",
   "metadata": {},
   "source": [
    "# Get (or cache) the road network with OSMnx’s built-in GraphML I/O\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Tell OSMnx to keep all cache files inside the project folder (optional)\n",
    "#ox.settings.cache_folder = str(data_dir / \"osmnx_http_cache\")  # HTTP/tile cache\n",
    "#ox.settings.use_cache = True  # default is True\n",
    "ox.settings.use_cache = False      # <- prevents the “cache” dir being written\n",
    "\n",
    "\n",
    "graph_file = data_dir / \"road_network.graphml\"  # one self-contained file\n",
    "\n",
    "if graph_file.exists():\n",
    "    print(\"Loading road network from GraphML cache …\")\n",
    "    road_graph = ox.load_graphml(graph_file)\n",
    "else:\n",
    "    print(\"Downloading road network from OSM …\")\n",
    "    road_graph = ox.graph_from_polygon(boundary_polygon, network_type=\"drive\")\n",
    "    ox.save_graphml(road_graph, graph_file)\n",
    "    print(f\"Graph saved to {graph_file}\")\n",
    "\n",
    "# Convert to GeoDataFrame of edges for downstream sampling/plotting\n",
    "edges_gdf = ox.graph_to_gdfs(road_graph, nodes=False)\n",
    "\n",
    "print(f\"Number of road segments: {len(edges_gdf)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "723fa123b2c7417",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot roads first (thin gray lines)\n",
    "edges_gdf.plot(ax=ax, linewidth=0.4, color=\"gray\")\n",
    "\n",
    "# Plot the study-area outline on top (thicker red line)\n",
    "boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c788f0f174d1fc23",
   "metadata": {},
   "source": [
    "## Generate a Sample of Points along the road network"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2cc0dd3dae623d5",
   "metadata": {},
   "source": [
    "if DOWNLOAD_IMAGES:\n",
    "    # Generate a sample of points along the street network (probabilistic)\n",
    "    # --------------------------------------------------------------------\n",
    "    # PARAMETERS\n",
    "    min_points_per_edge = 0  # allow very short edges to have none\n",
    "    max_points_per_edge = 10  # safety cap per edge\n",
    "\n",
    "    # Re-project roads to a metric CRS for length calculations\n",
    "    utm_crs = boundary_gdf.estimate_utm_crs()\n",
    "    edges_proj = edges_gdf.to_crs(utm_crs)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # Loop over every edge geometry\n",
    "    for geom in edges_proj.geometry:\n",
    "        if geom is None or geom.length == 0:\n",
    "            continue\n",
    "\n",
    "        length_m = geom.length\n",
    "        expected = length_m / 1000 * density_per_km  # λ for Poisson\n",
    "        n_points = np.random.poisson(expected)  # 0, 1, 2, …\n",
    "\n",
    "        # Clip to limits\n",
    "        n_points = max(min_points_per_edge, min(n_points, max_points_per_edge))\n",
    "\n",
    "        if n_points == 0:\n",
    "            continue\n",
    "\n",
    "        # Evenly distribute interior points (skip endpoints)\n",
    "        distances = np.linspace(0, length_m, n_points + 2)[1:-1]\n",
    "        for d in distances:\n",
    "            samples.append(geom.interpolate(d))\n",
    "\n",
    "    # Build GeoDataFrame of sample points (back to WGS-84 for API use)\n",
    "    points_gdf = (\n",
    "        gpd.GeoDataFrame(geometry=gpd.GeoSeries(samples), crs=utm_crs)\n",
    "        .to_crs(epsg=4326)\n",
    "    )\n",
    "    points_gdf[\"lon\"] = points_gdf.geometry.x.round(6)\n",
    "    points_gdf[\"lat\"] = points_gdf.geometry.y.round(6)\n",
    "    points_gdf = points_gdf.drop_duplicates(subset=[\"lat\", \"lon\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f\"Generated {len(points_gdf)} points \"\n",
    "        f\"with expected density {density_per_km} pts/km.\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"DOWNLOAD_IMAGES is false, so not sampling from the road network\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c714470e04becead",
   "metadata": {},
   "source": [
    "if DOWNLOAD_IMAGES:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot roads first (thin gray lines)\n",
    "    edges_gdf.plot(ax=ax, linewidth=0.4, color=\"gray\")\n",
    "\n",
    "    # Plot the sample points\n",
    "    points_gdf.plot(ax=ax, color=\"blue\", markersize=2, label=\"Sample Points\")\n",
    "\n",
    "    # Plot the study-area outline on top (thicker red line)\n",
    "    boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "    ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "    ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "    ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "417ed56ad756c48c",
   "metadata": {},
   "source": [
    "## Download street view images for each point\n",
    "\n",
    "Note: expects a valid Google Maps API key in the file `google_maps_api_key.txt` in the same directory as this script (not synced to github for obvious reasons)."
   ]
  },
  {
   "cell_type": "code",
   "id": "8b7d49a176b8317e",
   "metadata": {},
   "source": [
    "# Get the API key from a file (of needed)\n",
    "if DOWNLOAD_IMAGES:\n",
    "    with open('google_maps_api_key.txt', 'r') as f:\n",
    "        api_key = f.readline().strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dfff25f11b7ee74",
   "metadata": {},
   "source": [
    "A load of images were downloaded incorrectly (just got a streetview blank jpeg not a proper image). The following code identifies the first image that went wrong (I found that it was image 8116 by looking at the saved picture files) and removes it and all other from the point records. Then I delete the corresponding images. Hopefully this removed the bad images and kept the point cache and images aligned."
   ]
  },
  {
   "cell_type": "code",
   "id": "7082704bebf574a1",
   "metadata": {},
   "source": [
    "#with open(points_data_cache, \"rb\") as f:\n",
    "#    point_records = pickle.load(f)\n",
    "#\n",
    "#point_records_bak = point_records.copy()\n",
    "#index = next(i for i, d in enumerate(point_records) if d['point_id'] == 19196)\n",
    "#print(index)\n",
    "#point_records[index]  # This is the index of the record identified above\n",
    "#\n",
    "#point_records = point_records[:index]\n",
    "#\n",
    "#point_records[18895]\n",
    "#\n",
    "#with open(points_data_cache, \"wb\") as f:\n",
    "#    pickle.dump(point_records, f)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14bf73b5538e29e0",
   "metadata": {},
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir / \"points_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if points_data_cache.exists():\n",
    "    print(\"Loading cached point data …\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Iterate through newly‑sampled street‑network points (with a progress bar)\n",
    "# -----------------------------------------------------------\n",
    "if DOWNLOAD_IMAGES:\n",
    "    for _, row in tqdm(points_gdf.iterrows(), total=len(points_gdf), desc=\"Downloading images\"):\n",
    "        lat = row[\"lat\"]\n",
    "        lon = row[\"lon\"]\n",
    "\n",
    "        # Skip if imagery for this coordinate (rounded earlier) already exists\n",
    "        if (lat, lon) in existing_coords:\n",
    "            continue\n",
    "\n",
    "        point_id = next_id\n",
    "        next_id += 1\n",
    "        added_this_run += 1\n",
    "\n",
    "        # ---- Street‑View metadata ----\n",
    "        meta_params = {\"location\": f\"{lat},{lon}\", \"key\": api_key}\n",
    "        meta_url = \"https://maps.googleapis.com/maps/api/streetview/metadata\"\n",
    "        try:\n",
    "            meta = requests.get(meta_url, params=meta_params).json()\n",
    "        except Exception as e:\n",
    "            print(f\"[Point {point_id}] Metadata request failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        if meta.get(\"status\") != \"OK\":\n",
    "            if DEBUG:\n",
    "                print(f\"[Point {point_id}] Street View not available (status={meta.get('status')}).\")\n",
    "            continue\n",
    "\n",
    "        pano_id = meta.get(\"pano_id\")\n",
    "        date = meta.get(\"date\")  # e.g. \"2024‑08\"\n",
    "\n",
    "        # ---- Download images for the specified headings ----\n",
    "        point_image_files = []\n",
    "        for heading in np.linspace(0, 360, num=n_directions, endpoint=False):\n",
    "            fname = f\"point{point_id}_heading{int(heading)}.jpg\"\n",
    "            image_path = image_dir / fname\n",
    "            point_image_files.append(str(image_path))\n",
    "\n",
    "            if image_path.exists():\n",
    "                continue  # already on disk\n",
    "\n",
    "            img_params = {\n",
    "                \"size\": image_size,\n",
    "                \"pano\": pano_id,\n",
    "                \"heading\": str(int(heading)),\n",
    "                \"pitch\": \"0\",\n",
    "                \"key\": api_key,\n",
    "            }\n",
    "            img_url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
    "            try:\n",
    "                img_resp = requests.get(img_url, params=img_params)\n",
    "                with open(image_path, \"wb\") as f:\n",
    "                    f.write(img_resp.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download image for point {point_id}, heading {heading}: {e}\")\n",
    "\n",
    "        # ---- Append the new record ----\n",
    "        point_records.append(\n",
    "            {\n",
    "                \"point_id\": point_id,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"date\": date,\n",
    "                \"image_files\": point_image_files,\n",
    "                \"embedding\": None,  # to be filled later\n",
    "            }\n",
    "        )\n",
    "        existing_coords.add((lat, lon))\n",
    "\n",
    "    print(f\"Added {added_this_run} new points this run (total now {len(point_records)}).\")\n",
    "\n",
    "    # Persist the (possibly) updated cache immediately\n",
    "    with open(points_data_cache, \"wb\") as f:\n",
    "        pickle.dump(point_records, f)\n",
    "\n",
    "else:\n",
    "    print(\"DOWNLOAD_IMAGES set to false, so not downloading any images\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47ba1f32-9f79-4985-a01f-6de3b3a0a942",
   "metadata": {},
   "source": "Map of the full sample (cache + any others just downlaoded)"
  },
  {
   "cell_type": "code",
   "id": "b952715d-e889-404b-96be-72e783920876",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Plot roads first (thin gray lines)\n",
    "#edges_gdf.to_crs(epsg=3857).plot(ax=ax, linewidth=0.2, color=\"grey\")\n",
    "\n",
    "# Plot the sample points (note the on-the-fly projection to Web Mercator for Contextily later)\n",
    "gpd.GeoDataFrame(point_records,\n",
    "                 geometry=[Point(rec[\"longitude\"], rec[\"latitude\"]) for rec in point_records],\n",
    "                 crs=\"EPSG:4326\"\n",
    "                 ).to_crs(epsg=3857).plot(ax=ax, color=\"blue\", markersize=3, label=\"Sample Points\")\n",
    "\n",
    "# Plot the study-area outline on top (thicker red line)\n",
    "boundary_gdf.to_crs(epsg=3857).boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "# Basemap\n",
    "cx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "ax.set_title(\"Greater Manchester road network and street view sample locations\", pad=12)\n",
    "ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "\n",
    "# Save the figure as PNG\n",
    "plt.savefig(\"img/gm_street_images.png\", dpi=300, bbox_inches=\"tight\")  # High resolution and tight layout\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0dbabc74-c89f-4ae2-a500-36165868c612",
   "metadata": {},
   "source": [
    "Show some randomly chosen images"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3cd3ed63eec4c4f",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------\n",
    "n_points_to_show = 6          # rows\n",
    "headings_per_pt  = n_directions   # columns; assumption: same for all points\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SAMPLE POINTS\n",
    "# -------------------------------------------------------\n",
    "# point_records was created earlier when you downloaded images\n",
    "valid_records = [rec for rec in point_records if rec.get(\"image_files\")]\n",
    "if len(valid_records) < n_points_to_show:\n",
    "    raise ValueError(f\"Need at least {n_points_to_show} points with images; \"\n",
    "                     f\"found {len(valid_records)}\")\n",
    "\n",
    "sample_pts = random.sample(valid_records, n_points_to_show)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# PLOT\n",
    "# -------------------------------------------------------\n",
    "fig, axes = plt.subplots(n_points_to_show,\n",
    "                         headings_per_pt,\n",
    "                         figsize=(headings_per_pt * 3.5, n_points_to_show * 3))\n",
    "\n",
    "for row, rec in enumerate(sample_pts):\n",
    "    imgs = rec[\"image_files\"]\n",
    "    # If fewer than expected headings (e.g. download failure), pad with blanks\n",
    "    while len(imgs) < headings_per_pt:\n",
    "        imgs.append(None)\n",
    "\n",
    "    for col, img_path in enumerate(imgs[:headings_per_pt]):\n",
    "        ax = axes[row, col] if n_points_to_show > 1 else axes[col]\n",
    "\n",
    "        if img_path and os.path.exists(img_path):\n",
    "            ax.imshow(Image.open(img_path))\n",
    "        else:\n",
    "            # blank panel if the image is missing\n",
    "            ax.text(0.5, 0.5, \"no image\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # column headers once at top\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"heading {col*360/headings_per_pt:.0f}°\", fontsize=10, pad=6)\n",
    "\n",
    "    # label the leftmost image with point info\n",
    "    axes[row, 0].set_ylabel(\n",
    "        f\"point {rec['point_id']}\\n({rec['latitude']:.3f}, {rec['longitude']:.3f})\",\n",
    "        fontsize=8, rotation=0, ha=\"right\", va=\"center\"\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Street-View snapshots: 6 random points × 4 headings\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "id": "29c7c3272d3d1d0b",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------\n",
    "# Load ViT-Base (ImageNet-21k) and pick CUDA ▸ MPS ▸ CPU device\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "print(f\"Loading {model_name} …\")\n",
    "processor   = AutoImageProcessor.from_pretrained(model_name)\n",
    "model       = AutoModel.from_pretrained(model_name).eval()   # no classifier head\n",
    "\n",
    "# ----- smart device selection -----\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✔ Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")      # Apple-Silicon Metal\n",
    "    print(\"✔ Using Apple MPS GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"✔ Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Embed all images for each point\n",
    "# -------------------------------------------------------------\n",
    "already_have_embedding = 0\n",
    "for rec in point_records:\n",
    "    embeds = []\n",
    "    # Skip records that already have an embedding\n",
    "    if rec.get(\"embedding\") is not None:\n",
    "        already_have_embedding += 1\n",
    "        continue\n",
    "    embeds = []\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        # Load & convert to RGB (some JPEGs are encoded as P)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Tokenize and normalize the image\n",
    "        inputs = processor(img, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}   # move tensors\n",
    "\n",
    "        # Forward pass through the model (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "\n",
    "        # Extract the CLS token embedding (first token in the sequence)\n",
    "        cls = out.last_hidden_state[0, 0, :].cpu().numpy()     # CLS token\n",
    "        embeds.append(cls)\n",
    "\n",
    "    # Store the mean embedding for this point (or none)\n",
    "    rec[\"embedding\"] = None if not embeds else np.mean(embeds, axis=0)\n",
    "\n",
    "print(f\"Created {len(point_records)-already_have_embedding} new embeddings. \"\n",
    "      f\"{already_have_embedding} points had existing embeddings.\")\n",
    "\n",
    "# cache the enriched records\n",
    "with open(points_data_cache, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"✓ {len(point_records)} image embeddings computed and cached.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53cc4eea4d87b48c",
   "metadata": {},
   "source": [
    "## Analyse the embeddings\n",
    "\n",
    "No idea what these show really, so do some anaysis to see how they vary over space etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f4253247e9533",
   "metadata": {},
   "source": [
    "### PCA embeddings map\n",
    "\n",
    "Do a PCA to get 3 dimensions for each embedding, the scale to RGB and map them. Click on a point to see the images."
   ]
  },
  {
   "cell_type": "code",
   "id": "e2830031dc9e007b",
   "metadata": {},
   "source": [
    "# Visualise embeddings in “RGB PCA space”\n",
    "# Each point’s 768-D embedding → 3-D PCA → scaled 0-1 → RGB colour\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Collect embeddings and GeoDataFrame of points\n",
    "# -------------------------------------------------------\n",
    "records = [rec for rec in point_records if rec[\"embedding\"] is not None]\n",
    "X = np.vstack([rec[\"embedding\"] for rec in records])  # (N, 768)\n",
    "coords = np.array([[rec[\"longitude\"], rec[\"latitude\"]] for rec in records])\n",
    "\n",
    "# optional: bring in points_gdf if you prefer plotting via GeoPandas\n",
    "# points_gdf = gpd.GeoDataFrame({'geometry': gpd.points_from_xy(coords[:,0], coords[:,1])})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. PCA → first 3 components\n",
    "# -------------------------------------------------------\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "rgb3 = pca.fit_transform(X)  # (N, 3)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Scale each PC separately to [0, 1]\n",
    "# -------------------------------------------------------\n",
    "rgb_min = rgb3.min(axis=0)  # per-column min\n",
    "rgb_max = rgb3.max(axis=0)\n",
    "rgb_scaled = (rgb3 - rgb_min) / (rgb_max - rgb_min + 1e-9)  # avoid /0\n",
    "colors = [tuple(c) for c in rgb_scaled]  # list of (r,g,b) floats 0-1\n",
    "hex_colors = [to_hex(tuple(c)) for c in rgb_scaled]             # '#rrggbb'\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Map to plot on lon/lat, coloured by PCA-RGB\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---- 4) Build GeoDataFrame (WGS84) ----\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    data = { \"pc1\": rgb_scaled[:, 0], \"pc2\": rgb_scaled[:, 1], \"pc3\": rgb_scaled[:, 2], \"hex\": hex_colors, },\n",
    "    geometry=gpd.points_from_xy(coords[:, 0], coords[:, 1]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# ---- 5) Base map via gpd.explore, then add per-point colours with Folium ----\n",
    "# (explore() returns a Folium map; we start with an empty layer for nice tiles/controls)\n",
    "m = gdf.iloc[:0].explore(tiles=\"CartoDB positron\", control_scale=True)\n",
    "\n",
    "# add one CircleMarker per point using the precomputed PCA colour\n",
    "for _, r in gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(r.geometry.y, r.geometry.x),\n",
    "        radius=4,\n",
    "        color=r[\"hex\"],\n",
    "        fill=True,\n",
    "        fill_color=r[\"hex\"],\n",
    "        fill_opacity=1.0,\n",
    "        weight=0,\n",
    "        tooltip=f\"PCs: ({r.pc1:.2f}, {r.pc2:.2f}, {r.pc3:.2f})\",\n",
    "    ).add_to(m)\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "m.fit_bounds([[miny, minx], [maxy, maxx]])\n",
    "\n",
    "m  # display in notebook / JupyterLab\n",
    "\n",
    "# Alternative: matplotlib static plot\n",
    "#fig, ax = plt.subplots(figsize=(7, 7))\n",
    "#ax.scatter(coords[:, 0], coords[:, 1], c=colors, s=8, alpha=0.9, linewidths=0)\n",
    "#\n",
    "#ax.set_aspect(\"equal\")\n",
    "#ax.set_title(\"Point embeddings visualised as RGB (PCA first 3 comps)\")\n",
    "#ax.axis(\"off\")  # hide ticks – remove if you want lon/lat grid\n",
    "#\n",
    "#plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a89094a0f1c88625",
   "metadata": {},
   "source": [
    "Map a small sample and display the images too"
   ]
  },
  {
   "cell_type": "code",
   "id": "5aea9419cd76c010",
   "metadata": {},
   "source": [
    "#MAP_SAMPLE_SIZE = 500  # Takes a while and usually crashes jupyter\n",
    "MAP_SAMPLE_SIZE = 200  # Easy to plot\n",
    "\n",
    "def img_tag(path, width=150):\n",
    "    \"\"\"\n",
    "    Return a <img> tag with the image inlined as base64.\n",
    "    Width in pixels; height auto.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return \"<div style='width:{}px;height:{}px;background:#ccc;'>no img</div>\".format(width, int(width*0.75))\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"<img src='data:image/jpeg;base64,{b64}' width='{width}px' style='margin:2px;'/>\"\n",
    "\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[coords[:, 1].mean(), coords[:, 0].mean()], # centred on the mean lat/lon\n",
    "    zoom_start=12,\n",
    "    tiles=\"cartodbpositron\"   # light background; try 'openstreetmap'\n",
    ")\n",
    "\n",
    "# ---------- Add points ----------\n",
    "#for rec, hx in zip(records, hex_colors):\n",
    "# Sample ome points, map breaks with too many\n",
    "for rec, hx in random.sample(list(zip(records, hex_colors)), MAP_SAMPLE_SIZE):\n",
    "    # Build HTML table of up to 4 images (N,E,S,W)\n",
    "    imgs_html = \"\".join(img_tag(p, width=140) for p in rec[\"image_files\"][:4])\n",
    "    popup_html = f\"\"\"\n",
    "    <div style=\"text-align:center\">\n",
    "        <b>point {rec['point_id']}</b><br/>\n",
    "        {imgs_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[rec[\"latitude\"], rec[\"longitude\"]],\n",
    "        radius=6,\n",
    "        color=hx, fill=True, fill_color=hx, fill_opacity=0.9, weight=0,\n",
    "        popup=folium.Popup(popup_html, max_width=600)\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "#m.save(\"embedding_rgb_with_photos.html\")\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "882337e4a549c6b0",
   "metadata": {},
   "source": [
    "Look at how similar an embedding is to it's K nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67dad-21bf-4f3f-8791-799858699930",
   "metadata": {},
   "source": [
    "### Embedding similarity maps\n",
    "\n",
    "Look at how similar the point embeddings are to each other"
   ]
  },
  {
   "cell_type": "code",
   "id": "56265d0d8a055ec3",
   "metadata": {},
   "source": [
    "# Find the nearest neighbours for each point\n",
    "nbrs = NearestNeighbors(n_neighbors=6).fit(coords)\n",
    "_, idxs = nbrs.kneighbors(coords)\n",
    "\n",
    "# Calculate the mean cosine similarity to the nearest neighbours (excluding self)\n",
    "gdf['local_sim'] = [np.mean(cosine_similarity([X[i]], X[idxs[i,1:]])[0]) for i in range(len(X))]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbb5f2283928f2c9",
   "metadata": {},
   "source": [
    "# Map the point similarity (not especially helpful)\n",
    "gdf.explore(column='local_sim', cmap='viridis', fit_bounds=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20f4dba929727a0b",
   "metadata": {},
   "source": [
    "# Aggreate similarity to a regular grid; more useful\n",
    "\n",
    "# project to a metric CRS first (so grid cell sizes are meaningful)\n",
    "gdf = gdf.to_crs(3857)  # https://epsg.io/3857\n",
    "\n",
    "# --- 1. Make grid ---\n",
    "cell_size = 2000    # metres\n",
    "xmin, ymin, xmax, ymax = gdf.total_bounds  # (need to do again with new crs)\n",
    "\n",
    "cols = np.arange(xmin, xmax + cell_size, cell_size)\n",
    "rows = np.arange(ymin, ymax + cell_size, cell_size)\n",
    "\n",
    "polys = [\n",
    "    shapely.geometry.box(x, y, x + cell_size, y + cell_size)\n",
    "    for x in cols for y in rows\n",
    "]\n",
    "grid = gpd.GeoDataFrame({'geometry': polys}, crs=gdf.crs)\n",
    "\n",
    "# --- 2. Spatial join + aggregate ---\n",
    "joined = gpd.sjoin(gdf, grid, predicate=\"within\")\n",
    "\n",
    "#agg = joined.groupby('index_right')['local_sim'].mean().reset_index()\n",
    "agg = (joined.groupby('index_right', observed=True)['local_sim'].mean())\n",
    "\n",
    "#grid['mean_sim'] = agg['local_sim']\n",
    "grid['mean_sim'] = np.nan\n",
    "grid.loc[agg.index, 'mean_sim'] = agg.values\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e21eb7bb-f888-4342-afad-51ca487f325e",
   "metadata": {},
   "source": [
    "ax = grid.plot(column='mean_sim', cmap='viridis', legend=True)\n",
    "#gdf.plot(ax=ax)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "476b6b4b-7798-4b46-82cc-2589694b5b02",
   "metadata": {},
   "source": [
    "# --- 3. Plot interactively --- (turned off for now)\n",
    "\n",
    "# The grid\n",
    "#m = grid.explore(column='mean_sim', cmap='viridis', legend=True, fit_bounds=True)\n",
    "\n",
    "# Add the points\n",
    "#gdf.explore(m=m, column='local_sim', cmap='viridis', marker_kwds=dict(radius=2, opacity=0.4, fill=True))\n",
    "\n",
    "#m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "890bfab7-d504-4b12-b515-4d8d2eccf305",
   "metadata": {},
   "source": [
    "Not particularly conclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f55715-b0bb-4f88-9709-3c4de0b0a779",
   "metadata": {},
   "source": [
    "### Embedding similarity over distance (_not sure what this does!_).\n",
    "\n",
    "_I'd like to see how the embeddings change with distance. My hypothesis is that for any given point, it will be quite similar to its neighbours, but will also be very similar to some distant points as well. I thought about using Moran's I, but it is univariate, so can't be calculated on dense embeddings. ChatGPT suggested a Mantel Correlogram which compares Euclidean with cosine similarity over distance bands, but I'm not sure what that does either really. I does make a graph though! I'm leaving the code in below but am going to move on to look at clustering embeddings._"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aa09ea2-7751-4f4c-9f3f-0cf3fff9f08e",
   "metadata": {},
   "source": [
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Mantel correlogram: correlation of geographic vs. embedding\n",
    "# distances within geographic distance bins.\n",
    "#\n",
    "# Inputs:\n",
    "#   X      : (N, D) embeddings (float)\n",
    "#   coords : (N, 2) projected coords in metres (same CRS as gdf.to_crs(...))\n",
    "#   edges  : 1D array of bin edges in metres (monotonic increasing)\n",
    "#   metric : embedding distance metric (\"cosine\" recommended)\n",
    "# Returns:\n",
    "#   DataFrame with one row per bin (band centre, rho, n_pairs)\n",
    "# ------------------------------------------------------------\n",
    "def mantel_correlogram(X, coords, edges, metric=\"cosine\", min_pairs=50):\n",
    "    # 1) Pairwise embedding distances (condensed form, size N*(N-1)/2)\n",
    "    #    We prefer cosine distance for embeddings.\n",
    "    D_emb = pdist(X, metric=metric)\n",
    "\n",
    "    # 2) Pairwise geographic distances (Euclidean in metres)\n",
    "    D_geo = pdist(coords, metric=\"euclidean\")\n",
    "\n",
    "    # 3) Turn into square form to facilitate boolean masks by bin\n",
    "    D_emb_sq = squareform(D_emb)\n",
    "    D_geo_sq = squareform(D_geo)\n",
    "\n",
    "    bands = []\n",
    "    rhos = []\n",
    "    nps = []\n",
    "\n",
    "    # Iterate over consecutive bins: [edges[i], edges[i+1])\n",
    "    for lo, hi in zip(edges[:-1], edges[1:]):\n",
    "        # Mask pairs whose geographic distance falls in this bin\n",
    "        mask = (D_geo_sq >= lo) & (D_geo_sq < hi)\n",
    "\n",
    "        # Only look at upper triangle to avoid double-counting (i<j)\n",
    "        iu = np.triu_indices_from(D_geo_sq, k=1)\n",
    "        mask_u = mask[iu]\n",
    "\n",
    "        # Skip bins that are too sparse for a stable correlation\n",
    "        if np.count_nonzero(mask_u) < min_pairs:\n",
    "            continue\n",
    "\n",
    "        # Extract paired distances for correlation\n",
    "        geo_vals = D_geo_sq[iu][mask_u]\n",
    "        emb_vals = D_emb_sq[iu][mask_u]\n",
    "\n",
    "        # Spearman rank correlation (robust choice)\n",
    "        rho, _ = spearmanr(geo_vals, emb_vals)\n",
    "\n",
    "        bands.append(0.5 * (lo + hi))   # bin centre for plotting\n",
    "        rhos.append(rho)\n",
    "        nps.append(mask_u.sum())\n",
    "\n",
    "    return pd.DataFrame({\"band_m\": bands, \"rho\": rhos, \"n_pairs\": nps})\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Example usage\n",
    "# ---------------------\n",
    "# Ensure your gdf is in a metric CRS (done earlier):\n",
    "# gdf = gdf.to_crs(3857)\n",
    "\n",
    "# Build inputs\n",
    "X = np.vstack([rec[\"embedding\"] for rec in records])              # (N, D)\n",
    "coords = np.c_[gdf.geometry.x.values, gdf.geometry.y.values]      # (N, 2)\n",
    "\n",
    "# Optional: keep only rows with finite embeddings if needed\n",
    "# (usually not necessary if X is clean)\n",
    "# good = np.isfinite(X).all(axis=1)\n",
    "# X, coords = X[good], coords[good]\n",
    "\n",
    "# Define geographic distance bins (metres)\n",
    "#edges = np.arange(0, 2200, 200)  # 0–200, 200–400, …, 2000–2200\n",
    "edges = np.arange(0, 10500, 500) \n",
    "\n",
    "corr_df = mantel_correlogram(X, coords, edges, metric=\"cosine\", min_pairs=50)\n",
    "\n",
    "# ---------------------\n",
    "# Plot the correlogram\n",
    "# ---------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(corr_df[\"band_m\"], corr_df[\"rho\"], marker=\"o\")\n",
    "ax.set_xlabel(\"Geographic distance bin centre (m)\")\n",
    "ax.set_ylabel(\"Spearman ρ (geo distance vs embedding distance)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (Optional) show number of pairs contributing to each bin\n",
    "for x, y, n in zip(corr_df[\"band_m\"], corr_df[\"rho\"], corr_df[\"n_pairs\"]):\n",
    "    ax.annotate(str(n), (x, y), textcoords=\"offset points\", xytext=(0,6), ha=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ae658-e291-4fbd-afc5-284e4df586b9",
   "metadata": {},
   "source": [
    "_Similarly with the following ChatGPT code, it calculates similarity over distance bands and then plots these distances and the uncertainty (mean and IQR). May be useful later but again I'm not confident in what it does so am not using it at this stage_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d40e258-4877-4f5d-b6d4-7231c41a5740",
   "metadata": {},
   "source": [
    "# Radial similarity profiles for high-D embeddings\n",
    "# ------------------------------------------------\n",
    "# For each point i and distance bin [lo, hi), compute the mean cosine\n",
    "# similarity between X[i] and all neighbours whose geographic distance\n",
    "# to i falls within that annulus. Then summarise across points.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs (assumes metric CRS):\n",
    "# -----------------------------\n",
    "# gdf.geometry: points in metres (e.g. EPSG:3857)\n",
    "# X           : (N, D) embedding matrix for the same N points\n",
    "#               If you have 'records', build X from them as before:\n",
    "#               X = np.vstack([rec[\"embedding\"] for rec in records])\n",
    "\n",
    "coords = np.c_[gdf.geometry.x.values, gdf.geometry.y.values]  # (N, 2)\n",
    "N = coords.shape[0]\n",
    "\n",
    "# (1) Cosine similarity needs normalised vectors\n",
    "X = X.astype(np.float64, copy=False)\n",
    "X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# (2) Choose geographic distance bins (metres)\n",
    "# e.g. finer near the origin; adjust to your study scale\n",
    "edges = np.array([0, 100, 200, 300, 500, 750, 1000, 1500, 2000])\n",
    "centres = 0.5 * (edges[:-1] + edges[1:])\n",
    "B = len(centres)\n",
    "\n",
    "# (3) Build a BallTree for fast radius queries in Euclidean metres\n",
    "tree = BallTree(coords, metric=\"euclidean\", leaf_size=40)\n",
    "\n",
    "# (4) Pre-query neighbours within each *upper* radius once\n",
    "#     This avoids O(B*N) repeated searches from scratch.\n",
    "inds_hi = [tree.query_radius(coords, r=hi, return_distance=False)\n",
    "           for hi in edges[1:]]\n",
    "\n",
    "# (5) Compute per-point, per-band mean cosine similarity\n",
    "profiles = np.full((N, B), np.nan, dtype=np.float64)\n",
    "\n",
    "for b, (lo, hi) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "    # neighbours within hi\n",
    "    neigh_hi = inds_hi[b]\n",
    "\n",
    "    if lo == 0:\n",
    "        # 0–hi annulus: drop self\n",
    "        for i in range(N):\n",
    "            idx = neigh_hi[i]\n",
    "            if idx.size:\n",
    "                # remove self index if present\n",
    "                idx = idx[idx != i]\n",
    "            if idx.size:\n",
    "                # mean cosine similarity to neighbours in this bin\n",
    "                profiles[i, b] = X_norm[i].dot(X_norm[idx].T).mean()\n",
    "    else:\n",
    "        # annulus lo–hi: remove those within lo (use precomputed hi of previous band if available)\n",
    "        # find or compute neighbours within lo\n",
    "        # if lo matches a previous hi, reuse; else query fresh\n",
    "        try:\n",
    "            lo_pos = np.where(edges[1:] == lo)[0][0]\n",
    "            neigh_lo = inds_hi[lo_pos]\n",
    "        except IndexError:\n",
    "            neigh_lo = tree.query_radius(coords, r=lo, return_distance=False)\n",
    "\n",
    "        for i in range(N):\n",
    "            # set difference: neighbours in hi but not in lo\n",
    "            # np.setdiff1d is fine for moderate degrees; for very large degree, consider boolean hashing\n",
    "            idx = np.setdiff1d(neigh_hi[i], neigh_lo[i], assume_unique=False)\n",
    "            if idx.size:\n",
    "                idx = idx[idx != i]  # just in case\n",
    "            if idx.size:\n",
    "                profiles[i, b] = X_norm[i].dot(X_norm[idx].T).mean()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Summarise profiles across space (dataset-level correlogram)\n",
    "# -----------------------------------------------------------\n",
    "summary = pd.DataFrame({\n",
    "    \"band_m\": centres,\n",
    "    \"mean\":   np.nanmean(profiles, axis=0),\n",
    "    \"p25\":    np.nanpercentile(profiles, 25, axis=0),\n",
    "    \"p75\":    np.nanpercentile(profiles, 75, axis=0),\n",
    "    \"n_pts\":  np.sum(np.isfinite(profiles), axis=0)  # #points contributing per bin\n",
    "})\n",
    "\n",
    "# Plot: mean curve with IQR ribbon (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(summary[\"band_m\"], summary[\"mean\"], marker=\"o\", label=\"Mean similarity\")\n",
    "ax.fill_between(summary[\"band_m\"], summary[\"p25\"], summary[\"p75\"], alpha=0.2, label=\"IQR\")\n",
    "ax.set_xlabel(\"Geographic distance bin centre (m)\")\n",
    "ax.set_ylabel(\"Mean cosine similarity\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Optional: per-point \"decorrelation distance\"\n",
    "# (distance where similarity falls below a fraction of baseline)\n",
    "# -----------------------------------------------------------\n",
    "# Use the first non-empty bin as the baseline per point:\n",
    "baseline = profiles[:, 0]  # here 0–100 m; adapt if you use a different first bin\n",
    "ratio = 0.5                 # e.g., 50% of baseline\n",
    "\n",
    "decor = np.full(N, np.nan)\n",
    "for i in range(N):\n",
    "    if np.isfinite(baseline[i]):\n",
    "        threshold = baseline[i] * ratio\n",
    "        below = np.where(profiles[i, :] < threshold)[0]\n",
    "        if below.size:\n",
    "            decor[i] = centres[below[0]]\n",
    "\n",
    "# Attach to gdf for mapping\n",
    "gdf[\"decor_m\"] = decor\n",
    "\n",
    "# Quick interactive map: where do embeddings decorrelate faster/slower?\n",
    "m = gdf.explore(column=\"decor_m\", cmap=\"viridis\", legend=True, fit_bounds=True)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf68f377-5089-45a3-abcf-e32232a0920f",
   "metadata": {},
   "source": [
    "### Clustering Embeddings\n",
    "\n",
    "If we cluster embeddings then there is some interesting analysis that can follow.\n",
    "\n",
    "Start by deciding on the appropriate number of clusters (thanks ChatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "id": "f937c648-27d0-4b88-80fb-e37366b28d32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# X : (N, D) embedding matrix (e.g., from your Street View model)\n",
    "# We'll try a range of candidate cluster counts.\n",
    "# -----------------------------------------------------------\n",
    "K_range = range(2, 16)  # test 2→15 clusters\n",
    "\n",
    "inertia = []   # total within-cluster sum of squares (elbow method)\n",
    "silh = []      # silhouette (higher = better separation)\n",
    "ch = []        # Calinski–Harabasz (higher = better)\n",
    "db = []        # Davies–Bouldin (lower = better)\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    labels = km.labels_\n",
    "\n",
    "    inertia.append(km.inertia_)\n",
    "    silh.append(silhouette_score(X, labels))\n",
    "    ch.append(calinski_harabasz_score(X, labels))\n",
    "    db.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "# Collect results for inspection\n",
    "df_scores = pd.DataFrame({\n",
    "    \"k\": K_range,\n",
    "    \"inertia\": inertia,\n",
    "    \"silhouette\": silh,\n",
    "    \"calinski_harabasz\": ch,\n",
    "    \"davies_bouldin\": db\n",
    "})\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Plot diagnostic curves\n",
    "# -----------------------------------------------------------\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(df_scores[\"k\"], df_scores[\"inertia\"], \"-o\", color=\"tab:gray\", label=\"Inertia (↓)\")\n",
    "ax1.set_xlabel(\"Number of clusters (k)\")\n",
    "ax1.set_ylabel(\"Inertia (total within-cluster variance)\", color=\"tab:gray\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:gray\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_scores[\"k\"], df_scores[\"silhouette\"], \"-o\", color=\"tab:blue\", label=\"Silhouette (↑)\")\n",
    "ax2.plot(df_scores[\"k\"], df_scores[\"calinski_harabasz\"]/np.max(df_scores[\"calinski_harabasz\"]),\n",
    "         \"--o\", color=\"tab:green\", label=\"Calinski–Harabasz (↑, scaled)\")\n",
    "ax2.plot(df_scores[\"k\"], 1/np.array(df_scores[\"davies_bouldin\"]),\n",
    "         \"-.o\", color=\"tab:red\", label=\"1 / Davies–Bouldin (↑)\")\n",
    "ax2.set_ylabel(\"Relative cluster quality (higher = better)\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# combine legends\n",
    "lines, labels = [], []\n",
    "for a in (ax1, ax2):\n",
    "    L, lab = a.get_legend_handles_labels()\n",
    "    lines += L; labels += lab\n",
    "ax1.legend(lines, labels, loc=\"best\")\n",
    "\n",
    "plt.title(\"Cluster-number diagnostics for embedding space\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Interpret\n",
    "# -----------------------------------------------------------\n",
    "# - \"Elbow\" point on the inertia curve = plausible k (diminishing returns).\n",
    "# - Silhouette / CH peak and DB minimum should roughly coincide.\n",
    "# Choose k where those metrics stabilise or peak.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a82ffe91-7a05-4bf8-9bfc-43aa117f3c5c",
   "metadata": {},
   "source": [
    "It isn't conclusive, so go with _N=8_ for now"
   ]
  },
  {
   "cell_type": "code",
   "id": "a48aad5f-ffa9-4fe7-9947-a22d4b51ee11",
   "metadata": {},
   "source": [
    "n_clusters = 8"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e46db383-d8ae-41d9-a1d1-e32fafd201d3",
   "metadata": {},
   "source": [
    "Calculate the clusters"
   ]
  },
  {
   "cell_type": "code",
   "id": "7639c431-4c87-405e-a589-f93f760b94fc",
   "metadata": {},
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters).fit(X)\n",
    "gdf['cluster'] = kmeans.labels_.astype(str)\n",
    "\n",
    "gdf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60f61908-8141-4d9a-bc63-e36eafab3288",
   "metadata": {},
   "source": [
    "Map the clusters"
   ]
  },
  {
   "cell_type": "code",
   "id": "52d6b8c1-3c13-4826-9a95-3947a480fb7e",
   "metadata": {},
   "source": [
    "m = gdf.explore(\n",
    "    column=\"cluster\",\n",
    "    categorical=True,\n",
    "    categories=sorted(gdf[\"cluster\"].unique()),\n",
    "    cmap=\"tab20\",                 # good categorical palette\n",
    "    legend=True,\n",
    "    legend_kwds={\"caption\": \"Embedding clusters\"},\n",
    "    marker_kwds={\"radius\": 3, \"fillOpacity\": 1.0},\n",
    "    style_kwds={\"weight\": 0},\n",
    "    fit_bounds=True,\n",
    "    tiles = \"CartoDB positron\"\n",
    ")\n",
    "m "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48a0c495-aded-451a-a78d-07dfc93f79ce",
   "metadata": {},
   "source": [
    "Estimate degree to which classes are clustered, controlling for the arbitrary spatial arrangement of the points. Conclusion: looks like there is clustering **but need to check the code before doing anything with this**."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d3b930d-65e7-41b8-ab83-4453653e5fcd",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Join-count test on a k-NN graph (random-labelling null)\n",
    "#    - Builds an undirected k-NN graph from coordinates\n",
    "#    - Counts same-class edges overall and per class\n",
    "#    - Uses permutations to get p-values\n",
    "# ------------------------------------------------------------\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Inputs:\n",
    "#   gdf[\"cluster\"] : integer labels from your embedding clustering\n",
    "#   gdf.geometry   : points (use projected CRS; metres not required here but OK)\n",
    "labels = gdf[\"cluster\"].to_numpy()\n",
    "coords = np.c_[gdf.geometry.x, gdf.geometry.y]\n",
    "N = len(gdf)\n",
    "\n",
    "k = n_clusters        # neighbours per node (tune)\n",
    "B = 999               # permutations for p-values (e.g., 999)\n",
    "\n",
    "# --- build undirected k-NN edge list (as pairs of indices) ---\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)  # +1 includes self\n",
    "_, idxs = nbrs.kneighbors(coords)\n",
    "edges = set()\n",
    "for i in range(N):\n",
    "    for j in idxs[i, 1:]:    # skip self at position 0\n",
    "        a, b = (i, j) if i < j else (j, i)\n",
    "        edges.add((a, b))    # undirected: store once\n",
    "edges = np.array(list(edges), dtype=int)\n",
    "E = len(edges)\n",
    "\n",
    "# --- observed join counts ---\n",
    "ell = labels\n",
    "same = (ell[edges[:,0]] == ell[edges[:,1]])\n",
    "JC_all_obs = int(np.sum(same))  # total same-class joins\n",
    "\n",
    "# per-class same-class joins\n",
    "classes = np.unique(ell)\n",
    "JC_cls_obs = {c: int(np.sum((ell[edges[:,0]] == c) & (ell[edges[:,1]] == c)))\n",
    "              for c in classes}\n",
    "\n",
    "# --- permutation test (random labelling) ---\n",
    "rng = np.random.default_rng(0)\n",
    "JC_all_null = np.zeros(B, dtype=int)\n",
    "JC_cls_null = {c: np.zeros(B, dtype=int) for c in classes}\n",
    "\n",
    "for b in range(B):\n",
    "    perm = rng.permutation(N)\n",
    "    lp = ell[perm]\n",
    "    same_p = (lp[edges[:,0]] == lp[edges[:,1]])\n",
    "    JC_all_null[b] = int(np.sum(same_p))\n",
    "    for c in classes:\n",
    "        JC_cls_null[c][b] = int(np.sum((lp[edges[:,0]] == c) & (lp[edges[:,1]] == c)))\n",
    "\n",
    "# --- p-values (one-sided: clustered = more same-class joins than null) ---\n",
    "def perm_pval(obs, null):\n",
    "    return (np.sum(null >= obs) + 1) / (len(null) + 1)\n",
    "\n",
    "p_all = perm_pval(JC_all_obs, JC_all_null)\n",
    "p_cls = {c: perm_pval(JC_cls_obs[c], JC_cls_null[c]) for c in classes}\n",
    "\n",
    "# --- tidy summary ---\n",
    "out_rows = [{\"stat\": \"ALL\", \"obs_same_joins\": JC_all_obs,\n",
    "             \"exp_mean\": float(JC_all_null.mean()),\n",
    "             \"p_value\": float(p_all)}]\n",
    "for c in classes:\n",
    "    out_rows.append({\"stat\": f\"class_{int(c)}\",\n",
    "                     \"obs_same_joins\": JC_cls_obs[c],\n",
    "                     \"exp_mean\": float(JC_cls_null[c].mean()),\n",
    "                     \"p_value\": float(p_cls[c])})\n",
    "joincount_results = pd.DataFrame(out_rows)\n",
    "print(joincount_results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efd7f11e-af38-46e2-9f95-e4dbbfb944a6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bed83faf-00bf-43b4-a4ae-a662f00c6736",
   "metadata": {},
   "source": [
    "WHAT NOW?!?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "303fc6c0-2686-4f05-8b86-2dc6767b2b92",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d912603d-e9ed-4de1-943e-cdb23689d3e7",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e82d319c6d584dfe",
   "metadata": {},
   "source": [
    "## Prepare training data for the gentrification model\n",
    "\n",
    "Note that we don't actually calculate the model later unless `RUN_GENTRIFICATION_MODEL` is true, mostly because it takes bloody ages. But some of the operations, like joining LSOAs to embeddings, are needed for the IMD model so we do run some of this. "
   ]
  },
  {
   "cell_type": "code",
   "id": "13af61fecf0f1878",
   "metadata": {},
   "source": [
    "# Load GMGI\n",
    "gmgi = pd.read_csv(gentrification_file)\n",
    "gmgi = gmgi.iloc[:,1:]  # Drop the first column\n",
    "gmgi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11eb47fc52933ea2",
   "metadata": {},
   "source": [
    "Attach GMGI to LSOAs that we read earlier"
   ]
  },
  {
   "cell_type": "code",
   "id": "a38feff5e907726f",
   "metadata": {},
   "source": [
    "# Attach GMGI columns to lsoa data\n",
    "gm_gmgi_lsoa = pd.merge(left=gm_lsoa, right=gmgi, left_on=\"LSOA11CD\", right_on=\"LSOA11CD\")\n",
    "# Map the gentrification index (sanity check)\n",
    "gm_gmgi_lsoa.plot(column=\"gi_n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8060cad100582810",
   "metadata": {},
   "source": [
    "Join image embeddings points to gentrification LSOAs (so we have the 'ground truth' gentrification score)."
   ]
  },
  {
   "cell_type": "code",
   "id": "557a20b7d1e3b3e2",
   "metadata": {},
   "source": [
    "# Check all records have an embeding\n",
    "invalid_records = [rec for rec in point_records if rec.get('embedding') is None]\n",
    "assert len(invalid_records)==0, f\"Found {len(invalid_records)} invalid points\"\n",
    "\n",
    "# Now join the embeddings points to the LSOAs\n",
    "point_coords = [Point(rec['longitude'], rec['latitude']) for rec in point_records]\n",
    "points_labels_gdf = gpd.GeoDataFrame(point_records, geometry=point_coords, crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to get gentrification label for each point\n",
    "points_labels_gdf = gpd.sjoin(points_labels_gdf, gm_gmgi_lsoa, how='inner', predicate='within')\n",
    "# sjoin may add an index from the polygon ('index_right'); we can drop it\n",
    "if 'index_right' in points_labels_gdf.columns:\n",
    "    points_labels_gdf = points_labels_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(f\"Points after spatial join: {len(points_labels_gdf)} / {len(point_records)}\"\n",
    "      f\" `(some points may lie outside the label polygons and were dropped)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78f9aa7d-710b-4a3d-a4af-d3e59fa6736a",
   "metadata": {},
   "source": [
    "Show them on a map"
   ]
  },
  {
   "cell_type": "code",
   "id": "bff9073d-0bb2-4526-a47b-eeec5751d9ba",
   "metadata": {},
   "source": [
    "label_col = 'gi_n'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e528cb9af1e778a",
   "metadata": {},
   "source": [
    "ax = gm_gmgi_lsoa.plot(column=label_col)\n",
    "# Add the points\n",
    "points_labels_gdf.plot(ax=ax, column=label_col, markersize=10, edgecolor=\"black\", linewidth=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20328006b4cf7238",
   "metadata": {},
   "source": [
    "Prepare X and y data for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b49dab5f8f5a9043",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Validate point records before building X, y\n",
    "# ------------------------------------------------------------------\n",
    "def _is_bad_embedding(e):\n",
    "    \"\"\"True if embedding is missing, not a numpy array, or empty.\"\"\"\n",
    "    return (e is None) or (not isinstance(e, np.ndarray)) or (e.size == 0)\n",
    "\n",
    "\n",
    "# Boolean masks\n",
    "missing_label  = points_labels_gdf[label_col].isna()\n",
    "bad_embedding  = points_labels_gdf[\"embedding\"].apply(_is_bad_embedding)\n",
    "\n",
    "# Report any problems\n",
    "n_bad_label   = missing_label.sum()\n",
    "n_bad_embed   = bad_embedding.sum()\n",
    "n_bad_total   = (missing_label | bad_embedding).sum()\n",
    "\n",
    "if n_bad_total:\n",
    "    msg = (f\"⚠️  {n_bad_total} invalid point(s) detected \"\n",
    "           f\"({n_bad_label} with missing label, \"\n",
    "           f\"{n_bad_embed} with missing/empty embedding).\")\n",
    "    print(msg)\n",
    "\n",
    "    # show first few offending rows for inspection\n",
    "    print(points_labels_gdf.loc[missing_label | bad_embedding,\n",
    "                                [\"point_id\", label_col, \"embedding\"]].head())\n",
    "\n",
    "else:\n",
    "    print(f\"{n_bad_total} invalid point(s) detected \")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Keep only valid rows\n",
    "# ------------------------------------------------------------------\n",
    "points_labels_gdf = points_labels_gdf.loc[~(missing_label | bad_embedding)].reset_index(drop=True)\n",
    "\n",
    "if points_labels_gdf.empty:\n",
    "    raise ValueError(\"No valid points left after cleaning — cannot train model.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build feature matrix X and target vector y\n",
    "# ------------------------------------------------------------------\n",
    "X = np.stack(points_labels_gdf[\"embedding\"].values)     # shape (n_points, embed_dim)\n",
    "y = points_labels_gdf[label_col].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Target vector shape:\", y.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc8c1d7b3e311955",
   "metadata": {},
   "source": [
    "## Run the gentrification ML models\n",
    "\n",
    "To predict gentrification from the image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "560f3e742116b980",
   "metadata": {},
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X, y, np.arange(X.shape[0]), test_size=0.2, random_state=42)\n",
    "    print(f\"Training points: {X_train.shape[0]}, Test points: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Define model pipelines and parameter grids for cross-validation\n",
    "    models = []\n",
    "    param_grids = []\n",
    "    \n",
    "    # 1. Linear Regression (with standard scaling)\n",
    "    pipe_linear = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', LinearRegression())\n",
    "    ])\n",
    "    # No hyperparameters to tune for plain LinearRegression (we could consider Ridge/Lasso alphas, but skip for simplicity)\n",
    "    models.append(pipe_linear)\n",
    "    param_grids.append({})  # empty grid means just evaluate the baseline linear model\n",
    "    \n",
    "    # 2. Random Forest Regressor\n",
    "    pipe_rf = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # scaler doesn't affect RF but included for uniformity\n",
    "        ('reg', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_rf)\n",
    "    param_grids.append({\n",
    "        'reg__n_estimators': [100, 200],   # try 100 and 200 trees\n",
    "        'reg__max_depth': [None, 10, 20]  # try unlimited depth and a couple of depth limits\n",
    "    })\n",
    "    \n",
    "    # 3. Neural Network (MLPRegressor)\n",
    "    pipe_mlp = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', MLPRegressor(max_iter=500, random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_mlp)\n",
    "    param_grids.append({\n",
    "        'reg__hidden_layer_sizes': [(100,), (100,50)],  # one hidden layer vs two layers\n",
    "        'reg__alpha': [1e-4, 1e-3]  # L2 regularization strengths\n",
    "        # (Other hyperparameters like learning_rate_init can be added if needed)\n",
    "    })\n",
    "    \n",
    "    # Perform cross-validation for each model to find the best hyperparameters\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_name = None\n",
    "    \n",
    "    print(\"Training models\")\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    ncores = min(multiprocessing.cpu_count()-1, 100)\n",
    "    print(f\"USing {ncores} cores.\")  # Take all cores but one, and not more than 100\n",
    "    for model, param_grid, name in zip(models, param_grids, [\"LinearReg\", \"RandomForest\", \"NeuralNet\"]):\n",
    "        print(f\"\\tTraining: {model}...\")\n",
    "        if param_grid:\n",
    "            # Use GridSearchCV for models with hyperparameters\n",
    "            grid = GridSearchCV(model, param_grid, cv=cv, scoring='r2', n_jobs=ncores)\n",
    "            grid.fit(X_train, y_train)\n",
    "            cv_score = grid.best_score_\n",
    "            model_best = grid.best_estimator_\n",
    "            params_best = grid.best_params_\n",
    "        else:\n",
    "            # For Linear Regression (no params to tune), just do cross_val_score\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "            cv_score = np.mean(scores)\n",
    "            model.fit(X_train, y_train)  # train on full training data\n",
    "            model_best = model\n",
    "            params_best = None\n",
    "        print(f\"{name} CV mean R^2 = {cv_score:.3f} {('(best params: '+str(params_best)+')') if params_best else ''}\")\n",
    "        # Track the best model\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model_best\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"Best model from CV: {best_model_name} with R^2 = {best_score:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"RUN_GENTRIFICATION_MODEL is False so not running the gentrification predictor model\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81e4ba14-612f-436e-a5fb-4178eab6252b",
   "metadata": {},
   "source": [
    "Test on a held out set:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ac8baf0-e935-4f98-a1b8-5135bd657357",
   "metadata": {},
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Ensure best_model is trained on the entire training set (GridSearchCV already did refit; for linear we did manually)\n",
    "    # If best_model was from cross_val_score (Linear), we already called model.fit above.\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    # rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # (not avialable in older skikitlearn)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"\\nTest R^2 score: {r2_test:.3f}\")\n",
    "    print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "    \n",
    "    # Attach predictions to the test points for mapping\n",
    "    test_points = points_labels_gdf.iloc[test_idx].copy()\n",
    "    test_points['predicted'] = y_pred\n",
    "    test_points['error'] = test_points['predicted'] - test_points[label_col]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92c7624f-6b54-4ea2-9d6a-08ee2a7c269e",
   "metadata": {},
   "source": [
    "Visualise gentrification maps and error predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "7145134f-d0f8-4886-b977-89e0b36213c2",
   "metadata": {},
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Plot the actual gentrification by area\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    gm_gmgi_lsoa.plot(column=label_col, ax=ax, legend=True, cmap='plasma', edgecolor='gray')\n",
    "    ax.set_title(\"Actual Gentrification by Area\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the prediction errors at test points\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot polygons outlines for context\n",
    "    gm_gmgi_lsoa.boundary.plot(ax=ax, color='lightgray')\n",
    "    # Plot test points with error coloration\n",
    "    test_points.plot(column='error', ax=ax, legend=True, cmap='coolwarm', markersize=50)\n",
    "    ax.set_title(\"Model Prediction Error at Test Points\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13ec6dad-6b37-4f30-9fcb-0ce91d6a976b",
   "metadata": {},
   "source": [
    "## Prepare data for the deprivation model\n",
    "\n",
    "Repeat the gentrification stuff but this time try to predict deprivation (measured using the IMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab8813-a64b-4038-8b8a-ae955e72a0e2",
   "metadata": {},
   "source": [
    "Get the IMD data and read it in. I am using the file [File_2_-_IoD2019_Domains_of_Deprivation.xlsx](../data/File_2_-_IoD2019_Domains_of_Deprivation.xlsx) from the main [IMD 2019 gov page](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1859847e-f332-4b60-aadf-5bdc4f25b7af",
   "metadata": {},
   "source": [
    "\n",
    "# Read Excel file\n",
    "imd = pd.read_excel(imd_file, sheet_name=\"IoD2019 Domains\", header=0)\n",
    "\n",
    "# Rename columns to simpler versions\n",
    "imd_col_map = {\n",
    "    \"LSOA code (2011)\": \"lsoa_2011_code\",\n",
    "    \"LSOA name (2011)\": \"lsoa_2011_name\",\n",
    "    \"Local Authority District code (2019)\": \"lad_2019_code\",\n",
    "    \"Local Authority District name (2019)\": \"lad_2019_name\",\n",
    "    \"Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)\": \"imd_rank\",\n",
    "    \"Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)\": \"imd_decile\",\n",
    "    \"Income Rank (where 1 is most deprived)\": \"income_rank\",\n",
    "    \"Income Decile (where 1 is most deprived 10% of LSOAs)\": \"income_decile\",\n",
    "    \"Employment Rank (where 1 is most deprived)\": \"employment_rank\",\n",
    "    \"Employment Decile (where 1 is most deprived 10% of LSOAs)\": \"employment_decile\",\n",
    "    \"Education, Skills and Training Rank (where 1 is most deprived)\": \"education_rank\",\n",
    "    \"Education, Skills and Training Decile (where 1 is most deprived 10% of LSOAs)\": \"education_decile\",\n",
    "    \"Health Deprivation and Disability Rank (where 1 is most deprived)\": \"health_rank\",\n",
    "    \"Health Deprivation and Disability Decile (where 1 is most deprived 10% of LSOAs)\": \"health_decile\",\n",
    "    \"Crime Rank (where 1 is most deprived)\": \"crime_rank\",\n",
    "    \"Crime Decile (where 1 is most deprived 10% of LSOAs)\": \"crime_decile\",\n",
    "    \"Barriers to Housing and Services Rank (where 1 is most deprived)\": \"housing_rank\",\n",
    "    \"Barriers to Housing and Services Decile (where 1 is most deprived 10% of LSOAs)\": \"housing_decile\",\n",
    "    \"Living Environment Rank (where 1 is most deprived)\": \"environment_rank\",\n",
    "    \"Living Environment Decile (where 1 is most deprived 10% of LSOAs)\": \"environment_decile\"\n",
    "}\n",
    "imd = imd.rename(columns=imd_col_map)\n",
    "imd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb746f57-c43b-447a-85fb-27ed262dd33f",
   "metadata": {},
   "source": [
    "# Join the a-spatial IMD data to the LSOAs (these already have the gmgi data)\n",
    "gm_gmgi_imd_lsoa = pd.merge(left=gm_gmgi_lsoa, right=imd, left_on=\"LSOA11CD\", right_on=\"lsoa_2011_code\")\n",
    "# Map the gentrification index (sanity check)\n",
    "#gm_gmgi_imd_lsoa.explore()\n",
    "gm_gmgi_imd_lsoa.plot(column=\"imd_rank\", legend=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64918e36-b9c6-4777-a468-c7dcb2dbde41",
   "metadata": {},
   "source": [
    "type(gm_gmgi_imd_lsoa.loc[:,imd_col_map.values()])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "093c0dfb-60e5-48aa-b98a-ce7721509553",
   "metadata": {},
   "source": [
    "Now we have LSOAs with IMD measures. Next join the image embeddings (points, called `points_labels_gdf`) to these LSOAs to get the imd measures. We add to the existing points dataset which was created when we did the gentrification stuff"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dc4ec61-c405-45db-8807-867fde19cf24",
   "metadata": {},
   "source": [
    "# Note we only take the new IMD-related columns from the LSOA geodataframe (hence the .loc) \n",
    "# but also need the geometry otherwise gpd can't do the spatial join\n",
    "points_labels_gdf = gpd.sjoin(\n",
    "    left_df = points_labels_gdf, \n",
    "    right_df = gm_gmgi_imd_lsoa.loc[:, list(imd_col_map.values()) + [\"geometry\"]],\n",
    "    how='inner', predicate='within')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2961ea8-b22c-4a70-a153-3a478a77f661",
   "metadata": {},
   "source": [
    "label_col = 'imd_rank'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "205c9bf8-0570-43d4-a640-54c593f76d41",
   "metadata": {},
   "source": [
    "ax = gm_gmgi_imd_lsoa.plot(column=label_col)\n",
    "# Add the points\n",
    "points_labels_gdf.plot(ax=ax, column=label_col, markersize=10, edgecolor=\"black\", linewidth=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "487f816d-089c-43a5-ab88-70a9c90d0061",
   "metadata": {},
   "source": [
    "## Run the IMD model\n",
    "\n",
    "_Mostly this copies code from the gentrification model. Would be nicer to add it to functions but whatever..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf6533-6886-44b6-9f5f-01c8b80d758b",
   "metadata": {},
   "source": [
    "### Grid search for optimal model and hyper-pareters\n",
    "\n",
    "Start by applying a few ML models and choosing the best one (and the best parameters). Then later we can use this model to explore the relationship between street view embeddings and deprivation"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa9c366f-5080-4d7e-9685-be638660059b",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Build feature matrix X and target vector y\n",
    "# ------------------------------------------------------------------\n",
    "X = np.stack(points_labels_gdf[\"embedding\"].values)     # shape (n_points, embed_dim)\n",
    "y = points_labels_gdf[label_col].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Target vector shape:\", y.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19d46c2d-f6fb-4168-a622-95666479712a",
   "metadata": {},
   "source": [
    "# TEMPORARY SAMPLE FOR TESTING\n",
    "#rng = np.random.default_rng(seed=42)\n",
    "#idx = rng.choice(len(y), size=100, replace=False)\n",
    "#X = X[idx]\n",
    "#y = y[idx]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4ee294d-aa39-4c82-8f2b-e4f273a06ad2",
   "metadata": {},
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X , y, np.arange(X.shape[0]), test_size=0.2, random_state=42)\n",
    "    print(f\"Training points: {X_train.shape[0]}, Test points: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Define model pipelines and parameter grids for cross-validation\n",
    "    models = []\n",
    "    param_grids = []\n",
    "    \n",
    "    # 1. Linear Regression (with standard scaling)\n",
    "    pipe_linear = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', LinearRegression())\n",
    "    ])\n",
    "    # No hyperparameters to tune for plain LinearRegression (we could consider Ridge/Lasso alphas, but skip for simplicity)\n",
    "    models.append(pipe_linear)\n",
    "    param_grids.append({})  # empty grid means just evaluate the baseline linear model\n",
    "    \n",
    "    # 2. Random Forest Regressor\n",
    "    pipe_rf = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # scaler doesn't affect RF but included for uniformity\n",
    "        ('reg', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    models.append(pipe_rf)\n",
    "    param_grids.append({\n",
    "        'reg__n_estimators': [100, 200],   # try 100 and 200 trees\n",
    "        'reg__max_depth': [None, 10, 20]  # try unlimited depth and a couple of depth limits\n",
    "    })\n",
    "    \n",
    "    # 3. Neural Network (MLPRegressor)\n",
    "    pipe_mlp = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', MLPRegressor(max_iter=500, random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_mlp)\n",
    "    param_grids.append({\n",
    "        'reg__hidden_layer_sizes': [(100,), (100,50)],  # one hidden layer vs two layers\n",
    "        'reg__alpha': [1e-4, 1e-3]  # L2 regularization strengths\n",
    "        # (Other hyperparameters like learning_rate_init can be added if needed)\n",
    "    })\n",
    "    \n",
    "    # Perform cross-validation for each model to find the best hyperparameters\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_name = None\n",
    "    best_params = {}\n",
    "    \n",
    "    print(\"Training models\")\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    ncores = min(multiprocessing.cpu_count()-1, 100)\n",
    "    print(f\"USing {ncores} cores.\")  # Take all cores but one, and not more than 100 (don't want to kill the HPC)\n",
    "    for model, param_grid, name in zip(models, param_grids, [\"LinearReg\", \"RandomForest\", \"NeuralNet\"]):\n",
    "        print(f\"\\tTraining: {model}...\")\n",
    "        if param_grid:\n",
    "            # Use GridSearchCV for models with hyperparameters\n",
    "            grid = GridSearchCV(model, param_grid, cv=cv, scoring='r2', n_jobs=ncores)\n",
    "            grid.fit(X_train, y_train)\n",
    "            cv_score = grid.best_score_\n",
    "            model_best = grid.best_estimator_\n",
    "            params_best = grid.best_params_\n",
    "        else:\n",
    "            # For Linear Regression (no params to tune), just do cross_val_score\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "            cv_score = np.mean(scores)\n",
    "            model.fit(X_train, y_train)  # train on full training data\n",
    "            model_best = model\n",
    "            params_best = {}\n",
    "        print(f\"{name} CV mean R^2 = {cv_score:.3f} {('(best params: '+str(params_best)+')') if params_best else ''}\")\n",
    "        # Track the best model\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model_best\n",
    "            best_model_name = name\n",
    "            best_params = params_best.copy()         # <- capture the *winning* params\n",
    "\n",
    "    \n",
    "    print(f\"Best model from CV: {best_model_name} with R^2 = {best_score:.3f} and params {best_params}\")\n",
    "\n",
    "    # Save the meta-data so the model can be trained on the different domains of deprivation\n",
    "    best_model_info = {\n",
    "        \"model_name\": best_model_name,\n",
    "        \"best_score\": float(best_score),\n",
    "        \"best_params\": best_params\n",
    "    }\n",
    "    with open(os.path.join(data_dir, \"5-imd_best_model_info.json\"), \"w\") as f:\n",
    "        json.dump(best_model_info, f, indent=2)\n",
    "    print(f\"Cached model info:\", best_model_info)\n",
    "\n",
    "    ## ---- SAVE the model ----\n",
    "    #bundle = {\n",
    "    #    \"model\": best_model,                 # the fitted Pipeline\n",
    "    #    \"name\": best_model_name,             # e.g., \"RandomForest\"\n",
    "    #    \"cv_score_r2\": float(best_score),    # ensure JSON-safe\n",
    "    #    \"trained_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    #    \"sklearn_version\": sklearn.__version__\n",
    "    #}\n",
    "    #dump(bundle, os.path.join(data_dir, \"5-imd_best_model_bundle.joblib\", compress=3)\n",
    "    #print(\"Saved to best_model_bundle.joblib\")\n",
    "    \n",
    "else:\n",
    "    print(\"RUN_IMD_MODEL is False so not running the gentrification predictor model\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "580366f7-2c9b-4bf8-9cc3-ebc2751b82b7",
   "metadata": {},
   "source": [
    "## ---- LOAD the model ----\n",
    "#if RUN_IMD_MODEL:\n",
    "#    bundle = load(\"best_model_bundle.joblib\")\n",
    "#    print(\"Loading cached model\")\n",
    "#    best_model = bundle[\"model\"]\n",
    "#\n",
    "#    print(\"\\t\", bundle[\"name\"], bundle[\"cv_score_r2\"], bundle[\"trained_at\"], bundle[\"sklearn_version\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c75fb9b-2372-4111-bb52-3a82af3b128d",
   "metadata": {},
   "source": [
    "Run the best model on the test set to get a reliable error estimate"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c4d2797-2a9e-413a-a3ef-79b43595e79f",
   "metadata": {},
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Ensure best_model is trained on the entire training set (GridSearchCV already did refit; for linear we did manually)\n",
    "    # If best_model was from cross_val_score (Linear), we already called model.fit above.\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    # rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # (not avialable in older skikitlearn)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"\\nTest R^2 score: {r2_test:.3f}\")\n",
    "    print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "    \n",
    "    # Attach predictions to the test points for mapping\n",
    "    test_points = points_labels_gdf.iloc[test_idx].copy()\n",
    "    test_points['predicted'] = y_pred\n",
    "    test_points['error'] = test_points['predicted'] - test_points[label_col]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2c5af5d-4093-47d8-b488-e9e86b67090a",
   "metadata": {},
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Plot the actual gentrification by area\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    gm_gmgi_imd_lsoa.plot(column=label_col, ax=ax, legend=True, cmap='plasma', edgecolor='gray')\n",
    "    ax.set_title(\"Actual IMD by Area\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the prediction errors at test points\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot polygons outlines for context\n",
    "    gm_gmgi_imd_lsoa.boundary.plot(ax=ax, color='lightgray')\n",
    "    # Plot test points with error coloration\n",
    "    test_points.plot(column='error', ax=ax, legend=True, cmap='coolwarm', markersize=50)\n",
    "    ax.set_title(\"Model Prediction Error at Test Points\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab258f2f-7aaf-4e0a-825c-4ca853b6faf0",
   "metadata": {},
   "source": [
    "Now we have the best model type and optimal hyper-parameters. Now use that modal type and hyper-parameters to predict the different domains of deprivation. We cached the model and parmaeters so don't need to re-run the GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "id": "72331f0b-8d6f-4c43-a5c0-4cdbf9e94890",
   "metadata": {},
   "source": [
    "assert os.path.exists(os.path.join(data_dir, \"5-imd_best_model_info.json\")), \\\n",
    "    \"IMD grid search params json file doesn't exist, need to run the grid search - set RUN_IMD_MODEL to True and re-run)\"\n",
    "\n",
    "with open(os.path.join(data_dir, \"5-imd_best_model_info.json\")) as f:\n",
    "    info = json.load(f)\n",
    "    print(\"Loaded model info:\", info)\n",
    "\n",
    "# Recreate a fresh model with the same hyperparameters\n",
    "model_map = {\n",
    "    \"LinearReg\": LinearRegression,\n",
    "    \"RandomForest\": RandomForestRegressor,\n",
    "    \"NeuralNet\": MLPRegressor\n",
    "}\n",
    "\n",
    "ModelClass = model_map[info[\"model_name\"]]\n",
    "\n",
    "params = {k.replace(\"reg__\", \"\"): v for k, v in info[\"best_params\"].items()} # Strip any 'reg__' prefixes (used by Pipeline)\n",
    "model = ModelClass(**params)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1b4956d-2e46-4c2d-9d40-bdcecd54f21e",
   "metadata": {},
   "source": [
    "### Running IMD models on different domains of deprivation\n",
    "\n",
    "Nowe we have a model and optimal hyper-parameters, estimate new models that can predict both the overall deprivation as well as the individual domains\n",
    "\n",
    "[ ] TODO Run model using different predictor"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c2d724e-f2e2-4d7e-b8b0-8fd04270355e",
   "metadata": {},
   "source": [
    "# Now re-train on different domains of deprivation\n",
    "#XXXX HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2364be9-b25f-45a6-aa68-f5ccc5baeb7b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3b4c050-9b42-40ab-ab50-b924bbec18e0",
   "metadata": {},
   "source": [
    "### Aggregating the image imbeddings\n",
    "\n",
    "Currently we have large numbers of images (and embeddings) per LSOA. But each LSOA only has a single deprivation value\n",
    "\n",
    "[ ] TODO Try aggregating the image embeddings to LSOA and then predicting the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0c3ee44-e8e8-4499-bab8-92212e3a2392",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08c95d8b-242d-473c-ab12-1f775375516b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5687e2d-3e35-40de-83c1-72689258e484",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cd9f1c0-4092-48d9-bd8c-3029bef0791b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b674544-6d30-4684-a28c-72ddb0fd6f15",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
