{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba0d7a54de93bd1",
   "metadata": {},
   "source": [
    "# Estimating Gentrification using Street View Images and Embeddings\n",
    "\n",
    "This script (initially produced by ChatGPT) does the following (_this was my query_):\n",
    " - Read a spatial boundary file (that I will hard code)\n",
    " - Obtain the road network (from OSM?) for that area\n",
    " - Generate sample points on the road network roughly X meters apart\n",
    " - At each sample point, download the most recent street images for that location (either a single 360 degree view of a few smaller images). Use whichever API service is the most appropriate for obtaining the images. Importantly please record the date that the image was taken.\n",
    " - For each image, calculate an embedding using an appropriate foundation model (one that has been pre-trained to distinguish street environments specifically). Please use Hugging Face libraries.\n",
    " - If necessary, calculate the mean embedding for each point (is this the best way to calculate a single embedding for a point represented by multiple images?)\n",
    " - Now, for each sampled point there will be a dataframe with information about the point and its embedding. Read another polygon spatial data file, that I will provide, which contains area-level estimates of gentrification.\n",
    " - Use point-in-polygon to get the gentrification for each point.\n",
    " - Use cross-validation to train a couple of ML models (probaly random forest, linear regression and a neural network) to estimate gentrification from the embedding vectors\n",
    " - Choose the best model and parameter configuration and test this model on some held-out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db77133c636e6a5",
   "metadata": {},
   "source": [
    "## Configuration and library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8f5f7d1fdc5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.114777Z",
     "start_time": "2025-09-04T09:36:29.111180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import folium\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import torch\n",
    "from tqdm.auto import tqdm                    # auto picks notebook / console style\n",
    "import multiprocessing\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Hugging Face Transformers for image embedding\n",
    "from transformers import AutoImageProcessor, AutoModel  # will load a vision model\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------- Configuration -----------------\n",
    "#np.random.seed(42)\n",
    "density_per_km = 0.3  # number of points to sample per km of road\n",
    "#density_per_km = 0.1  # VERY FEW WHILE TESTING\n",
    "DOWNLOAD_IMAGES = False  # If false then don't download any images, just load those that have been cached\n",
    "\n",
    "# Can decide, after analysing images, which models we want to run (to predict gentrification and/or deprivation)\n",
    "RUN_GENTRIFICATION_MODEL = False  \n",
    "RUN_IMD_MODEL = True\n",
    "\n",
    "data_dir = Path(os.path.join(\"..\", \"data\", \"airbnb-manchester\"))\n",
    "boundary_file = os.path.join(data_dir, \"greater_manchester_lsoas.geojson\")  # Path to boundary polygon file\n",
    "gentrification_file = os.path.join(\"..\", \"data\", \"gmgi_data\", \"lsoa_summary_jan25.csv\")  # Path to polygons with gentrification index\n",
    "lsoas_file = os.path.join(\"..\", \"data\", \"LSOAs_2011\", \"LSOA_2011_EW_BSC_V4.shp\")\n",
    "imd_file = os.path.join(\"..\", \"data\", \"imd\", \"File_2_-_IoD2019_Domains_of_Deprivation.xlsx\")\n",
    "sample_spacing = 200.0   # distance in meters between sample points on roads\n",
    "#sample_spacing = 5000.0  # Very large for testing\n",
    "n_directions = 4         # number of street view images per point (e.g., 4 cardinal directions)\n",
    "image_size = \"640x640\"   # requested image resolution from Street View API\n",
    "\n",
    "# Create directories for caching if not exist\n",
    "#Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "image_dir = Path(os.path.join(data_dir, \"street_images\"))\n",
    "image_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a13cd0696105",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a11b33a5e00526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:29.661138Z",
     "start_time": "2025-09-04T09:36:29.529524Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Load neighbourhood polygons and dissolve to one study-area boundary ---\n",
    "boundary_neighs = gpd.read_file(boundary_file)\n",
    "# (Optional) keep original neighbourhoods for later mapping/stratified sampling\n",
    "#neighbourhoods_gdf = boundary_neighs.copy()\n",
    "# Make sure we're in WGS84 (lat/lon) for OSM and APIs\n",
    "boundary_neighs = boundary_neighs.to_crs(epsg=4326)\n",
    "# Dissolve: merge all geometries into one polygon (MultiPolygon possible)\n",
    "boundary_polygon = boundary_neighs.unary_union  # shapely (multi)polygon\n",
    "boundary_gdf = gpd.GeoDataFrame(\n",
    "    data={'name': ['study_area']},\n",
    "    geometry=[boundary_polygon],\n",
    "    crs=boundary_neighs.crs\n",
    ")\n",
    "\n",
    "# TEMP! Just keep one LSOA for testing\n",
    "#boundary_gdf = boundary_neighs.sample(1)\n",
    "\n",
    "print(\"Merged neighbourhoods into single study-area boundary.\")\n",
    "print(\"Bounds:\", boundary_polygon.bounds)\n",
    "boundary_gdf.plot(color='lightblue', edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c742d-721d-41d7-8287-027c8d4bc523",
   "metadata": {},
   "source": [
    "## Read the LSOA boundary data\n",
    "\n",
    "(later it will be joined to the Greater Manchester Gentrification Index and IMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc5e31eb629a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoas =  gpd.read_file(lsoas_file)\n",
    "manc_lads = ['Manchester', 'Rochdale', 'Bolton', 'Bury', 'Wigan', 'Oldham',  'Trafford', 'Salford', 'Tameside', 'Stockport']\n",
    "manc_lads_pattern = '|'.join(manc_lads)\n",
    "gm_lsoa=lsoas[lsoas['LSOA11NMW'].str.contains(manc_lads_pattern)]\n",
    "gm_lsoa = gm_lsoa.to_crs(epsg=4326)\n",
    "gm_lsoa.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b35733a70d8a7",
   "metadata": {},
   "source": [
    "## Get Road Network for the Area from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64dc94ca59522d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:37.673731Z",
     "start_time": "2025-09-04T09:36:29.920834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get (or cache) the road network with OSMnx’s built-in GraphML I/O\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Tell OSMnx to keep all cache files inside the project folder (optional)\n",
    "#ox.settings.cache_folder = str(data_dir / \"osmnx_http_cache\")  # HTTP/tile cache\n",
    "#ox.settings.use_cache = True  # default is True\n",
    "ox.settings.use_cache = False      # <- prevents the “cache” dir being written\n",
    "\n",
    "\n",
    "graph_file = data_dir / \"road_network.graphml\"  # one self-contained file\n",
    "\n",
    "if graph_file.exists():\n",
    "    print(\"Loading road network from GraphML cache …\")\n",
    "    road_graph = ox.load_graphml(graph_file)\n",
    "else:\n",
    "    print(\"Downloading road network from OSM …\")\n",
    "    road_graph = ox.graph_from_polygon(boundary_polygon, network_type=\"drive\")\n",
    "    ox.save_graphml(road_graph, graph_file)\n",
    "    print(f\"Graph saved to {graph_file}\")\n",
    "\n",
    "# Convert to GeoDataFrame of edges for downstream sampling/plotting\n",
    "edges_gdf = ox.graph_to_gdfs(road_graph, nodes=False)\n",
    "\n",
    "print(f\"Number of road segments: {len(edges_gdf)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fa123b2c7417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:40.873453Z",
     "start_time": "2025-09-04T09:36:37.775557Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot roads first (thin gray lines)\n",
    "edges_gdf.plot(ax=ax, linewidth=0.4, color=\"gray\")\n",
    "\n",
    "# Plot the study-area outline on top (thicker red line)\n",
    "boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788f0f174d1fc23",
   "metadata": {},
   "source": [
    "## Generate a Sample of Points along the road network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc0dd3dae623d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:42.902505Z",
     "start_time": "2025-09-04T09:36:40.975907Z"
    }
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_IMAGES:\n",
    "    # Generate a sample of points along the street network (probabilistic)\n",
    "    # --------------------------------------------------------------------\n",
    "    # PARAMETERS\n",
    "    min_points_per_edge = 0  # allow very short edges to have none\n",
    "    max_points_per_edge = 10  # safety cap per edge\n",
    "\n",
    "    # Re-project roads to a metric CRS for length calculations\n",
    "    utm_crs = boundary_gdf.estimate_utm_crs()\n",
    "    edges_proj = edges_gdf.to_crs(utm_crs)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # Loop over every edge geometry\n",
    "    for geom in edges_proj.geometry:\n",
    "        if geom is None or geom.length == 0:\n",
    "            continue\n",
    "\n",
    "        length_m = geom.length\n",
    "        expected = length_m / 1000 * density_per_km  # λ for Poisson\n",
    "        n_points = np.random.poisson(expected)  # 0, 1, 2, …\n",
    "\n",
    "        # Clip to limits\n",
    "        n_points = max(min_points_per_edge, min(n_points, max_points_per_edge))\n",
    "\n",
    "        if n_points == 0:\n",
    "            continue\n",
    "\n",
    "        # Evenly distribute interior points (skip endpoints)\n",
    "        distances = np.linspace(0, length_m, n_points + 2)[1:-1]\n",
    "        for d in distances:\n",
    "            samples.append(geom.interpolate(d))\n",
    "\n",
    "    # Build GeoDataFrame of sample points (back to WGS-84 for API use)\n",
    "    points_gdf = (\n",
    "        gpd.GeoDataFrame(geometry=gpd.GeoSeries(samples), crs=utm_crs)\n",
    "        .to_crs(epsg=4326)\n",
    "    )\n",
    "    points_gdf[\"lon\"] = points_gdf.geometry.x.round(6)\n",
    "    points_gdf[\"lat\"] = points_gdf.geometry.y.round(6)\n",
    "    points_gdf = points_gdf.drop_duplicates(subset=[\"lat\", \"lon\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f\"Generated {len(points_gdf)} points \"\n",
    "        f\"with expected density {density_per_km} pts/km.\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"DOWNLOAD_IMAGES is false, so not sampling from the road network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714470e04becead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:36:52.022021Z",
     "start_time": "2025-09-04T09:36:47.908276Z"
    }
   },
   "outputs": [],
   "source": [
    "if DOWNLOAD_IMAGES:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Plot roads first (thin gray lines)\n",
    "    edges_gdf.plot(ax=ax, linewidth=0.4, color=\"gray\")\n",
    "\n",
    "    # Plot the sample points\n",
    "    points_gdf.plot(ax=ax, color=\"blue\", markersize=2, label=\"Sample Points\")\n",
    "\n",
    "    # Plot the study-area outline on top (thicker red line)\n",
    "    boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "    ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "    ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "    ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ed56ad756c48c",
   "metadata": {},
   "source": [
    "## Download street view images for each point\n",
    "\n",
    "Note: expects a valid Google Maps API key in the file `google_maps_api_key.txt` in the same directory as this script (not synced to github for obvious reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d49a176b8317e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T09:37:05.083513Z",
     "start_time": "2025-09-04T09:37:05.080899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the API key from a file (of needed)\n",
    "if DOWNLOAD_IMAGES:\n",
    "    with open('google_maps_api_key.txt', 'r') as f:\n",
    "        api_key = f.readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff25f11b7ee74",
   "metadata": {},
   "source": [
    "A load of images were downloaded incorrectly (just got a streetview blank jpeg not a proper image). The following code identifies the first image that went wrong (I found that it was image 8116 by looking at the saved picture files) and removes it and all other from the point records. Then I delete the corresponding images. Hopefully this removed the bad images and kept the point cache and images aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082704bebf574a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T07:38:15.083017Z",
     "start_time": "2025-08-01T07:38:15.081159Z"
    }
   },
   "outputs": [],
   "source": [
    "#with open(points_data_cache, \"rb\") as f:\n",
    "#    point_records = pickle.load(f)\n",
    "\n",
    "#point_records_bak = point_records.copy()\n",
    "#index = next(i for i, d in enumerate(point_records) if d['point_id'] == 8116)\n",
    "#print(index)\n",
    "#point_records[index]\n",
    "\n",
    "#point_records = point_records[:index]\n",
    "\n",
    "#point_records[8001]\n",
    "\n",
    "#with open(points_data_cache, \"wb\") as f:\n",
    "#    pickle.dump(point_records, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf73b5538e29e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:16.919921Z",
     "start_time": "2025-09-04T09:37:51.755699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cache file for the entire points data with embeddings (images are stored separately)\n",
    "DEBUG = False\n",
    "points_data_cache = data_dir / \"points_with_embeddings.pkl\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Load existing cache so we can *append* new sample points\n",
    "# -----------------------------------------------------------\n",
    "if points_data_cache.exists():\n",
    "    print(\"Loading cached point data …\")\n",
    "    with open(points_data_cache, \"rb\") as f:\n",
    "        point_records = pickle.load(f)\n",
    "    existing_coords = {(rec[\"latitude\"], rec[\"longitude\"]) for rec in point_records}\n",
    "    next_id = max(rec[\"point_id\"] for rec in point_records) + 1\n",
    "else:\n",
    "    point_records = []\n",
    "    existing_coords = set()\n",
    "    next_id = 0\n",
    "\n",
    "print(f\"Cache currently has {len(point_records)} points.\")\n",
    "added_this_run = 0\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Iterate through newly‑sampled street‑network points (with a progress bar)\n",
    "# -----------------------------------------------------------\n",
    "if DOWNLOAD_IMAGES:\n",
    "    for _, row in tqdm(points_gdf.iterrows(), total=len(points_gdf), desc=\"Downloading images\"):\n",
    "        lat = row[\"lat\"]\n",
    "        lon = row[\"lon\"]\n",
    "\n",
    "        # Skip if imagery for this coordinate (rounded earlier) already exists\n",
    "        if (lat, lon) in existing_coords:\n",
    "            continue\n",
    "\n",
    "        point_id = next_id\n",
    "        next_id += 1\n",
    "        added_this_run += 1\n",
    "\n",
    "        # ---- Street‑View metadata ----\n",
    "        meta_params = {\"location\": f\"{lat},{lon}\", \"key\": api_key}\n",
    "        meta_url = \"https://maps.googleapis.com/maps/api/streetview/metadata\"\n",
    "        try:\n",
    "            meta = requests.get(meta_url, params=meta_params).json()\n",
    "        except Exception as e:\n",
    "            print(f\"[Point {point_id}] Metadata request failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        if meta.get(\"status\") != \"OK\":\n",
    "            if DEBUG:\n",
    "                print(f\"[Point {point_id}] Street View not available (status={meta.get('status')}).\")\n",
    "            continue\n",
    "\n",
    "        pano_id = meta.get(\"pano_id\")\n",
    "        date = meta.get(\"date\")  # e.g. \"2024‑08\"\n",
    "\n",
    "        # ---- Download images for the specified headings ----\n",
    "        point_image_files = []\n",
    "        for heading in np.linspace(0, 360, num=n_directions, endpoint=False):\n",
    "            fname = f\"point{point_id}_heading{int(heading)}.jpg\"\n",
    "            image_path = image_dir / fname\n",
    "            point_image_files.append(str(image_path))\n",
    "\n",
    "            if image_path.exists():\n",
    "                continue  # already on disk\n",
    "\n",
    "            img_params = {\n",
    "                \"size\": image_size,\n",
    "                \"pano\": pano_id,\n",
    "                \"heading\": str(int(heading)),\n",
    "                \"pitch\": \"0\",\n",
    "                \"key\": api_key,\n",
    "            }\n",
    "            img_url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
    "            try:\n",
    "                img_resp = requests.get(img_url, params=img_params)\n",
    "                with open(image_path, \"wb\") as f:\n",
    "                    f.write(img_resp.content)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download image for point {point_id}, heading {heading}: {e}\")\n",
    "\n",
    "        # ---- Append the new record ----\n",
    "        point_records.append(\n",
    "            {\n",
    "                \"point_id\": point_id,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"date\": date,\n",
    "                \"image_files\": point_image_files,\n",
    "                \"embedding\": None,  # to be filled later\n",
    "            }\n",
    "        )\n",
    "        existing_coords.add((lat, lon))\n",
    "\n",
    "    print(f\"Added {added_this_run} new points this run (total now {len(point_records)}).\")\n",
    "\n",
    "    # Persist the (possibly) updated cache immediately\n",
    "    with open(points_data_cache, \"wb\") as f:\n",
    "        pickle.dump(point_records, f)\n",
    "\n",
    "else:\n",
    "    print(\"DOWNLOAD_IMAGES set to false, so not downloading any images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba1f32-9f79-4985-a01f-6de3b3a0a942",
   "metadata": {},
   "source": [
    "Map of the full sample (cache + any others just downlaoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952715d-e889-404b-96be-72e783920876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:17:15.525112Z",
     "start_time": "2025-09-04T11:17:15.239938Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot roads first (thin gray lines)\n",
    "#edges_gdf.plot(ax=ax, linewidth=0.4, color=\"gray\")\n",
    "\n",
    "# Plot the sample points\n",
    "gpd.GeoDataFrame(point_records, \n",
    "                 geometry=[Point(rec[\"longitude\"], rec[\"latitude\"]) for rec in point_records], \n",
    "                 crs=\"EPSG:4326\").plot(ax=ax, color=\"blue\", markersize=0.5, label=\"Sample Points\")\n",
    "\n",
    "# Plot the study-area outline on top (thicker red line)\n",
    "boundary_gdf.boundary.plot(ax=ax, linewidth=2, edgecolor=\"red\")\n",
    "\n",
    "ax.set_title(\"Road network & study-area boundary\", pad=12)\n",
    "ax.set_axis_off()          # hides lat/lon ticks for a cleaner look\n",
    "ax.set_aspect(\"equal\")     # keeps the map from looking stretched\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbabc74-c89f-4ae2-a500-36165868c612",
   "metadata": {},
   "source": [
    "Show some randomly chosen images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd3ed63eec4c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:16:31.079133Z",
     "start_time": "2025-09-04T11:16:30.077955Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------\n",
    "n_points_to_show = 6          # rows\n",
    "headings_per_pt  = n_directions   # columns; assumption: same for all points\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SAMPLE POINTS\n",
    "# -------------------------------------------------------\n",
    "# point_records was created earlier when you downloaded images\n",
    "valid_records = [rec for rec in point_records if rec.get(\"image_files\")]\n",
    "if len(valid_records) < n_points_to_show:\n",
    "    raise ValueError(f\"Need at least {n_points_to_show} points with images; \"\n",
    "                     f\"found {len(valid_records)}\")\n",
    "\n",
    "sample_pts = random.sample(valid_records, n_points_to_show)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# PLOT\n",
    "# -------------------------------------------------------\n",
    "fig, axes = plt.subplots(n_points_to_show,\n",
    "                         headings_per_pt,\n",
    "                         figsize=(headings_per_pt * 3.5, n_points_to_show * 3))\n",
    "\n",
    "for row, rec in enumerate(sample_pts):\n",
    "    imgs = rec[\"image_files\"]\n",
    "    # If fewer than expected headings (e.g. download failure), pad with blanks\n",
    "    while len(imgs) < headings_per_pt:\n",
    "        imgs.append(None)\n",
    "\n",
    "    for col, img_path in enumerate(imgs[:headings_per_pt]):\n",
    "        ax = axes[row, col] if n_points_to_show > 1 else axes[col]\n",
    "\n",
    "        if img_path and os.path.exists(img_path):\n",
    "            ax.imshow(Image.open(img_path))\n",
    "        else:\n",
    "            # blank panel if the image is missing\n",
    "            ax.text(0.5, 0.5, \"no image\", ha=\"center\", va=\"center\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # column headers once at top\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"heading {col*360/headings_per_pt:.0f}°\", fontsize=10, pad=6)\n",
    "\n",
    "    # label the leftmost image with point info\n",
    "    axes[row, 0].set_ylabel(\n",
    "        f\"point {rec['point_id']}\\n({rec['latitude']:.3f}, {rec['longitude']:.3f})\",\n",
    "        fontsize=8, rotation=0, ha=\"right\", va=\"center\"\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Street-View snapshots: 6 random points × 4 headings\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7712fd21270dd8",
   "metadata": {},
   "source": [
    "## Compute the Embeddings\n",
    "\n",
    "(Note: would like to use Places365 but not available in Hugging Face yet, so using ViT base model instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7c3272d3d1d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:28:16.975400Z",
     "start_time": "2025-09-04T11:17:26.939039Z"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Load ViT-Base (ImageNet-21k) and pick CUDA ▸ MPS ▸ CPU device\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "print(f\"Loading {model_name} …\")\n",
    "processor   = AutoImageProcessor.from_pretrained(model_name)\n",
    "model       = AutoModel.from_pretrained(model_name).eval()   # no classifier head\n",
    "\n",
    "# ----- smart device selection -----\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✔ Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")      # Apple-Silicon Metal\n",
    "    print(\"✔ Using Apple MPS GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"✔ Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Embed all images for each point\n",
    "# -------------------------------------------------------------\n",
    "already_have_embedding = 0\n",
    "for rec in point_records:\n",
    "    embeds = []\n",
    "    # Skip records that already have an embedding\n",
    "    if rec.get(\"embedding\") is not None:\n",
    "        already_have_embedding += 1\n",
    "        continue\n",
    "    embeds = []\n",
    "    for img_path in rec[\"image_files\"]:\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        # Load & convert to RGB (some JPEGs are encoded as P)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Tokenize and normalize the image\n",
    "        inputs = processor(img, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}   # move tensors\n",
    "\n",
    "        # Forward pass through the model (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "\n",
    "        # Extract the CLS token embedding (first token in the sequence)\n",
    "        cls = out.last_hidden_state[0, 0, :].cpu().numpy()     # CLS token\n",
    "        embeds.append(cls)\n",
    "\n",
    "    # Store the mean embedding for this point (or none)\n",
    "    rec[\"embedding\"] = None if not embeds else np.mean(embeds, axis=0)\n",
    "\n",
    "print(f\"Created {len(point_records)-already_have_embedding} new embeddings. \"\n",
    "      f\"{already_have_embedding} points had existing embeddings.\")\n",
    "\n",
    "# cache the enriched records\n",
    "with open(points_data_cache, \"wb\") as f:\n",
    "    pickle.dump(point_records, f)\n",
    "\n",
    "print(f\"✓ {len(point_records)} image embeddings computed and cached.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f4253247e9533",
   "metadata": {},
   "source": [
    "Do a PCA to get 3 dimensions for each embedding, the scale to RGB and map them. Click on a point to see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2830031dc9e007b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T11:30:14.401831Z",
     "start_time": "2025-09-04T11:30:14.254417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise embeddings in “RGB PCA space”\n",
    "# Each point’s 768-D embedding → 3-D PCA → scaled 0-1 → RGB colour\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Collect embeddings and GeoDataFrame of points\n",
    "# -------------------------------------------------------\n",
    "records = [rec for rec in point_records if rec[\"embedding\"] is not None]\n",
    "X = np.vstack([rec[\"embedding\"] for rec in records])  # (N, 768)\n",
    "coords = np.array([[rec[\"longitude\"], rec[\"latitude\"]] for rec in records])\n",
    "\n",
    "# optional: bring in points_gdf if you prefer plotting via GeoPandas\n",
    "# points_gdf = gpd.GeoDataFrame({'geometry': gpd.points_from_xy(coords[:,0], coords[:,1])})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. PCA → first 3 components\n",
    "# -------------------------------------------------------\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "rgb3 = pca.fit_transform(X)  # (N, 3)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Scale each PC separately to [0, 1]\n",
    "# -------------------------------------------------------\n",
    "rgb_min = rgb3.min(axis=0)  # per-column min\n",
    "rgb_max = rgb3.max(axis=0)\n",
    "rgb_scaled = (rgb3 - rgb_min) / (rgb_max - rgb_min + 1e-9)  # avoid /0\n",
    "colors = [tuple(c) for c in rgb_scaled]  # list of (r,g,b) floats 0-1\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Scatter plot on lon/lat, coloured by PCA-RGB\n",
    "# -------------------------------------------------------\n",
    "#fig, ax = plt.subplots(figsize=(7, 7))\n",
    "#ax.scatter(coords[:, 0], coords[:, 1], c=colors, s=8, alpha=0.9, linewidths=0)\n",
    "\n",
    "#ax.set_aspect(\"equal\")\n",
    "#ax.set_title(\"Point embeddings visualised as RGB (PCA first 3 comps)\")\n",
    "#ax.axis(\"off\")  # hide ticks – remove if you want lon/lat grid\n",
    "\n",
    "#plt.show()\n",
    "                              # last expression → interactive map inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea9419cd76c010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T08:17:39.464822Z",
     "start_time": "2025-09-05T08:17:34.377852Z"
    }
   },
   "outputs": [],
   "source": [
    "def img_tag(path, width=150):\n",
    "    \"\"\"\n",
    "    Return a <img> tag with the image inlined as base64.\n",
    "    Width in pixels; height auto.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return \"<div style='width:{}px;height:{}px;background:#ccc;'>no img</div>\".format(width, int(width*0.75))\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"<img src='data:image/jpeg;base64,{b64}' width='{width}px' style='margin:2px;'/>\"\n",
    "\n",
    "\n",
    "# Convert [0–1] RGB tuples -> HEX strings for Leaflet\n",
    "hex_colors = [\n",
    "    \"#\" + \"\".join(f\"{int(c*255):02x}\" for c in rgb) for rgb in colors\n",
    "]\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[coords[:, 1].mean(), coords[:, 0].mean()], # centred on the mean lat/lon\n",
    "    zoom_start=12,\n",
    "    tiles=\"cartodbpositron\"   # light background; try 'openstreetmap'\n",
    ")\n",
    "\n",
    "# ---------- Add points ----------\n",
    "#for rec, hx in zip(records, hex_colors):\n",
    "# Sample ome points, map breaks with too many\n",
    "for rec, hx in random.sample(list(zip(records, hex_colors)), 200):\n",
    "    # Build HTML table of up to 4 images (N,E,S,W)\n",
    "    imgs_html = \"\".join(img_tag(p, width=140) for p in rec[\"image_files\"][:4])\n",
    "    popup_html = f\"\"\"\n",
    "    <div style=\"text-align:center\">\n",
    "        <b>point {rec['point_id']}</b><br/>\n",
    "        {imgs_html}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[rec[\"latitude\"], rec[\"longitude\"]],\n",
    "        radius=6,\n",
    "        color=hx, fill=True, fill_color=hx, fill_opacity=0.9, weight=0,\n",
    "        popup=folium.Popup(popup_html, max_width=600)\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "#m.save(\"embedding_rgb_with_photos.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d319c6d584dfe",
   "metadata": {},
   "source": [
    "## Prepare training data for the gentrification model\n",
    "\n",
    "Note that we don't actually calculate the model later unless `RUN_GENTRIFICATION_MODEL` is true, mostly because it takes bloody ages. But some of the operations, like joining LSOAs to embeddings, are needed for the IMD model so we do run some of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af61fecf0f1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GMGI\n",
    "gmgi = pd.read_csv(gentrification_file)\n",
    "gmgi = gmgi.iloc[:,1:]  # Drop the first column\n",
    "gmgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb47fc52933ea2",
   "metadata": {},
   "source": [
    "Attach GMGI to LSOAs that we read earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38feff5e907726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach GMGI columns to lsoa data\n",
    "gm_gmgi_lsoa = pd.merge(left=gm_lsoa, right=gmgi, left_on=\"LSOA11CD\", right_on=\"LSOA11CD\")\n",
    "# Map the gentrification index (sanity check)\n",
    "gm_gmgi_lsoa.plot(column=\"gi_n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060cad100582810",
   "metadata": {},
   "source": [
    "Join image embeddings points to gentrification LSOAs (so we have the 'ground truth' gentrification score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a20b7d1e3b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all records have an embeding\n",
    "invalid_records = [rec for rec in point_records if rec.get('embedding') is None]\n",
    "assert len(invalid_records)==0, f\"Found {len(invalid_records)} invalid points\"\n",
    "\n",
    "# Now join the embeddings points to the LSOAs\n",
    "point_coords = [Point(rec['longitude'], rec['latitude']) for rec in point_records]\n",
    "points_labels_gdf = gpd.GeoDataFrame(point_records, geometry=point_coords, crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to get gentrification label for each point\n",
    "points_labels_gdf = gpd.sjoin(points_labels_gdf, gm_gmgi_lsoa, how='inner', predicate='within')\n",
    "# sjoin may add an index from the polygon ('index_right'); we can drop it\n",
    "if 'index_right' in points_labels_gdf.columns:\n",
    "    points_labels_gdf = points_labels_gdf.drop(columns=['index_right'])\n",
    "\n",
    "print(f\"Points after spatial join: {len(points_labels_gdf)} / {len(point_records)}\"\n",
    "      f\" `(some points may lie outside the label polygons and were dropped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9aa7d-710b-4a3d-a4af-d3e59fa6736a",
   "metadata": {},
   "source": [
    "Show them on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9073d-0bb2-4526-a47b-eeec5751d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'gi_n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e528cb9af1e778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gm_gmgi_lsoa.plot(column=label_col)\n",
    "# Add the points\n",
    "points_labels_gdf.plot(ax=ax, column=label_col, markersize=10, edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20328006b4cf7238",
   "metadata": {},
   "source": [
    "Prepare X and y data for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dab5f8f5a9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Validate point records before building X, y\n",
    "# ------------------------------------------------------------------\n",
    "def _is_bad_embedding(e):\n",
    "    \"\"\"True if embedding is missing, not a numpy array, or empty.\"\"\"\n",
    "    return (e is None) or (not isinstance(e, np.ndarray)) or (e.size == 0)\n",
    "\n",
    "\n",
    "# Boolean masks\n",
    "missing_label  = points_labels_gdf[label_col].isna()\n",
    "bad_embedding  = points_labels_gdf[\"embedding\"].apply(_is_bad_embedding)\n",
    "\n",
    "# Report any problems\n",
    "n_bad_label   = missing_label.sum()\n",
    "n_bad_embed   = bad_embedding.sum()\n",
    "n_bad_total   = (missing_label | bad_embedding).sum()\n",
    "\n",
    "if n_bad_total:\n",
    "    msg = (f\"⚠️  {n_bad_total} invalid point(s) detected \"\n",
    "           f\"({n_bad_label} with missing label, \"\n",
    "           f\"{n_bad_embed} with missing/empty embedding).\")\n",
    "    print(msg)\n",
    "\n",
    "    # show first few offending rows for inspection\n",
    "    print(points_labels_gdf.loc[missing_label | bad_embedding,\n",
    "                                [\"point_id\", label_col, \"embedding\"]].head())\n",
    "\n",
    "else:\n",
    "    print(f\"{n_bad_total} invalid point(s) detected \")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Keep only valid rows\n",
    "# ------------------------------------------------------------------\n",
    "points_labels_gdf = points_labels_gdf.loc[~(missing_label | bad_embedding)].reset_index(drop=True)\n",
    "\n",
    "if points_labels_gdf.empty:\n",
    "    raise ValueError(\"No valid points left after cleaning — cannot train model.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build feature matrix X and target vector y\n",
    "# ------------------------------------------------------------------\n",
    "X = np.stack(points_labels_gdf[\"embedding\"].values)     # shape (n_points, embed_dim)\n",
    "y = points_labels_gdf[label_col].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Target vector shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c1d7b3e311955",
   "metadata": {},
   "source": [
    "## Run the gentrification ML models\n",
    "\n",
    "To predict gentrification from the image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f3e742116b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X, y, np.arange(X.shape[0]), test_size=0.2, random_state=42)\n",
    "    print(f\"Training points: {X_train.shape[0]}, Test points: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Define model pipelines and parameter grids for cross-validation\n",
    "    models = []\n",
    "    param_grids = []\n",
    "    \n",
    "    # 1. Linear Regression (with standard scaling)\n",
    "    pipe_linear = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', LinearRegression())\n",
    "    ])\n",
    "    # No hyperparameters to tune for plain LinearRegression (we could consider Ridge/Lasso alphas, but skip for simplicity)\n",
    "    models.append(pipe_linear)\n",
    "    param_grids.append({})  # empty grid means just evaluate the baseline linear model\n",
    "    \n",
    "    # 2. Random Forest Regressor\n",
    "    pipe_rf = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # scaler doesn't affect RF but included for uniformity\n",
    "        ('reg', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_rf)\n",
    "    param_grids.append({\n",
    "        'reg__n_estimators': [100, 200],   # try 100 and 200 trees\n",
    "        'reg__max_depth': [None, 10, 20]  # try unlimited depth and a couple of depth limits\n",
    "    })\n",
    "    \n",
    "    # 3. Neural Network (MLPRegressor)\n",
    "    pipe_mlp = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', MLPRegressor(max_iter=500, random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_mlp)\n",
    "    param_grids.append({\n",
    "        'reg__hidden_layer_sizes': [(100,), (100,50)],  # one hidden layer vs two layers\n",
    "        'reg__alpha': [1e-4, 1e-3]  # L2 regularization strengths\n",
    "        # (Other hyperparameters like learning_rate_init can be added if needed)\n",
    "    })\n",
    "    \n",
    "    # Perform cross-validation for each model to find the best hyperparameters\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_name = None\n",
    "    \n",
    "    print(\"Training models\")\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    ncores = min(multiprocessing.cpu_count()-1, 40)\n",
    "    print(f\"USing {ncores} cores.\")  # Take all cores but one, and not more than 40\n",
    "    for model, param_grid, name in zip(models, param_grids, [\"LinearReg\", \"RandomForest\", \"NeuralNet\"]):\n",
    "        print(f\"\\tTraining: {model}...\")\n",
    "        if param_grid:\n",
    "            # Use GridSearchCV for models with hyperparameters\n",
    "            grid = GridSearchCV(model, param_grid, cv=cv, scoring='r2', n_jobs=ncores)\n",
    "            grid.fit(X_train, y_train)\n",
    "            cv_score = grid.best_score_\n",
    "            model_best = grid.best_estimator_\n",
    "            params_best = grid.best_params_\n",
    "        else:\n",
    "            # For Linear Regression (no params to tune), just do cross_val_score\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "            cv_score = np.mean(scores)\n",
    "            model.fit(X_train, y_train)  # train on full training data\n",
    "            model_best = model\n",
    "            params_best = None\n",
    "        print(f\"{name} CV mean R^2 = {cv_score:.3f} {('(best params: '+str(params_best)+')') if params_best else ''}\")\n",
    "        # Track the best model\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model_best\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"Best model from CV: {best_model_name} with R^2 = {best_score:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"RUN_GENTRIFICATION_MODEL is False so not running the gentrification predictor model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4ba14-612f-436e-a5fb-4178eab6252b",
   "metadata": {},
   "source": [
    "Test on a held out set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8baf0-e935-4f98-a1b8-5135bd657357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Ensure best_model is trained on the entire training set (GridSearchCV already did refit; for linear we did manually)\n",
    "    # If best_model was from cross_val_score (Linear), we already called model.fit above.\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    # rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # (not avialable in older skikitlearn)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"\\nTest R^2 score: {r2_test:.3f}\")\n",
    "    print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "    \n",
    "    # Attach predictions to the test points for mapping\n",
    "    test_points = points_labels_gdf.iloc[test_idx].copy()\n",
    "    test_points['predicted'] = y_pred\n",
    "    test_points['error'] = test_points['predicted'] - test_points[label_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7624f-6b54-4ea2-9d6a-08ee2a7c269e",
   "metadata": {},
   "source": [
    "Visualise gentrification maps and error predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145134f-d0f8-4886-b977-89e0b36213c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GENTRIFICATION_MODEL:\n",
    "    # Plot the actual gentrification by area\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    gm_gmgi_lsoa.plot(column=label_col, ax=ax, legend=True, cmap='plasma', edgecolor='gray')\n",
    "    ax.set_title(\"Actual Gentrification by Area\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the prediction errors at test points\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot polygons outlines for context\n",
    "    gm_gmgi_lsoa.boundary.plot(ax=ax, color='lightgray')\n",
    "    # Plot test points with error coloration\n",
    "    test_points.plot(column='error', ax=ax, legend=True, cmap='coolwarm', markersize=50)\n",
    "    ax.set_title(\"Model Prediction Error at Test Points\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec6dad-6b37-4f30-9fcb-0ce91d6a976b",
   "metadata": {},
   "source": [
    "## Prepare data for the deprivation model\n",
    "\n",
    "Repeat the gentrification stuff but this time try to predict deprivation (measured using the IMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab8813-a64b-4038-8b8a-ae955e72a0e2",
   "metadata": {},
   "source": [
    "Get the IMD data and read it in. I am using the file [File_2_-_IoD2019_Domains_of_Deprivation.xlsx](../data/File_2_-_IoD2019_Domains_of_Deprivation.xlsx) from the main [IMD 2019 gov page](https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859847e-f332-4b60-aadf-5bdc4f25b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read Excel file\n",
    "imd = pd.read_excel(imd_file, sheet_name=\"IoD2019 Domains\", header=0)\n",
    "\n",
    "# Rename columns to simpler versions\n",
    "imd_col_map = {\n",
    "    \"LSOA code (2011)\": \"lsoa_2011_code\",\n",
    "    \"LSOA name (2011)\": \"lsoa_2011_name\",\n",
    "    \"Local Authority District code (2019)\": \"lad_2019_code\",\n",
    "    \"Local Authority District name (2019)\": \"lad_2019_name\",\n",
    "    \"Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)\": \"imd_rank\",\n",
    "    \"Index of Multiple Deprivation (IMD) Decile (where 1 is most deprived 10% of LSOAs)\": \"imd_decile\",\n",
    "    \"Income Rank (where 1 is most deprived)\": \"income_rank\",\n",
    "    \"Income Decile (where 1 is most deprived 10% of LSOAs)\": \"income_decile\",\n",
    "    \"Employment Rank (where 1 is most deprived)\": \"employment_rank\",\n",
    "    \"Employment Decile (where 1 is most deprived 10% of LSOAs)\": \"employment_decile\",\n",
    "    \"Education, Skills and Training Rank (where 1 is most deprived)\": \"education_rank\",\n",
    "    \"Education, Skills and Training Decile (where 1 is most deprived 10% of LSOAs)\": \"education_decile\",\n",
    "    \"Health Deprivation and Disability Rank (where 1 is most deprived)\": \"health_rank\",\n",
    "    \"Health Deprivation and Disability Decile (where 1 is most deprived 10% of LSOAs)\": \"health_decile\",\n",
    "    \"Crime Rank (where 1 is most deprived)\": \"crime_rank\",\n",
    "    \"Crime Decile (where 1 is most deprived 10% of LSOAs)\": \"crime_decile\",\n",
    "    \"Barriers to Housing and Services Rank (where 1 is most deprived)\": \"housing_rank\",\n",
    "    \"Barriers to Housing and Services Decile (where 1 is most deprived 10% of LSOAs)\": \"housing_decile\",\n",
    "    \"Living Environment Rank (where 1 is most deprived)\": \"environment_rank\",\n",
    "    \"Living Environment Decile (where 1 is most deprived 10% of LSOAs)\": \"environment_decile\"\n",
    "}\n",
    "imd = imd.rename(columns=imd_col_map)\n",
    "imd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb746f57-c43b-447a-85fb-27ed262dd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the a-spatial IMD data to the LSOAs (these already have the gmgi data)\n",
    "gm_gmgi_imd_lsoa = pd.merge(left=gm_gmgi_lsoa, right=imd, left_on=\"LSOA11CD\", right_on=\"lsoa_2011_code\")\n",
    "# Map the gentrification index (sanity check)\n",
    "#gm_gmgi_imd_lsoa.explore()\n",
    "gm_gmgi_imd_lsoa.plot(column=\"imd_rank\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64918e36-b9c6-4777-a468-c7dcb2dbde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gm_gmgi_imd_lsoa.loc[:,imd_col_map.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c0dfb-60e5-48aa-b98a-ce7721509553",
   "metadata": {},
   "source": [
    "Now we have LSOAs with IMD measures. Next join the image embeddings (points, called `points_labels_gdf`) to these LSOAs to get the imd measures. We add to the existing points dataset which was created when we did the gentrification stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4ec61-c405-45db-8807-867fde19cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we only take the new IMD-related columns from the LSOA geodataframe (hence the .loc) \n",
    "# but also need the geometry otherwise gpd can't do the spatial join\n",
    "points_labels_gdf = gpd.sjoin(\n",
    "    left_df = points_labels_gdf, \n",
    "    right_df = gm_gmgi_imd_lsoa.loc[:, list(imd_col_map.values()) + [\"geometry\"]],\n",
    "    how='inner', predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2961ea8-b22c-4a70-a153-3a478a77f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'imd_rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c9bf8-0570-43d4-a640-54c593f76d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gm_gmgi_imd_lsoa.plot(column=label_col)\n",
    "# Add the points\n",
    "points_labels_gdf.plot(ax=ax, column=label_col, markersize=10, edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f816d-089c-43a5-ab88-70a9c90d0061",
   "metadata": {},
   "source": [
    "## Run the IMD model\n",
    "\n",
    "_Mostly this copies code from the gentrification model. Would be nicer to add it to functions but whatever..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c366f-5080-4d7e-9685-be638660059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Build feature matrix X and target vector y\n",
    "# ------------------------------------------------------------------\n",
    "X = np.stack(points_labels_gdf[\"embedding\"].values)     # shape (n_points, embed_dim)\n",
    "y = points_labels_gdf[label_col].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Target vector shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee294d-aa39-4c82-8f2b-e4f273a06ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "        X, y, np.arange(X.shape[0]), test_size=0.2, random_state=42)\n",
    "    print(f\"Training points: {X_train.shape[0]}, Test points: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Define model pipelines and parameter grids for cross-validation\n",
    "    models = []\n",
    "    param_grids = []\n",
    "    \n",
    "    # 1. Linear Regression (with standard scaling)\n",
    "    pipe_linear = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', LinearRegression())\n",
    "    ])\n",
    "    # No hyperparameters to tune for plain LinearRegression (we could consider Ridge/Lasso alphas, but skip for simplicity)\n",
    "    models.append(pipe_linear)\n",
    "    param_grids.append({})  # empty grid means just evaluate the baseline linear model\n",
    "    \n",
    "    # 2. Random Forest Regressor\n",
    "    pipe_rf = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # scaler doesn't affect RF but included for uniformity\n",
    "        ('reg', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_rf)\n",
    "    param_grids.append({\n",
    "        'reg__n_estimators': [100, 200],   # try 100 and 200 trees\n",
    "        'reg__max_depth': [None, 10, 20]  # try unlimited depth and a couple of depth limits\n",
    "    })\n",
    "    \n",
    "    # 3. Neural Network (MLPRegressor)\n",
    "    pipe_mlp = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', MLPRegressor(max_iter=500, random_state=42))\n",
    "    ])\n",
    "    models.append(pipe_mlp)\n",
    "    param_grids.append({\n",
    "        'reg__hidden_layer_sizes': [(100,), (100,50)],  # one hidden layer vs two layers\n",
    "        'reg__alpha': [1e-4, 1e-3]  # L2 regularization strengths\n",
    "        # (Other hyperparameters like learning_rate_init can be added if needed)\n",
    "    })\n",
    "    \n",
    "    # Perform cross-validation for each model to find the best hyperparameters\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_model_name = None\n",
    "    \n",
    "    print(\"Training models\")\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    ncores = min(multiprocessing.cpu_count()-1, 100)\n",
    "    print(f\"USing {ncores} cores.\")  # Take all cores but one, and not more than 100 (don't want to kill the HPC)\n",
    "    for model, param_grid, name in zip(models, param_grids, [\"LinearReg\", \"RandomForest\", \"NeuralNet\"]):\n",
    "        print(f\"\\tTraining: {model}...\")\n",
    "        if param_grid:\n",
    "            # Use GridSearchCV for models with hyperparameters\n",
    "            grid = GridSearchCV(model, param_grid, cv=cv, scoring='r2', n_jobs=ncores)\n",
    "            grid.fit(X_train, y_train)\n",
    "            cv_score = grid.best_score_\n",
    "            model_best = grid.best_estimator_\n",
    "            params_best = grid.best_params_\n",
    "        else:\n",
    "            # For Linear Regression (no params to tune), just do cross_val_score\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "            cv_score = np.mean(scores)\n",
    "            model.fit(X_train, y_train)  # train on full training data\n",
    "            model_best = model\n",
    "            params_best = None\n",
    "        print(f\"{name} CV mean R^2 = {cv_score:.3f} {('(best params: '+str(params_best)+')') if params_best else ''}\")\n",
    "        # Track the best model\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model_best\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"Best model from CV: {best_model_name} with R^2 = {best_score:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"RUN_IMD_MODEL is False so not running the gentrification predictor model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d2797-2a9e-413a-a3ef-79b43595e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Ensure best_model is trained on the entire training set (GridSearchCV already did refit; for linear we did manually)\n",
    "    # If best_model was from cross_val_score (Linear), we already called model.fit above.\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    # rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # (not avialable in older skikitlearn)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"\\nTest R^2 score: {r2_test:.3f}\")\n",
    "    print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "    \n",
    "    # Attach predictions to the test points for mapping\n",
    "    test_points = points_labels_gdf.iloc[test_idx].copy()\n",
    "    test_points['predicted'] = y_pred\n",
    "    test_points['error'] = test_points['predicted'] - test_points[label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5af5d-4093-47d8-b488-e9e86b67090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_IMD_MODEL:\n",
    "    # Plot the actual gentrification by area\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    gm_gmgi_imd_lsoa.plot(column=label_col, ax=ax, legend=True, cmap='plasma', edgecolor='gray')\n",
    "    ax.set_title(\"Actual IMD by Area\")\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the prediction errors at test points\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot polygons outlines for context\n",
    "    gm_gmgi_imd_lsoa.boundary.plot(ax=ax, color='lightgray')\n",
    "    # Plot test points with error coloration\n",
    "    test_points.plot(column='error', ax=ax, legend=True, cmap='coolwarm', markersize=50)\n",
    "    ax.set_title(\"Model Prediction Error at Test Points\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bf4dc-27bf-40ec-93cb-76f5120600b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
